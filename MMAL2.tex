\documentclass[a4paper,10pt]{book} %type de document et paramètres


\usepackage[utf8]{inputenc} %package fondamental
\usepackage[T1]{fontenc} %package fondamental
\usepackage[english,francais]{babel} %package de langues
\usepackage{lmodern} %police de caractère

\usepackage[top=3cm, bottom=3cm, left=4cm, right=2cm]{geometry} %permet de paramétrer les marges par défaut
\usepackage{changepage} %permet de modifier localement une mise en page (marges,...) : utilisé pour la page de garde
%\usepackage{multicol} %permet de mettre plusieurs colonnes (\begin{multicols}{2} \end{multicols} jusqu'à 10 colonnes)
\usepackage[pdftex, pdfauthor={Pierre Gimalac}, pdftitle={Algèbre Élémentaire II}, pdfsubject={Algèbre}, pdfkeywords={Algèbre}, colorlinks=true, linkcolor=black]{hyperref} %permet de se déplacer dans le pdf depuis le sommaire en cliquant sur les titres, ainsi que de parametrer les meta données du PDF
\usepackage{url} %permet de mettre des URL actifs \url{}
\let\urlorig\url
\renewcommand{\url}[1]{\begin{otherlanguage}{english}\urlorig{#1}\end{otherlanguage}}

\usepackage{mathtools,amssymb,amsthm} %maths
\usepackage{mathrsfs} %maths (par exemple les lettres caligraphiées)
\usepackage{stmaryrd} %maths (par exemple les ensembles d'entiers)
\usepackage{calrsfs} %maths (par exemple les notations des ensembles)
\usepackage{xlop} %permet d'afficher des opérations mathématiques
\usepackage[squaren,Gray]{SIunits} %permet de noter des unités proprement

\usepackage{graphicx} %permet d'insérer des images proprement (ajoute des parametres)
\usepackage{wrapfig} %permet de mettre des images à coté d'un texte
\usepackage{pdfpages} %permet d'insérer un pdf \includepdf[pages={1-2}]{truc.pdf}
\usepackage{enumitem} %permet de personnaliser les listes
\usepackage{setspace} %permet de modifier l'interligne

\usepackage{tikz} %package trooop bien permet de dessiner tout et n'importe quoi ! \begin{tikzpicture}
%\usepackage{circuitikz} %permet de dessiner des circuits logiques (entre autre) avec la syntaxe de tikz (\begin{circuitikz}) par exemple \node[american not port] pour le 'non'


\newcommand{\R}{\mathbb{R}}
\newcommand{\Rpe}{\mathbb{R}_{+}^{*}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\B}{\mathcal{B}}

\begin{document}

\begin{titlepage}
\newgeometry{margin=2.7cm}
\thispagestyle{empty}
\begin{center}
\vspace*{7cm}
\Huge \textsc{Algèbre Élémentaire II}\\
\vspace{1.5cm}
\Large Pierre Gimalac\\
\vspace{0.5cm}
\large \textit{Licence de Mathématiques}
\vfill
\end{center}
\large \textit{Janvier - Mai 2017}
\hfill
\large Cours de Bernhard Keller
\restoregeometry
\end{titlepage}

\renewcommand{\contentsname}{Sommaire}
\thispagestyle{empty}
\tableofcontents
\thispagestyle{empty}


\setcounter{MaxMatrixCols}{13}

\chapter{Polynômes}
\section{Généralités}
\subsection{Définition}
Un polynôme à coefficients dans $\K$ est une expression de la forme\\
$P=a_{n}X^{n}+a_{n-1}X^{n-1}+...+a_{1}X+a_{0}$ où $n \in \N$ et $a_{n},...,a_{0} \in \K^{n+1}$.\\

L'ensemble des polynômes est noté $\K[X]$.

\subsection{Coefficients}
Les $a_{i}$ sont appelés les coefficients du polynôme. Si tous les coefficients sont nuls,\\
P est appelé polynôme nul et on le note 0.

\subsection{Degré}
On appelle degré de $P\neq 0$ le plus grand entier i tel que $a_{i}\neq 0$. On le note $deg(P)$.\\

Le degré du polynôme nul est par convention $deg(0)=-\infty$.\\

Un polynôme de la forme $P=a_{0}$, $a_{0} \in \K$, est appelé un polynôme constant. Si $a_{0}\neq 0$, son degré est 0.

\subsubsection{Propriétés}
$deg(P\times Q)=deg(P)+deg(Q)$ et $deg(P+Q)\leq max(deg(P),deg(Q))$.

\subsection{Terminologie}
Les polynômes comportant un seul terme non nul sont appelés monômes.

Soit $P=a_{n}X^{n}+...+a_{0}$ un polynôme tel que $a_{n} \neq 0$. On appelle terme dominant le monôme $a_{n}X^{n}$. Le coefficient $a_{n}$ est appelé coefficient dominant de P.
Si le coefficient dominant est 1, P est un polynôme unitaire.

\subsection{Remarque}
Tout polynôme est donc une somme finie de monômes
\subsubsection{Exemple}
Soit $P=(X-1)(X^{n}+...+X+1)=X^{n+1}-1$. P est donc un polynôme de degré $n+1$, il est unitaire et somme des deux monômes $X^{n+1}$ et $-1$.

\section{Opérations sur les polynômes}
Soient $P=a_{n}X^{n}+a_{n-1}X^{n-1}+...+a_{1}X+a_{0}$, $Q=b_{n}X^{n}+b_{n-1}X^{n-1}+...+b_{1}X+b_{0}$ et R trois polynômes à coefficients dans $\K$.\\

\subsection{Égalité}
\subsubsection{Énoncé}
$P=Q$ si et seulement si les coefficients de P et Q sont égaux, c'est-à-dire si et seulement si $a_{i}=b_{i}$ pour tout i.\\

\subsection{Addition}
\subsubsection{Énoncé}
On définit la somme de deux polynômes comme\\
$P+Q=(a_{n}+b_{n})X^{n}+(a_{n-1}+b_{n-1})X^{n-1}+...+(a_{1}+b_{1})X+(a_{0}+b_{0})$.

\subsubsection{Propriétés}
On a $(P+Q)+R=P+(Q+R)$, $P+Q=Q+P$ et $P+0=P$.\\

\subsection{Multiplication}
\subsubsection{Énoncé}
On définit le produit de polynômes comme $P\times Q=\sum\limits_{k=0}^{r}c_{k}X^{k}$ où $r=n+m$ et $c_{k}=\sum\limits_{i+j=k}a_{i}b_{j}$.

\subsubsection{Exemple}
Soit $A=X^{3}+X^{2}+X+1$ et $B=X^{2}-1$. Alors $AB=X^{5}+X^{4}-1$.

\subsubsection{Propriétés}
On a $(P\times Q)\times R=P\times (Q\times R)$, $P\times Q=Q\times P$ et $P\times (Q+R)=P\times Q + P\times R$.\\

\subsection{Multiplication par un scalaire}
\subsubsection{Énoncé}
Si $\lambda \in \K$, alors $\lambda P$ est le polynôme dont, $ \forall i$, le i-ème coefficient est $\lambda a_{i}$.

\subsubsection{Propriétés}
On a $P\times 1=P$ et $P\times 0=0$.\\

\newpage

\subsection{Division euclidienne}
\subsubsection{Définition}
Soient $A,B \in \K[X]$. On dit que B divise A si et seulement s'il existe un polynôme $Q\in \K[X]$ tel que $A=Q\times B$.\\

On note $B|A$ "B divise A", on dit que A est multiple de B ou que A est divisible par B.

\subsubsection{Propositions}
On a $A|A$, $1|A$ et $A|0$.\\\\
Soient $A,B,C \in \K[X]$, si $A|B$ et $B|A$ alors $\exists \lambda\in\K^{*}$ tel que $A=\lambda B$.\\\\
Si $A|B$ et $B|C$, $A|C$.\\\\
Si $C|A$ et $C|B$ alors $C|AU+BV$ pour tous $U,V \in \K[X]$.\\\\

\subsubsection{Théorème de division euclidienne de polynômes}
Soient $A,B \in \K[X]$ tels que $B\neq 0$ alors il existe des polynômes Q et R uniques tels que $A=BQ+R$ et $deg R<deg B$.

\subsubsection{Remarques}
Q est appelé le quotient et R le reste de la division euclidienne de A par B.\\
On a $R=0$ si et seulement si $B|A$.

\subsubsection{Démonstration}
\emph{Unicité :}\\
Si $A=BQ+R$ et $A=BQ'+R'$ alors $B(Q-Q')=R'-R$ or $deg(R-R')<deg B$ donc $Q'-Q=0$ ainsi $Q=Q'$ d'où aussi $R=R'$.\\\\

\emph{Existence :}

On procède par récurrence sur le degré de A :\\
Si $deg(A)=0$ et $deg(B)>0$, on pose $Q=0$ et $R=A$.\\
Si $deg(A)=0$ et $deg(B)=0$, on pose $Q=\frac{A}{B}$ et $R=0$.\\

On suppose l'existence vraie pour un $deg(A)\leq n-1$.\\
Soit $A=a_{n}X^{n}+...+a_{0}$ un polynôme de degré n $(a_{n}\neq 0)$ et $B=b_{m}X^{m}+...+b_{0}$ où $b_{m}\neq 0$.\\
Si $n<m$, on pose $Q=0$ et $R=A$.\\
Si $n\geq m$, on écrit $A=\frac{a_{n}}{b_{m}}X^{n-m} B+A_{1}$ où $deg(A_{1})\leq n-1$.\\

On applique l'hypothèse de récurrence à $A_{1}$, il existe $Q_{1}$ et $R_{1}$ dans $\K[X]$ tels que $A_{1}=BQ_{1}+R_{1}$ et $deg R_{1}<deg B$.\\

Il vient $A=B(\frac{a_{n}}{b_{m}}X^{n-m}+Q_{1})+R_{1}$, donc $Q=\frac{a_{n}}{b_{m}}X^{n-m}+Q_{1}$ et $R=R_{1}$ conviennent.

\newpage

\section{Racines d'un polynôme}
\subsection{Fonction polynomiale}
\subsubsection{Définition}
Soit $P=a_{n}X^{n}+...+a_{0} \in \K[X]$. Pour un élément $x\in \K$, on note $P(x)=a_{n}x^{n}+...+a_{0} \in \K$.\\

On associe ainsi au polynôme P une fonction polynomiale notée P telle que $P : \begin{array}{rcl}
\K&\rightarrow& \K \\
X &\mapsto& P(X)
\end{array}$\\\\
Si $P=a_{n}X^{n}+...+a_{0}$ alors $P'=na_{n}X^{n-1}+...+a_{1}$ est le polynôme dérivé de P.

\subsection{Racines}
\subsubsection{Définition}
Soient $P\in \K[X]$ et $\alpha \in \K$. On dit que $\alpha$ est une racine (ou un zéro) de P si $P(\alpha)=0$.

\subsubsection{Proposition}
$\alpha\in\K$ est une racine de P si et seulement si $x-\alpha$ divise P.\\

\emph{Démonstration}\\
Lorsque l'on écrit la division euclidienne de P par $X-\alpha$, on obtient $P=Q(X-\alpha)+R$ avec R une constante car $deg(R) < deg(x-\alpha)=1$ donc $P(\alpha)=0 \Leftrightarrow Q(\alpha-\alpha)+R=0 \Leftrightarrow R(\alpha)=0\\
\Leftrightarrow R=0 \Leftrightarrow x-\alpha|P$.

\subsubsection{Racine multiple}
Soit $k\in \N^{*}$. On dit que $\alpha$ est racine de multiplicité k de P si $(x-\alpha)^{k}$ divise P et alors $(x-\alpha)^{k+1}$ ne divise pas P.\\
Lorsque k=1, on parle d'une racine simple, lorsque k=2 d'une racine double,... On dit aussi que $\alpha$ est une racine d'ordre k.

\subsubsection{Proposition}
Il y a équivalence entre :\begin{enumerate}
\item $\alpha$ est une racine de multiplicité k de P.
\item Il existe $Q\in \K[X]$ tel que $P=(X-\alpha)^{k}Q$ et $Q(\alpha) \neq 0$.
\item On a $P(\alpha)=P'(\alpha)=...=P^{(k-1)}(\alpha)=0$ et $P^{(k)}(\alpha)\neq 0$. \end{enumerate}

\subsection{Le théorème de d'Alembert Gauss}
\subsubsection{Énoncé}
Tout polynôme à coefficients complexes de degré $n\geq 1$ a au moins une racine dans $\C$. Il admet exactement n racines si on compte chaque racine avec sa multiplicité.

\subsubsection{Exemple}
On affirme que $P=x^{n}-1$ admet n racines distinctes. Sachant que P est de degré n, par le théorème de d'Alembert-Gauss on sait qu'il admet n racines comptées avec multiplicité. Il s'agit donc maintenant de montrer que ce sont des racines simples.\\

Supposons par l'absurde que $\alpha \in \C$ est une racine de multiplicité au moins 2, alors $P(\alpha)=0$ et $P'(\alpha)=0$, donc $\alpha^{n}-1=0$ et $n\alpha^{n-1}=0$.\\
De la seconde égalité, on déduit $\alpha=0$ ce qui est en contradiction avec la première égalité.\\
Ainsi toutes les racines sont simples : les n racines sont distinctes et sont les racines n-ièmes de l'unité $e^{\frac{2k\pi i}{n}}, k \in \llbracket 0;n-1\rrbracket$.

\section{Réductibilité}
\subsection{Polynôme réductible}
\subsubsection{Définition}
Soit $P\in \K[X]$ un polynôme non constant, on dit que P est irréductible si et seulement si pour tout $Q\in \K[X]$ divisant P, on a soit $Q\in \K^{*}$ soit $Q=\lambda P$ pour un $\lambda\in \K^{*}$.

\subsubsection{Remarques}
\begin{enumerate} \item Un polynôme irréductible est donc un polynôme non constant et dont les seuls diviseurs sont les constantes ou P lui-même (à une constante multiplicative près).
\item La notion de polynôme irréductible pour l'arithmétique de $\K[X]$ correspond à la notion de nombre premier pour l'arithmétique de $\Z$. \end{enumerate}

\subsubsection{Exemples}
Sur $\R$, les polynômes réductibles sont les polynômes de degré égal à 2 et de discriminant positif ou de degré supérieur à 2.\\
Sur $\C$, les polynômes réductibles sont les polynômes de degré supérieur ou égal à 2.\\

\subsection{Polynôme irréductible}
\subsubsection{Définition}
Si $P\in K[X]$ n'est pas irréductible, il est réductible.\\
Il existe alors des polynômes $A,B\in K[X]^{2}$ tels que $P=AB$, où $deg(A)\geq 1$ et $deg(B)\geq 1$.

\subsubsection{Exemples}
Sur $\R$, les polynômes irréductibles sont les polynômes de degré égal à 2 et de discriminant négatif ou de degré inférieur à 2.\\
Sur $\C$, les polynômes irréductibles sont les polynômes de degré inférieur strictement à 2.

\newpage

\section{Factorisation}
\subsection{Factorisation avec des polynômes irréductibles}
\subsubsection{Théorème}
Tout polynôme non constant $A\in \K[X]$ s'écrit comme un produit de polynômes irréductibles unitaires.\\
$A=\lambda \cdot P_{i}{}^{k_{i}}\cdot...\cdot P_{p}{}^{k_{p}}$ où $\lambda\in \K^{*}$, $p\in \N^{*}$ et les $P_{i}$ sont des polynômes irréductibles unitaires distincts. De plus, cette décomposition est unique à l'ordre des facteurs près.

\subsubsection{Remarque}
Il s'agit de l'analogue à la décomposition d'un entier en facteurs premiers.

\subsection{Factorisation dans $\C$}
\subsubsection{Théorème}
Les polynômes irréductibles de $\C[X]$ sont les polynômes de degré 1. Donc pour $P\in \C[X]$ de degré $n\geq 1$, la factorisation s'écrit $P=\lambda (X-\alpha_{1})^{k_{1}}...(X-\alpha_{p})^{k_{p}}$ où $\lambda\in \K^{*}$, $\alpha_{1},...,\alpha_{p}$ sont les racines distinctes de P et $k_{1},...,k_{p}$ leurs multiplicité.

\subsubsection{Démonstration}
Ce théorème résulte du théorème de d'Alembert-Gauss.

\subsection{Factorisation dans $\R$}
\subsubsection{Théorème}
Soit $P\in \R[X]$ de degré $n\geq 1$, alors la factorisation s'écrit $P=\lambda(X-\alpha_{})^{k_{1}}...(X-\alpha_{p})^{k_{p}}Q_{1}{}^{l_{1}}...Q_{s}{}^{l_{s}}$
où les $\alpha_{i}$ sont exactement les racines réelles distinctes de P et les $Q_{i}$ sont des polynômes irréductibles de degré 2: $Q=X^{2}+\beta_{i}X+\gamma_{i}$ avec $\beta_{i}{}^{2}-4\gamma_{i}<0$.

\subsubsection{Démonstration}
Dans $\C[X]$, P se décompose en
$P=\lambda (X-\alpha_{i})^{k_{i}}...(X-\alpha_{r})^{k_{r}}(X-\delta_{1})^{l_{1}}(X-\overline{\delta}_{1})^{l_{1}}... (X-\delta_{s})^{l_{s}}(X-\overline{\delta}_{s})^{l_{s}}$
où $\alpha_{i}$ sont les racines réelles distinctes et $\delta_{i}$,$\overline{\delta}_{i}$ les paires de racines complexes conjuguées distinctes.

\subsection{Exemples}
\begin{enumerate}
\item Soit $P=2X^{4}(X-1)^{3}(X^{2}+1)^{2}(X^{2}+X+1)$.\\
Il est déjà décomposé en facteurs irréductibles dans $\R[X]$.\\
Sa décomposition dans $\C[X]$ est $P=2X^{4}(X-1)^{3}(X-i)^{2}(X+i)^{2}(X-j)(X-\overline{j})$ où $j=e^{\frac{2i\pi}{3}}$.\\

\item Soit $P=X^{4}+1$.\\
Sur $\C$ on peut d'abord décomposer $P=(X^{2}-i)(X^{2}+i)$, les racines de P sont donc les racines carrées complexes de $i$ et $-i$ : ainsi P se factorise dans $\C[X]$ en\\  $P=(X-\frac{\sqrt{2}}{2}(1+i))(X+\frac{\sqrt{2}}{2}(1+i))(X-\frac{\sqrt{2}}{2}(1-i))(X+\frac{\sqrt{2}}{2}(1-i))$.\\

Sur $\R$ on regroupe les facteurs de la décomposition sur $\C$ ayant des racines conjuguées. Cela doit conduire à un polynôme à coefficients réels :\\
$P=(X-\frac{\sqrt{2}}{2}(1+i))(X+\frac{\sqrt{2}}{2}(1+i))(X-\frac{\sqrt{2}}{2}(1-i))(X+\frac{\sqrt{2}}{2}(1-i))$\\
\hspace*{0.35cm}$=(X^{2}+\sqrt{2}X+1)(X^{2}-\sqrt{2}X+1)=(X^{2}+1)^{2}-2X^{2}$.
\end{enumerate}

\section{Fractions rationnelles}
\subsection{Généralités}
\subsubsection{Définition}
Une fraction rationnelle à coefficient dans $\K$ est une expression de la forme $F=\frac{P}{Q}$ où P et Q sont des polynômes et $Q\neq 0$.

\subsubsection{Remarque}
Toute fraction rationnelle se décompose comme une somme de fractions rationnelles élémentaires que l'on appelle "éléments simples".
Les éléments simples sont différents sur $\R$ et sur $\C$.

\subsection{Décomposition en éléments simples sur $\C$}
\subsubsection{Théorème}
Soit $\frac{P}{Q}$ une fraction rationnelle avec $Q=(X-\alpha_{1})^{k_{1}}...(X-\alpha_{r})^{k_{r}}$ premier avec P, P et Q$\in \C[X]^{2}$.\\

Alors il existe une et une seule écriture de $\frac{P}{Q}$ telle que :\\
$$\frac{P}{Q}=E+\sum\limits_{i=1}^p\sum\limits_{j=1}^{k_i}
\frac{a_{i,j}}{(X-\alpha_i)^j}$$

Ici E est un polynôme appelé partie entière, les $a_{i,j}$ sont des nombres complexes, les termes $\frac{a}{(X-\alpha)^{i}}$ sont les éléments simples sur $\C$.

\subsubsection{Exemples}
$\frac{1}{X^{2}+1}=\frac{\frac{1}{2}i}{X-i}-\frac{\frac{1}{2}i}{X+i}$\\

$\frac{X^{4}-8X^{2}+9X-7}{(X-2)^{2}(X+3)}=X+1+\frac{-1}{(X-2)^{2}}+\frac{2}{X-2}+\frac{-1}{X+3}$

\subsubsection{Calcul de la décomposition}
On commence par déterminer la partie entière : si $deg(Q)>deg(P)$, $E=0$, sinon on effectue la division euclidienne de P par Q :
$P=QE+R$ donc $\frac{P}{Q}=E+\frac{R}{Q}$, où $deg(R)<deg(Q)$.\\

La partie entière est le quotient de cette division. On s'est ainsi ramené au cas d'une\\
fraction $\frac{R}{Q}$ où $deg(Q)>deg(R)$.\\

Il faut maintenant factoriser Q (pour trouver les racines). On sait par théorème qu'il existe une décomposition en éléments simples, on a donc $\frac{P}{Q}=E+\frac{a_{1}}{X-\alpha_{1}}^{k_{1}}...$ où les inconnues complexes sont les $a_{i}$, les $\alpha_{i}$ sont les racines et les $k_{i}$ leur multiplicité.\\\\

\emph{Exemple :}\\

$\frac{X^{5}-2X^{3}+4X^{2}-8X+11}{X^{3}-3X+2}=X^{2}+1+\frac{2X^{2}-5X+9}{X^{3}-3X+2}=\frac{2X^{2}-5X+9}{(X-1)^{2}(X+2)}\\
=X^{2}+1+\frac{a}{(X-1)^{2}}+\frac{b}{X-1}+\frac{c}{X+2}=X^{2}+1+\frac{(b+c)X^{2}+(a+b-2c)X+2a-2b+c}{(X-1)^{2}(X+2)}$\\

Par identification on a $\left\{\begin{array}{rcl}
b+c&=&2\\
a+b-2c&=&-5\\
2a-2b+c&=&9
\end{array} \right.$ et donc a=2, b=-1 et c=3.\\\\

D'où $\frac{X^{5}-2X^{3}+4X^{2}-8X+11}{X^{3}-3X+2}=X^{2}+1+\frac{2}{(X-1)^{2}}+\frac{-1}{X-1}+\frac{3}{X+2}$.

\subsection{Décomposition en éléments simples sur $\R$}
\subsubsection{Théorème}
Soit $\frac{P}{Q}$ une fraction rationnelle où $P,Q \in \R[X]^{2}$ sont premiers entre eux.\\

Alors $\frac{P}{Q}$ s'écrit d'une manière unique comme somme d'une partie entière E(X), d'éléments simples $\frac{a}{(X-\alpha)^{i}}$ et $\frac{aX+b}{(X^{2}+\alpha X+\beta)^{i}}$ où les $X-\alpha$ et $X^{2}+\alpha X+\beta$ sont les facteurs irréductibles de Q[X] et les exposants i sont inférieurs ou égaux à la puissance correspondante dans cette factorisation.\\

On a ainsi : $$\frac{P}{Q}=E+\sum\limits_{i=1}^p\sum\limits_{j=1}^{k_i}
\frac{\rho_{i,j}}{(X-r_i)^j}+\sum\limits_{i=1}^{q}\sum\limits_{j=1}^{n_i} \frac{\gamma_{i,j}X+\mu_{i,j}}{(X^2+a_iX+b_i)^j}$$

\subsubsection{Calcul de la décomposition}
Le principe du calcul de la décomposition en éléments simples sur $\R$ d'un polynôme est le même que sur $\C$.

\subsubsection{Exemple :}
Soit $\frac{P}{Q}=\frac{3X^{4}+5X^{3}+8X^{2}+5X+3}{(X^{2}+X+1)^{2}(X-1)}$.\\

Comme $deg(P)<deg(Q)$, on a E(X)=0. Le dénominateur est déjà factorisé dans $\R[X]$ car $X^{2}+X+1$ est irréductible.\\

La décomposition théorique est donc $\frac{P}{Q}=\frac{aX+b}{(X^{2}+X+1)^{2}}+\frac{cX+d}{X^{2}+X+1}+\frac{e}{X-1}$.\\

Il faut ensuite mener au calcul pour déterminer les coefficients afin d'obtenir\\
$\frac{P}{Q}=\frac{2X+1}{(X^{2}+X+1)^{2}}+\frac{-1}{X^{2}+X+1}+\frac{3}{X-1}$.


% ------------------------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------------------------ %



% ------------------------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------------------------ %


\chapter{Matrices}
\section{Généralités}
\subsection{Définition}
Une matrice A est un tableau rectangulaire d'éléments de $\K$. Elle est dite de taille (ou format) $n\times p$ si le tableau possède n lignes et p colonnes. Les éléments du tableau sont appelés les coefficients de A. Le coefficient situé à la i-ième ligne et à la j-ième colonne est noté $a_{i,j}$.

\subsection{Notation et représentation}
Un tel tableau est représenté de la manière suivante :\\

$A=\begin{pmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,p} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n,1} & a_{n,2} & \cdots & a_{n,p}
\end{pmatrix}$ ou $A=(a_{i,j})_{\substack{1\leq i \leq n \\ 1\leq j\leq p}}$ ou encore $A=(a_{i,j})$.\\\\

L'ensemble des matrices de taille $n\times p$ à coefficients dans $\K$ est noté $M_{n,p}(\K)$.

\begin{wrapfigure}[0]{r}{5cm} \begin{tikzpicture}
\node at (0,0) {$\phantom{A=}\begin{pmatrix} a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\ a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n,1} & a_{n,2} & \cdots & a_{n,n} \end{pmatrix}$} ;
\draw[rotate=62.5] (0.14,-0.325)ellipse(0.275 and 2) ;
\end{tikzpicture} \end{wrapfigure}

\subsection{Exemples de matrices}
\subsubsection{Matrice carrée}
Une matrice de dimension $n\times p$ est dite carrée si $n=p$.\\
On note $M_{n}(\K)$ l'ensemble des matrices carrées de taille $n\times n$.

Les coefficients $a_{i,i}$ forment la diagonale de la matrice carrée.

\subsubsection{Matrice ligne}
Une matrice n'ayant qu'une seule ligne est appelée matrice ligne ou vecteur ligne.\\
On la note $A=\begin{pmatrix} a_{1,1} & ... & a_{1,n} \end{pmatrix}$.

\subsubsection{Matrice colonne}
Une matrice colonne (ou vecteur colonne) ne possède qu'une colonne et est noté $\begin{pmatrix}
a_{1,1} \\
\vdots\\
a_{p,1} \end{pmatrix}$.

\subsubsection{Matrice nulle}
La matrice de taille $n\times p$ dont tous les coefficients sont nuls est appelée matrice nulle et notée $0_{n,p}$ ou simplement 0.

\section{Opérations sur les matrices}
\subsection{Égalité}
Deux matrices A et B sont égales si elles sont de mêmes dimensions et que $(A_{i,j}=B_{i,j})_{\substack{1\leq i\leq n \\ 1\leq j\leq p}}$.

\subsection{Somme}
\subsubsection{Énoncé}
Soient A et B deux matrices ayant la même taille $n\times p$, leurs somme $C=A+B$ est la matrice de taille $n\times p$ définie par $C_{i,j}=A_{i,j}+B_{i,j}$.

\subsubsection{Propriétés}
Soient $A,B,C \in M_{n,p}(\K)$, on a $(A+B)+C=A+(B+C)$, $A+B=B+A$ et $A+0_{n,p}=A$.

\subsection{Produit par un scalaire}
\subsubsection{Énoncé}
Le produit d'une matrice $A\in M_{n,p}(\K)$ par un scalaire $\alpha \in \K$ est la matrice $\alpha A=(\alpha A_{i,j})$.

\subsubsection{Opposé}
La matrice $(-1)A$ est l'opposée de A et est notée -A.

La différence $A-B$ est définie par $A+(-B)$.

\subsubsection{Propriétés}
Soient $A,B\in M_{n,p}(\K)$ et $\alpha, \beta \in \K$, on a $(\alpha+\beta)A=\alpha A+\beta A$, $\alpha(A+B)=\alpha A+\alpha B$.

\subsection{Produit de matrices}
\subsubsection{Remarque}
Le produit AB de deux matrices A et B est défini si et seulement si le nombre de colonnes\\
de A est égal au nombre de lignes de B.

\subsubsection{Énoncé}
Soient $A=(a_{i,j})$ une matrice $n\times p$ et $B=(b_{i,j})$ une matrice $p\times q$ alors le produit $C=AB$ est une matrice $n\times q$ de coefficients $c_{i,j}=\sum\limits_{k=1}^{p} a_{i,k}b_{k,j}$.

\subsubsection{Remarques}
Le produit de matrices n'est pas commutatif (la plupart du temps $AB\neq BA$).

On peut avoir $AB=0$ et A et B tous deux non nuls.

AB=AC n'implique pas B=C (même si $A\neq 0$).

\subsubsection{Exemple}
Soient $A=\begin{pmatrix}
1 & 2 & 3\\ 2 & 3 & 4
\end{pmatrix}$ et $B=\begin{pmatrix}
1 & 2 \\
-1 & 1 \\
1 & 1\\
\end{pmatrix}$.\\\\

$C=AB=\begin{pmatrix}
1-2+3 & 2+2+3 \\
2-3+4 & 4+3+4
\end{pmatrix}=\begin{pmatrix}
2 & 7 \\ 3 & 11
\end{pmatrix}$

\newpage

\subsection{Transposée d'une matrice}
\subsubsection{Définition}
Soit $A=(a_{i,j})_{\substack{1 \leq i \leq n \\ 1\leq j \leq p}}$ une matrice de $M_{n,p}(\K)$. Alors ${}^{t}A={}^{t}A=(a_{j,i})_{\substack{1 \leq j \leq p \\ 1\leq i \leq n}}\in M_{p,n}(\K)$ est la transposée de A.

\subsubsection{Proposition}
Soient A et B deux matrices de tailles respectives $n\times p$ et $p\times q$, alors
\begin{enumerate} \item ${}^{t}(AB)={}^{t}B$ ${}^{t}A$.
\item si $A\in Gl_n(\K)$ alors $({}^tA)^{-1}={}^t(A^{-1})$.
\end{enumerate}

\subsubsection{Démonstration}
\begin{enumerate} \item  ${}^{t}c_{i,j}=c_{j,i}=\sum\limits_{k=1}^{p}a_{j,k}b_{k,i}=\sum\limits_{k=1}^{p}{}^{t}a_{k,j}{}^{t}b_{i,k}=\sum\limits_{k=1}^{p}{}^{t}b_{i,k}{}^{t}a_{k,j}$
\item On a $AA^{-1}=I_n=A^{-1}A$.\\
En transposant on trouve ${}^t(A^{-1}){}^{t}A={}^t(AA^{-1})={}^{t}I_n={}^t(A^{-1}A)={}^tA{}^t(A^{-1})$.\\

Ce qui signifie que ${}^t(A^{-1})$ est l'inverse de ${}^tA$.

\end{enumerate}

\subsection{Proposition}
\subsubsection{Énoncé}
\begin{enumerate} \item $(AB)C=A(BC)$.
\item $A(B+C)=AB+AC$ et $(B+C)A=BA+CA$. \end{enumerate}

\subsubsection{Démonstration}
Soient A, B et C des matrices de taille $n\times p$, $p\times q$ et $q\times r$.\\

\begin{enumerate} \item Prouvons que $A(BC)=(AB)C$ en montrant que les matrices ont les mêmes coefficients.\\

Le terme d'indice $(i,j)$ de la matrice $AB$ est $x_{i,j}=\sum\limits_{l=i}^{p}a_{i,l}b_{l,k}$.\\
Le terme d'indice $(i,j)$ de ma matrice $(AB)C$ est $\sum\limits_{k=1}^{q}x_{i,k}c_{k,j}=\sum\limits_{k=1}^{q}(\sum\limits_{l=1}^{p}a_{i,l}b_{l,k})c_{k,j}$.\\
Le terme d'indice $(l,j)$ de la matrice $BC$ est $y_{l,j}=\sum\limits_{k=1}^{q}b_{l,k}c_{k,j}$.\\
Le terme d'indice $(i,j)$ de la matrice $A(BC)$ est donc $\sum\limits_{l=1}^{p}(a_{i,l}\sum\limits_{k=1}^{q}(b_{l,k}c_{k,j})$.\\\\

En comparant , on obtient l'égalité recherchée.\\

\item $A(B+C)=(\sum\limits_{k=0}^p a_{i,k}(b_{k,j}+c_{k,j}))_{\substack{1\leq i\leq n\\ 1\leq j\leq q}}= \left[\sum\limits_{k=0}^p (a_{i,k}b_{k,j}+a_{i,k}c_{k,j})\right]_{\substack{1\leq i\leq n\\ 1\leq j\leq q}}= AB+AC$.\\

De même, (A+B)C=AC+BC.

\end{enumerate}

\newpage

\section{Matrice identité}
\subsection{Définition}
On appelle matrice identité la matrice dont les éléments valent 1 s'ils appartiennent\\
à la diagonale et 0 sinon.\\

La matrice identité de taille $n\times n$ est $I_{n}=\begin{pmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{pmatrix}$.

\subsection{Symbole de Kronecker}
On peut formaliser la matrice identité en introduisant le symbole de Kronecker:\\

Si i et j sont deux entiers, on note $\delta_{i,j}=\left\{ \begin{array}{rcl}
0\text{ si }i\neq j \\
1\text{ si }i=j \end{array}\right.$,

alors le coefficient général de $I_{p}$ est $\delta_{i,j}$ où $i,j\in \{1,...,p\}$.

\subsection{Proposition}
\subsubsection{Énoncé}
Si A est une matrice $n\times p$, on a $I_{n}\times A=A=A\times I_{p}$.

\subsubsection{Démonstration}
La matrice produit $AI_{p}$ est une matrice de $M_{n,p}(\K)$ dont le terme général est donné par la formule $c_{i,j}=\sum\limits_{k=1}^{p} a_{i,k}\delta_{k,j}$.\\

Si $k\neq j$, on a $\delta_{k,j}=0$ et si $k=j$ alors $\delta_{k,j}=1$, donc dans la somme qui définit $c_{i,j}$ tous les termes correspondant à des valeurs de k différentes de j sont nuls et il reste donc $c_{i,j}=a_{i,j}\delta_{j,j}=a_{i,j}$.\\

Donc les matrices $AI_{p}$ et A ont le même terme général et sont donc égales, de même on montre que $I_{n}A=A$.

\newpage

\section{Puissance d'une matrice}
\subsection{Définition}
Pour tout $A\in M_{n}(\K)$, on définit les puissances successives de A par $A^{0}=I_{n}$ et $A^{p+1}=A\times A^{p}$ pour tout $p\in \N$.\\
Autrement dit $A^{p}=\underbrace{A\times ...\times A}_{\text{p facteurs}}$.

\subsection{Formule du binôme}
\subsubsection{Remarque}
Comme la multiplication n'est pas commutative, les identités binomiales usuelles sont fausses. En particulier $(A+B)^{2}$ ne vaut pas $A^{2}+2AB+B^{2}$ en général mais on sait seulement que $(A+B)^{2}=A^{2}+AB+BA+B^{2}$.

\subsubsection{Proposition}
Soient A et B deux éléments de $M_{n}(\K)$ qui commutent, c'est-à-dire tels que AB=BA, alors pour tout entier $p\geq 0$, on a la formule $(A+B)^{p}=\sum\limits_{k=0}^{p}\binom{p}{k}A^{p-k}B^{k}$.

\subsection{Matrice nilpotente}
\subsubsection{Définition}
Une matrice $A \in M_{n}(\K)$ est dite nilpotente si et seulement si il existe $k\in \N$ tel que $A^{k}=0$.

\subsubsection{Exemples}
Toute matrice de la forme $A=\begin{pmatrix}
0 & a_{1,2} & \cdots & a_{1,n-1} & a_{1,n} \\
0 & 0 & \cdots & a_{2,n-1} & a_{2,n} \\
\vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & \cdots & 0 & a_{n-1,n}\\
0 & 0 & \cdots & 0 & 0 \end{pmatrix}$\\

(où tous les coefficients de la diagonale ou en dessous sont nuls) est nilpotente.

\newpage

\section{Inverse d'une matrice}
\subsection{Définition}
Soit A une matrice carrée de taille $n\times n$. S'il existe une matrice carrée B de taille $n\times n$ telle que $AB=I_{n}=BA$, on dit que A est inversible et que B est la matrice inverse de A (et vice versa).\\

L'ensemble des matrices inversibles dans $M_{n}(\K)$ est noté $Gl_{n}(\K)$.

\subsubsection{Remarque}
Il suffit en fait de vérifier une seule des deux conditions $AB=I_{n}$ et $BA=I_{n}$.

\subsection{Proposition}
\begin{enumerate}
\item Si $A$ est inversible, son inverse est unique. On le note $A^{-1}$.\\
\item Soient A et B deux matrices inversibles de même taille, alors AB est inversible et $(AB)^{-1}=B^{-1}A^{-1}$.\\
\item Soient $A,B\in M_{n}(\K)$ et $C\in Gl_{p}(\K)$ alors l'égalité $AC=BC$ implique $A=B$.\\
\item Corollaire du 2.\\
Si $A_{1},...,A_{m}$ sont des matrices inversibles de taille $n\times n$ alors $\prod\limits_{i=1}^{m}A_{i}$ est inversible\\
et d'inverse $\prod\limits_{i=m}^{1}A_{i}^{-1}$.
\end{enumerate}

\subsubsection{Démonstration}
\begin{enumerate} \item Soient $A$, $B_{1}$ et $B_{2}$ trois matrices $n\times n$ telles que $AB_{1}=I_{n}=B_{1}A$ et $A_{B_{2}}=I_{n}=B_{2}A$.\\
On a alors $B_{1}=B_{1}\times I_{n}=B_{1}(AB_{2})=(B_{1}A)B_{2}=B_{2}$.\\

\item On a $(AB)(B^{-1}A^{-1})=A(BB^{-1})A^{-1}=AI_{n}A^{-1}=I_{n}$ et\\
$(B^{-1}A^{-1})(AB)=B^{-1}(A^{-1}A)B=B^{-1}I_{n}B=I_{n}$.\\

\item On multiplie à droite par $C^{-1}$.

\end{enumerate}

\subsection{Puissances de la matrice inverse}
Quand A est inversible, pour tout $p\in \N$, on note $A^{-p}=(A^{-1})^{p}=A^{-p}=\underbrace{A^{-1}\times ...\times A^{-1}}_{\text{p facteurs}}$.

\newpage

\section{Calcul de la matrice inverse}
\subsection{Matrices 2$\times$ 2}
\subsubsection{Proposition}
Soit $A=\begin{pmatrix} a & b \\ c & d \end{pmatrix}$
Si $\underbrace{ad-bc}_{det(A)}\neq 0$, alors A est inversible d'inverse $\frac{1}{ad-bc}\begin{pmatrix} d & -b \\ -c & a \end{pmatrix}$.

\subsubsection{Démonstration}

$\begin{pmatrix}
a & b\\ c & d
\end{pmatrix}\frac{1}{ad-bc} \begin{pmatrix}
ad-bc & 0 \\ 0 & -bc+ad
\end{pmatrix}=I_{2}=\frac{1}{ad-bc}\begin{pmatrix}
d & -b \\ -c & a
\end{pmatrix}\begin{pmatrix}
a & b \\ c & d
\end{pmatrix}$

\subsection{Méthode de gauss pour inverser une matrice}
\subsubsection{Opérations élémentaires}
Rappelons que les opérations élémentaires sur les lignes d'une matrice sont les suivantes :\begin{enumerate}
\item multiplier une ligne par un scalaire non nul.
\item rajouter un multiple d'une ligne à une autre ligne.
\item échanger deux lignes.
\end{enumerate}

\subsubsection{Théorème}
Soit $A\in M_{n}(\K)$. Si on peut transformer la matrice augmentée $[A|I_{n}]$ par des opérations élémentaires sur les lignes d'une matrice en une matrice $[I_{n}|B]$ alors A est inversible et $A^{-1}=B$.\\

\emph{Réciproquement :} Si A est inversible, on peut transformer $[A|I_{n}]$ en $[I_{n}|A^{-1}]$ par des opérations élémentaires sur les lignes.

\subsubsection{Démonstration}
Voir page \pageref{InverseMatrice}.

\subsubsection{Exemple}
Soit $A=\begin{pmatrix}
1 & 2 & 1 \\
4 & 0 & -1 \\
-1 & 2 & 2
\end{pmatrix}$

$[A|I_{n}]=\left(\begin{matrix} 1 & 2 & 1 \\ 4 & 0 & -1 \\ -1 & 2 & 2 \end{matrix} \left| \begin{matrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{matrix}\right) \right.
\mapsto\left(\begin{matrix} 1 & 2 & 1 \\ 0 & -8 & -5 \\ 0 & 4 & 3 \end{matrix} \left| \begin{matrix} 1 & 0 & 0 \\ -4 & 1 & 0 \\ 1 & 0 & 1 \end{matrix}\right) \right.
\hspace*{0.1cm}\mapsto\left(\begin{matrix} 1 & 2 & 1 \\ 0 & 1 & \frac{5}{8} \\ 0 & 4 & 3 \end{matrix} \left| \begin{matrix} 1 & 0 & 0 \\ \frac{1}{2} & -\frac{1}{8} & 0 \\ 1 & 0 & 1\end{matrix}\right) \right.\\
\hspace*{5.75cm}\mapsto\left(\begin{matrix} 1 & 2 & 1 \\ 0 & 1 & \frac{5}{8} \\ 0 & 0 & \frac{1}{2} \end{matrix} \left| \begin{matrix} 1 & 0 & 0 \\ \frac{1}{2} & -\frac{1}{8} & 0 \\ -1 & \frac{1}{2} & 1\end{matrix}\right) \right.
\hspace*{0.26cm}\mapsto\left(\begin{matrix} 1 & 2 & 1 \\ 0 & 1 & \frac{5}{8} \\ 0 & 0 & 1 \end{matrix} \left| \begin{matrix} 1 & 0 & 0 \\ \frac{1}{2} & -\frac{1}{8} & 0 \\ -2 & 1 & 2\end{matrix}\right) \right.\\
\hspace*{5.75cm}\mapsto\left(\begin{matrix} 1 & 0 & -\frac{1}{4} \\ 0 & 1 & \frac{5}{8} \\ 0 & 0 & 1 \end{matrix} \left| \begin{matrix} 0 & \frac{1}{4} & 0 \\ \frac{1}{2} & -\frac{1}{8} & 0 \\ -2 & 1 & 2\end{matrix}\right) \right.
\mapsto\left(\begin{matrix} 1 & 0 & 0 \\ 0 & 1 & \frac{5}{8} \\ 0 & 0 & 1 \end{matrix} \left| \begin{matrix} -\frac{1}{2} & \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & -\frac{1}{8} & 0 \\ -2 & 1 & 2\end{matrix}\right) \right.\\
\hspace*{5.75cm}\mapsto\left(\begin{matrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{matrix} \left| \begin{matrix} -\frac{1}{2} & \frac{1}{2} & \frac{1}{2} \\ \frac{7}{4} & -\frac{3}{4} & -\frac{5}{4} \\ -2 & 1 & 2\end{matrix}\right) \right.$\\

Donc $A^{-1}=\frac{1}{4}\begin{pmatrix} -2 & 2 & 2 \\ 7 & -3 & -5 \\ -8 & 4 & 8 \end{pmatrix}$.



\section{Systèmes linéaires et matrices élémentaires}
\subsection{Matrices et systèmes linéaires}
Considérons le système linéaire $\left\{\begin{matrix}
a_{1,1}x_1 +a_{1,2}x_2+\cdots+a_{1,p}x_p=b_1 \\
a_{2,1}x_1 +a_{2,2}x_2+\cdots+a_{2,p}x_p=b_2 \\
\hspace*{0.2cm}\vdots \hspace*{1.2cm} \vdots \hspace*{0.9cm} \ddots \hspace*{0.8cm} \vdots \hspace*{0.9cm} \vdots \\
a_{n,1}x_1 +a_{n,2}x_2+\cdots+a_{n,p}x_p=b_n
\end{matrix}\right.$

\bigskip

Ce système peut s'écrire sous forme matricielle $\underbrace{\begin{pmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,p}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,p}\\
\vdots & \vdots & \ddots & \vdots \\
a_{n,1} & a_{n,2} & \cdots & a_{n,p}
\end{pmatrix}}_{A}
\underbrace{\begin{pmatrix}
x_1 \\ x_2 \\ \vdots \\ x_p
\end{pmatrix}}_{X}
= \underbrace{\begin{pmatrix}
b_1 \\ b_2 \\ \vdots \\ b_n
\end{pmatrix}}_{B}$ \\\\

On appelle $A\in M_{n,p}(\K)$ la matrice des coefficients du systèmes, $B\in M_{n,1}(\K)$ le vecteur du second membre et $X\in M_{n,1}(\K)$ est solution du système si et seulement si $AX=B$.

\subsubsection{Rappel}
Un système d'équations linéaires possède soit une infinité de solutions, soit une seule, soit aucune.

\subsection{Matrices inversibles et systèmes linéaires}
Considérons le cas où le nombres d'équations est égal au nombre d'inconnues :
\begin{center} $\begin{pmatrix}
a_{1,1} & \cdots & a_{1,n} \\
\vdots & & \vdots \\
a_{n,1} & \cdots & a_{n,n}
\end{pmatrix} \begin{pmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{pmatrix}
= \begin{pmatrix}
b_1 \\ b_2 \\ \vdots \\ b_n
\end{pmatrix}$
ou $AX=B$. \end{center}

Alors $A\in M_n(\K)$ est une matrice carrée et B un vecteur de $M_{n,1}(\K)$.

\subsubsection{Proposition}
Si la matrice A est inversible alors le système AX=B admet pour unique solution $X=A^{-1}B$.

\subsubsection{Démonstration}
Si $X=A^{-1}B$, alors $AX=AA^{-1}B=B$. Réciproquement si X est tel que $AX=B$ alors $X=A^{-1}B$.

\newpage

\subsection{Les matrices élémentaires}
Rappelons les opérations élémentaires sur les lignes d'une matrice:\\
\begin{enumerate}
\item Multiplier une ligne $L_i$ par un scalaire non nul $\lambda$.
\item Rajouter à une ligne $L_i$ un multiple $aL_j$ d'une autre ligne.
\item Échanger deux lignes $L_i$ et $L_j$.
\end{enumerate}

\bigskip

Nous allons définir trois matrices élémentaires $D(i,\lambda)$, $E(i,j,a)$ et $P(i,j)$ correspondant à ces opérations. C'est-à-dire qu'effectuer le produit d'une de ces matrices par A correspondra à effectuer l'opération correspondante (respectivement multiplier par un réel, ajouter a fois une ligne à une autre et échanger deux lignes).\\

\begin{enumerate}
\item $D(i,\lambda)\in M_n(\K)$ :

\begin{tikzpicture}
\draw[densely dashed, very thin] (-1.4,0) -- (2.95,0) ;

\fill[color=white] (0.58,-0.25) -- (0.95,-0.25) -- (0.95,0.25) -- (0.58,0.25) -- cycle ;

\node at (0,0) {$D(i,\lambda)=\begin{pmatrix}
1 & & &&&&\\
& \ddots & & & & & \\
&& 1 &&&& \\
&&& \lambda &&& \\
&&&& 1 && \\
&&&&& \ddots & \\
&&&&&& 1
\end{pmatrix}$ } ;
\node[scale=0.8] at (3.75,0) {i-ème ligne} ;
\end{tikzpicture}

\item $E(i,j,a)\in M_n(\K)$ :

\begin{tikzpicture}
\draw[very thin, dashed] (-0.85,0.53) -- (2.65,0.53);
\draw[very thin, dashed] (1.65,-1.5) -- (1.65,1.5);

\fill[color=white] (1.4,0.38) -- (1.8,0.38) -- (1.8,0.78) -- (1.4,0.78) -- cycle ;

\node at (0,0) {$E(i,j,a)=\begin{pmatrix}
1 & & & & \\
& \ddots & & a & \\
&& \ddots && \\
&&&\ddots & \\
&&&& 1
\end{pmatrix}$};

\node[scale=0.8] at (1.75,1.7) {j-ième colonne};
\node[scale=0.8] at (3.45,0.53) {i-ième ligne};
\end{tikzpicture}

\item $P(i,j)\in M_n(\K)$ :

\begin{tikzpicture}
\draw[dashed, very thin] (-3.35,1) -- (3.4,1);
\draw[dashed, very thin] (-3.35,-0.9) -- (3.4,-0.9);

\draw[dashed, very thin] (-1.15,-2.8) -- (-1.15,2.8);
\draw[dashed, very thin] (1.15,-2.8) -- (1.15,2.8);

\fill[color=white] (-1.275,-1.12) -- (-1.1,-1.12) -- (-1.1,-0.65) -- (-1.275,-0.65) -- cycle ;
\fill[color=white] (1.025,0.78) -- (1.2,0.78) -- (1.2,1.25) -- (1.025,1.25) -- cycle ;
\fill[color=white] (-1.275,0.78) -- (-1.1,0.78) -- (-1.1,1.25) -- (-1.275,1.25) -- cycle ;
\fill[color=white] (1.025,-1.12) -- (1.2,-1.12) -- (1.2,-0.65) -- (1.025,-0.65) -- cycle ;

\node[scale=0.75] at (-1.15,3) {i-ème colonne} ;
\node[scale=0.75] at (1.15,3) {j-ème colonne} ;
\node[scale=0.75] at (4,-0.9) {j-ème ligne} ;
\node[scale=0.75] at (4,1) {i-ème ligne} ;

\node at (-0.74,0) {$P(i,j)=\begin{pmatrix}
1 & & & & & & & & & & \\
& \ddots & & & & & & & & & \\
& & 1 & & & & & & & & \\
& & & 0 & & & & 1 & & & \\
& & & & 1 & & & & & & \\
& & & & & \ddots & & & & & \\
& & & & & & 1 & & & & \\
& & & 1 & & & & 0 & & & \\
& & & & & & & & 1 & & \\
& & & & & & & & & \ddots & \\
& & & & & & & & & & 1
\end{pmatrix}$} ;
\end{tikzpicture} \end{enumerate}


\subsubsection{Proposition}
Soit $A\in M_{n,p}(\K)$
\begin{enumerate}
\item la matrice $D(i,\lambda)A$ est obtenue en multipliant par $\lambda$ la i-ème ligne de A.
\item la matrice $E(i,j,a)A$ est obtenue en rajoutant a fois la j-ème ligne de A à la i-ème ligne.
\item $P(i,j)A$ est obtenue en échangeant les i-ème et j-ème lignes de A.
\end{enumerate}

\section{Équivalence à une matrice échelonnée}
\subsection{Définitions}
\subsubsection{Matrices équivalentes}
Deux matrices A et B sont équivalentes par ligne si l'une peut être obtenue à partir de l'autre par des opérations élémentaires sur les lignes.

\subsubsection{Matrice échelonnée}
Une matrice est échelonnée si le nombre de zéros commençant une ligne croit strictement ligne par ligne jusqu'à ce qu'il ne reste que des 0.

\subsubsection{Matrice échelonnée réduite}
Une matrice est échelonnée réduite si en plus d'être échelonnée le premier coefficient d'une ligne non nulle vaut 1 et qu'il s'agit du seul coefficient non nul de sa colonne.

\subsubsection{Exemples}
On dit que + est un coefficient non nul et * un coefficient quelconque :\\\\

Matrice échelonnée : $\begin{pmatrix}
+ & * & * & * & * & * \\
0 & 0 & + & * & * & * \\
0 & 0 & 0 & + & * & *
\end{pmatrix}$\\\\

Matrice échelonnée réduite : $\begin{pmatrix}
1 & * & 0 & 0 & * & * \\
0 & 0 & 1 & 0 & * & * \\
0 & 0 & 0 & 1 & * & *
\end{pmatrix}$

\subsection{Unicité de la matrice échelonnée}
\subsubsection{Théorème}
Étant donnée une matrice $A\in M_{n,p}(\K)$, il existe une unique matrice échelonnée réduite obtenue à partir de A par des opérations élémentaires sur les lignes.

\subsubsection{Remarque}
On admet l'unicité, l'existence résulte de l'algorithme du pivot de Gauss.

\subsubsection{Exemple}
$\begin{pmatrix}
1 & 2 & 3 & 4\\
0 & 2 & 4 & 6\\
-2 & 0 & 1 & 0
\end{pmatrix}\mapsto
\begin{pmatrix} 1 & 2 & 3 & 4 \\
0 & 2 & 4 & 6 \\
0 & 2 & 4 & 4 \end{pmatrix}
\mapsto \begin{pmatrix}
1 & 2 & 3 & 4 \\
0 & 2 & 4 & 6 \\
0 & 0 & 0 & -2
\end{pmatrix}$ \\ \hspace*{7cm}(échelonnée) \\\\
\hspace*{3.15cm} $\mapsto \begin{pmatrix}
1 & 2 & 3 & 4\\
0 & 1 & 2 & 3\\
0 & 0 & 0 & 1
\end{pmatrix} \mapsto \begin{pmatrix}
1 & 0 & -1 & 0 \\
0 & 1 & 2 & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}$\\
\hspace*{6.3cm} (échelonnée réduite)

\newpage

\subsection{Inversibilité}
\subsubsection{Théorème}
Soit $A\in M_n(\K)$. La matrice A est inversible si et seulement si sa forme échelonnée est la matrice identité $I_n$.

\subsubsection{Démonstration}
Notons U la forme échelonnée réduite de A et notons E la matrice telle que U=EA.\\\\

Montrons que le fait qu'une matrice ait pour forme échelonnée la matrice identité implique que cette matrice est inversible :\\
Si $U=I_n$, alors $I_n=EA$ et donc A est inversible d'inverse E.\\\\

Montrons que si $U\neq I_n$ alors A n'est pas inversible (contraposée de l'autre implication) :\\

Si $U\neq I_n$ alors la dernière ligne de A est nulle (sinon il y aurait un pivot sur chaque ligne et U serait l'identité) et donc U n'est pas inversible : en effet pour toute matrice carrée V la dernière ligne de UV est nulle donc $UV\neq I_n$.\\

Alors A n'est pas inversible non plus : si A était inversible on aurait U=EA et U serait inversible car E est inversible en tant que produit de matrices élémentaires qui sont inversibles:\\
$D(i,\lambda)^{-1}=D(i,\lambda^{-1})$, $E(i,j,a)^{-1}=E(i,j,-a)$ et $P(i,j)^{-1}=P(i,j)$.

\subsubsection{Justification de la méthode de calcul de $A^{-1}$\label{InverseMatrice}}
Soit $A \in M_n(\K)$ une matrice inversible. On part de $[A|I_n]$ pour arriver à $[I_n|B]$ par des opérations élémentaires sur les lignes.\\
Effectuer une opération sur les lignes signifie multiplier à gauche par une matrice élémentaire.\\

Notons E le produit de ces matrices élémentaires. On a $E[A|I_n]=[EA|E]=[I_n|E]=[I_n|B]$.\\
Donc $EA=I_n$ et l’inverse de A est bien $B=E$.

\subsubsection{Corollaire}
Les assertions suivantes sont équivalentes :
\begin{enumerate}
\item la matrice $A\in M_n(\K)$ est inversible.
\item le système linéaire $AX=0$ admet pour unique solution $X=0$.
\item pour tout second membre B, le système $AX=B$ a une unique solution.
\end{enumerate}

\subsubsection{Démonstration}
\begin{wrapfigure}[6]{r}{5.3cm} $\begin{pmatrix}
1 & && & c_1 & * & \cdots & * \\
& 1 &&& c_2 & \vdots & & \vdots \\
&& \ddots && \vdots & \vdots && \vdots \\
&&& 1 & c_{l-1} & \vdots & & \vdots \\
&&&& 0 & \vdots & & \vdots \\
&&&&& \vdots & & \vdots \\
&&&&& * & \cdots & *
\end{pmatrix}$ \end{wrapfigure}
On a vu que 1. implique 3., l'implication 3. $\Rightarrow$ 2. est claire. Il reste à démontrer que 2. $\Rightarrow$ 1..\\\\
On raisonne par contraposée et on montre non 1. $\Rightarrow$ non 2..\bigskip

Si A n'est pas inversible alors sa forme échelonnée réduite U contient un premier zéro sur sa diagonale à la ligne l. \\

Alors U a la forme suivante :

\newpage

Posons $X=\begin{pmatrix}
-c_1 \\
-c_2 \\
\vdots \\
-c_{l-1} \\
1 \\
0 \\
\vdots \\
0
\end{pmatrix}$ alors $X\neq 0$ mais UX=0.\\

Comme U=EA et donc $A=E^{-1}U$ on a $AX=E^{-1}UX=0$.

\subsubsection{Corollaire}
Soit un système linéaire homogène AX=0 avec n inconnues et p équations.\\
Si n> p alors il existe une solution $X\neq 0$.

\subsubsection{Démonstration}
On peut poser A sous forme échelonnée réduite. Comme $n>p$, au moins l'un des échelons est plus long que 1.
Donc il existe des variables libres et on a une infinité de solutions.


% ------------------------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------------------------ %



% ------------------------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------------------------ %


\chapter{Algèbre linéaire}
\section{Rappels}
Voir le cours d'\textit{Algèbre et Analyse élémentaire I} au premier semestre, chapitre 3.

\section{Bases et matrices inversibles}
\subsection{Condition d'inversibilité}
Soit $A\in M_n(\K)$, alors $A$ est inversible si et seulement si ses colonnes forment une base de $\K^n$

\subsubsection{Démonstration}
$\Rightarrow$

Comme $A$ est inversible, le système homogène $AX=0$ ne possède que la solution nulle,\\
donc les seules solutions de $\sum\limits_{i=1}^n x_iA_i=0$ (où $A_i$ sont les colonnes de $A$) sont $\forall n\in\N$, $x_i=0$.\\

Donc les colonnes de $A$ sont linéairement indépendantes.\\
Comme elles sont au nombre de $n$ dans $\K^n$ qui est de dimension $n$, elles forment une base.\\

$\Leftarrow$

Pour $1\leq j\leq n$, écrivons $e_j=\sum\limits_{i=1}^nb_{i,j}A_i$ (ce qui est possible puisque les colonnes $A_i$ forment une base). La $k$-ème composante s'écrit $(e_j)_k=\sum\limits_{i=1}^n b_{i,j}a_{k,i}
\delta_{k,j}=\sum\limits_{i=1}^na_{k,i}b_{i,j}$.

\subsection{Proposition}
Soient $E$ un espace vectoriel, $(\beta_i)_{1\leq i\leq n}$ une base de $E$ et $(v_i)_{1\leq i\leq n}$ une famille de $E$ et $A$ la matrice dont la $j$-ème colonne contient les coordonnées de $v_j$ dans la base $\beta_i$ (donc on a $v_j=\sum\limits_{i=1}^n a_{i,j}\beta_i$).\\

Alors $A$ est inversible si et seulement si $(v_i)_{1\leq i \leq n}$ est une base de $E$.

\subsubsection{Remarque}
La proposition précédente est le cas particulier où $E=\K^n$ et $\beta_i$ est la base canonique.

\newpage

\subsubsection{Démonstration}
$\Rightarrow$

Supposons que $A$ est inversible, soit $(x_j)_{1\leq j\leq n}$ tels que $\sum\limits_{j=1}^n x_jv_j=0$ alors on a
$$0=\sum\limits_{j=1}^n x_jv_j=\sum\limits_{j=1}^nx_j \sum\limits_{i=1}^n a_{i,j}\beta_i=\sum\limits_{i=1}^n(\sum\limits_{j=1}^n a_{i,j}x_j)\beta_i\text{ et donc }AX=0\text{ où } X=\begin{pmatrix}x_1 \\ \vdots \\ x_n \end{pmatrix}$$
donc $X=0$ et les $v_i$ sont linéairement indépendants.\\

Comme ils sont au nombre de $n$ et que $\dim(E)=n$, $(v_i)_{1\leq i\leq n}$ forme une base de $E$.\\

$\Leftarrow$

Réciproquement, supposons que $v_i$ soit une base. Écrivons $\beta_j=\sum\limits_{i=1}^nb_{i,j}v_i$, $1\leq j\leq n$.\\
Alors on a $\beta_j=\sum\limits_{i=1}^n b_{i,j}v_i=\sum\limits_{i=1}^nb_{i,j}\sum\limits_{k=1}^n a_{k,i}\beta_k=\sum\limits_{k=1}^n(\sum\limits_{i=1}^n a_{k,i}b_{i,j})\beta_k$.\\
Il s'ensuit que $\delta_{k,j}=\sum\limits_{i=1}^n a_{k,i}b_{i,j}$, $1\leq k,j\leq n$
et donc $AB=I_n$.

\section{Opérations sur les colonnes}
Soient $A\in M_{n,p}(\K)$ et $(A_i)_{1\leq i\leq p}$ les colonnes de $A$.
\subsection{Définition}
Les opérations élémentaires sur les colonnes de $A$ sont les suivantes :
\begin{enumerate}
\item Multiplier une colonne par un scalaire non nul.
\item Rajouter à une colonne un multiple d'une autre colonne.
\item Échanger deux colonnes.
\end{enumerate}

\subsection{Image d'une matrice}
L'image de $A$, notée $Im(A)$, est le sous-espace vectoriel de $\K^n$ engendré par les colonnes de $A$.\\
$Im(A)=\{\sum\limits_{i=1}^px_iA_i | x_i \in \K\}=\{AX|X\in\K^p\}$.

\subsection{Rang d'une matrice}
Le rang de $A$ est $rg(A)=\dim(Im(A))$.

\subsection{Proposition}
Si $B$ est obtenue à partir de $A$ par des opérations élémentaires sur ses colonnes,\\
alors $Im(A)=Im(B)$ et donc $rg(A)=rg(B)$.

\subsubsection{Démonstration}
Il est clair que les opérations 1. et 3. ne changent pas $Vect((A_i)_{1\leq i\leq p})$.\\

Pour l'opération 2, supposons par exemple que $B$ est obtenue à partir de $A$ en rajoutant\\
$xA_2$ à $A_1$. On a alors
$$Im(B)=\{\lambda_1(A_1+xA_2)+\sum\limits\limits_{i=2}^p\lambda_iA_i | \lambda_i\in\K \}
=\{\lambda_1A_1+(\lambda_1x+\lambda_2)A_2+\sum\limits\limits_{i=3}^p\lambda_iA_i | \lambda_i\in\K \}=Im(A)$$

\newpage

\subsection{Matrice échelonnée (réduite) par rapport aux colonnes}
$A$ est échelonnée (réduite) par rapport aux colonnes si ${}^tA$ est échelonnée réduite par rapport aux lignes.

\subsection{Théorème}
Soit $A \in M_{n,p}(\K)$. Il existe une unique matrice échelonnée réduite par rapport aux colonnes obtenue à partir de $A$ par des opérations élémentaires sur les colonnes.

\subsubsection{Remarque}
On obtient l'existence en appliquant la transposée de l'algorithme de Gauss.

En particulier, on obtient un nouvel algorithme pour trouver un système d'équations pour un sous-espace de $\K^n$ donné par des générateurs.

\subsubsection{Proposition}
Si $B$ est obtenue à partir de $A$ par une opération élémentaires sur les colonnes alors $B=AE$, où $E$ est un produit des matrices élémentaires $D(i,\lambda)$, $E(i,j,x)$, $P(i,j)$.

\subsubsection{Remarque}
\begin{enumerate}
\item Notons qu'on utilise la multiplication à droite.
\item Il s'ensuit qu'il existe une matrice inversible $P$ telle que $A'=AP$ est échelonnée réduite par rapport aux colonnes. \end{enumerate}

\section{Opérations sur les lignes et les colonnes}
\subsection{Propriété}
Soit $A\in M_{n,p}(\K)$. Il existe des opérations élémentaires sur les lignes et les colonnes qui transforment $A$ en la matrice $\begin{pmatrix}
I_r & 0 \\ 0 & 0 \end{pmatrix}$, où $r=rg(A)$.

\subsubsection{Démonstration}
\begin{tikzpicture}
\node at (-0.5,1) {Par des opérations élémentaires sur les colonnes, on met $A$};
\node at (-0.63,0.5) {sous forme échelonnée réduite par rapport aux colonnes :};
\node at (7,0) {$\begin{pmatrix}
0&\cdots&\cdots&\cdots& 0\\
1&  &  &  & \\
*&  &  &  & \\
0& 1&  &  & \\
*& *&  &  & \\
*& *&  &  & \\
0& 0& 0& 1&
\end{pmatrix}$};
\node at (-1,-0.5) {Il est alors clair que par des opérations élémentaires};
\node at (-1.75,-1) {sur les lignes, on peut atteindre la matrice};
\node at (4,-0.75) {$\begin{pmatrix}
I_p&0\\
0&0
\end{pmatrix}$};
\end{tikzpicture}

\subsection{Corollaire}
Soit $A \in M_{n,p}$, alors il existe des matrices inversibles $P$ et $Q$\\
telles que $PAQ=\begin{pmatrix}
I_r & 0 \\ 0& 0
\end{pmatrix}$, où $r=rg(A)$.

\subsubsection{Démonstration}
Effectuer des opérations élémentaires correspond à multiplier à droite et à gauche par des matrices inversibles.

\subsection{Application}
\begin{enumerate}
\item Toute matrice carrée est somme de deux matrices inversibles.\\

En effet, Soit $A\in M_n(\mathbb{K})$,
$\begin{pmatrix}
I_p&0\\
0&0
\end{pmatrix}=\begin{pmatrix}
\frac{1}{2}I_p&0\\
0&-\frac{1}{2}I_s
\end{pmatrix}+\begin{pmatrix}
\frac{1}{2}I_p&0\\
0&\frac{1}{2}I_s
\end{pmatrix}$.\\

On a :
$A=P'\begin{pmatrix}
I_p&0\\
0&0
\end{pmatrix}Q'$
pour des matrices inversibles $P'$ et $Q'$.\\

Donc :
$A=P'\left(\begin{pmatrix}
\frac{1}{2}I_p&0\\
0&-\frac{1}{2}I_s
\end{pmatrix}+\begin{pmatrix}
\frac{1}{2}I_p&0\\
0&\frac{1}{2}I_s
\end{pmatrix}\right)Q'=P'\begin{pmatrix}
\frac{1}{2}I_p&0\\
0&-\frac{1}{2}I_s
\end{pmatrix}Q'+P'\begin{pmatrix}
\frac{1}{2}I_p&0\\
0&\frac{1}{2}I_s
\end{pmatrix}Q'$
et ces deux dernières matrices sont bien inversibles.\\

\item Toute matrice carrée est limite d'une suite de matrices inversibles :
$\begin{pmatrix}
I_p& 0\\
0& 0
\end{pmatrix}=\lim\limits_{n\rightarrow \infty}\begin{pmatrix}
I_p& 0\\
0&\frac{1}{n}I_s
\end{pmatrix}$
Soit $A\in M_n(\K)$. On a $A=P'\begin{pmatrix}
\frac{1}{2}I_p&0\\
0&-\frac{1}{2}I_s
\end{pmatrix}Q'$ pour des matrices inversibles P' et Q'.\\

Donc $A=\lim\limits_{n\rightarrow \infty}P'\begin{pmatrix}
I_p&0\\
0&\frac{1}{n}I_s
\end{pmatrix}Q'$.
\end{enumerate}

\subsection{Propriété}
Soit $A\in M_{n,p}(\mathbb{K})$. Si on a $PAQ=\begin{pmatrix}
I_s& 0\\
0& 0
\end{pmatrix}$ pour des matrices inversibles $P,Q$, alors $s=rg(A)$.

\subsubsection{Démonstration}
On a $Im(AQ)=Im(A)$ car $Im(AQ)=\{AQX|X\in \K^p\}=\{AQX|QX\in \K^p\}={AY|Y\in \K^p}$.\\

On a $\dim(Im(PAQ))=\dim(Im(AQ))$ car si $v_1,...,v_p$ est une base de $Im(AQ)$, on vérifie facilement que $P_{v_1},...,P_{v_p}$ est une base de $Im(PAQ)$.\\
Finalement $s=\dim(Im(PAQ))=\dim(Im(A))=rg(A)$.

\subsection{Théorème}
Soit $A\in M_{n,p}(\K)$. On a $rg(A)=rg(^t A)$.

\subsubsection{Remarque}
Cela signifie que le sous-espace engendré par les lignes de $A$ est de même dimension que l'espace engendré par les colonnes ce qui n'est pas du tout évident à priori.

\subsubsection{Démonstration}

Par des opérations élémentaires sur les lignes, on transforme $A$ en une matrice échelonnée réduite $PA$, $P\in GL_n(\K):$\\
$$PA=\begin{pmatrix}
0&\cdots& 0 & 1 &*&*& 0&*&*& 0&*&*&*\\
 &      &   &   & & & 1&*&*& 0&*&*&*\\
 &      &   &   & & &  & & & 1&*&*&*\\
 &      &   &   & & &  & & &  & & &
\end{pmatrix}$$

On a $rg(^tA)=rg ({}^t(PA))$ car les opérations élémentaires sur les lignes ne changent pas le sous-espace qu'elles engendrent. Le nombre de lignes non nulles $s$ de $PA$ est $rg {}^t(PA)=rg(^tA)$.\\
Ensuite, par des opérations élémentaires sur les colonnes, on transforme $PA$ en\\
$PAQ=\begin{pmatrix}
I_s& 0\\
0& 0
\end{pmatrix}, \text{ où } Q\in GL_p(\K), s=rg(^tA)$.


% ------------------------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------------------------ %



% ------------------------------------------------------------------------------------------------------ %
% ------------------------------------------------------------------------------------------------------ %


\chapter{Applications linéaires}
\section{Généralités}
\subsection{Définition}
Soient E et F deux espaces vectoriels, une application $f:E\rightarrow F$ est linéaire si elle respecte les lois internes et externes : \begin{enumerate}
\item $\forall u,v\in E^2$, $f(u+v)=f(u)+f(v)$
\item $\forall u\in E$ et $\forall\lambda\in \K$, $f(\lambda u)=\lambda f(u)$
\end{enumerate}

On note $\mathcal{L}(E,F)$ l'ensemble des applications linéaires de $E$ dans $F$.

\subsubsection{Exemples}
L'application $f:\R^3 \rightarrow \R^2$ définie par $f(x,y,z)=(-2x,y+3z)$ est linéaire.

L'application $f:\R \rightarrow \R$ définie par $f(x)=x^2$ n'est pas linéaire.\\
$f(1+1)=f(2)=4\neq f(1)+f(1)=2$.\\

Soit $E=\Rpe$ doté de la loi interne $+_E:
\begin{matrix} E\times E\rightarrow \R \\ (u,v)\mapsto u\cdot v \end{matrix}$
et de la loi externe $\begin{matrix}\R\rightarrow E \\ (\lambda,u)\mapsto u^\lambda \end{matrix}$.\\
L'application $f :E\rightarrow \R$ définie par $f(u)=\ln(u)$ est linéaire.\\
$f(u+_{E} v)=\ln(uv)=\ln(u)+\ln(v)=f(u)+f(v)$ et $f(\lambda u)=\ln(u^\lambda)=\lambda \ln(u)=\lambda f(u)$.\\

L'application nulle $O_{\mathcal{L}(E,F)}:E\rightarrow F$, $u\mapsto 0$ est linéaire.

L'application identité $id_E:E\rightarrow E$, $u\mapsto u$ est linéaire.

\subsection{Définition matricielle}
Une application $f:\K^p\rightarrow \K^n$ est linéaire si et seulement si il existe une matrice $A\in M_{n,p}(\K)$ telle que $\forall X\in \K^p$, $f(X)=AX$.

\subsubsection{Démonstration}
$\Leftarrow$\\
On a $f(X+Y)=A(X+Y)=AX+AY=f(X)+f(Y)$ et $f(\lambda X)=A(\lambda X)=\lambda AX=\lambda f(X)$.\\

$\Rightarrow$\\
On suppose $f$ linéaire et on appelle $e_k$ la k-ème colonne de $I_p$ (donc $e_{i,k}=1$ si $i=k$ et 0 sinon).\\
Définissons $A\in M_{n,p} (\K)$ par $(A_j)_{1\leq j\leq p}=f(e_j)=\sum\limits_{i=1}^n a_{i,j}e_i$.\\
Alors on a $\displaystyle f(X)=f(\begin{pmatrix}
x_1 \\ \vdots \\ x_p
\end{pmatrix})=f(\sum\limits_{i=1}^p x_ie_i)=\sum\limits_{i=1}^p x_if(e_i)=\sum\limits_{i=1}^p x_iA_i=AX$.

\newpage

\subsection{Propriétés}
Si $f :E\rightarrow F$ est une application linéaire, on a f$(O_E)=O_F$ et $\forall u\in E$, $f(-u)=-f(u)$.

\subsubsection{Démonstration}
On a $f(O_E)+f(O_E)=f(O_E+O_E)=f(O_E)$ donc $f(O_E)=O_F$
et $-f(u)=(-1)f(u)=f(-u)$.

\subsection{Terminologie}
Soient $E$ et $F$ deux espaces vectoriels.\\
\begin{itemize}[label=$\cdot$]
\item Une application linéaire de $E$ dans $F$ est appelée morphisme d'espaces vectoriels ou\\
homomorphisme d'espaces vectoriels.\\

\item Une application linéaire de $E$ dans $E$ est appelée endomorphisme de $E$.\\
L'ensemble des endomorphismes est noté $\mathcal{L}(E)$.
\end{itemize}

\section{Exemples d'application linéaires}
\subsection{Symétrie centrale}
Soit $E$ un $\K$-espace vectoriel. On définit l'application $f:E\rightarrow E$ $\forall u\in E$ par $f(u)=-u$.\\
$f$ est linéaire et s'appelle la symétrie centrale par rapport à l'origine $O_E$.

\subsection{Homothétie}
Soient $E$ un espace vectoriel et $\lambda\in \K$, on définit l'application $f_\lambda:E\rightarrow E$ par $f_\lambda (u)=\lambda u$.\\
$f$ est linéaire et s'appelle l'homothétie de rapport $\lambda$.\\

Cas particuliers : \begin{itemize}[label=$\cdot$]
\item $\lambda=1 \Rightarrow f_\lambda=id_E$
\item $\lambda=0 \Rightarrow f_\lambda$ est l'application nulle
\item $\lambda=-1 \Rightarrow f_\lambda$ est la symétrie centrale par rapport à $O_E$
\end{itemize}

\subsection{Projection}\label{Projection}
Soient $E$ un espace vectoriel et $F$, $G$ deux sous-espaces supplémentaires, c'est-à-dire $E=F\oplus G$.
Tout vecteur $u\in E$ s'écrit de façon unique : $u=v+w$, $v\in F$ et $w\in G$.\\

La projection de $u$ sur $F$ (le long de $G$) est l'application $p:E\rightarrow E$, $u\mapsto p(u)=v$.


\subsubsection{Lemme}
$p$ est linéaire et $\forall u\in E$, $p(p(u))=p(u)$.

\subsubsection{Démonstration}
\begin{itemize}
\item Soient $u,u'\in E^2$ et $\lambda,\mu\in\K$. On décompose $u$ et $u'$ sur $F$ et $G$:\\
$u=v+w$, $v\in F$, $w\in G$ et $u'=v'+w'$, $v'\in F$, $w'\in G$.\\

On a alors $\lambda u+\mu u'=\lambda(v+w)+\mu(v'+w')=(\lambda v+\mu v')+(\lambda w+\mu w') \in F+G=E$.\\
Ainsi $p(\lambda u+\mu u')=\lambda v+\mu v'=\lambda p(u)+\mu p(u')$.\\

\item Soit $u\in E$, écrivons $u=v+w$, $v\in F$, $w\in G$. On a $p(u)=v$. La décomposition de $v$ s'écrit $p(u)=v+O_G$, $v\in F$, $O_G\in G$. Ainsi $p(p(u))=p(v)=v=p(u)$.
\end{itemize}

\newpage

\subsection{Symétrie}\label{Symétrie}
Soient $E$ un espace vectoriel et $F$ et $G$ des sous-espaces supplémentaires (donc $E=F\oplus G$).\\
Tout vecteur $u\in E$ s'écrit de façon unique $u=v+w$, $v\in F$, $w\in G$.\\
La symétrie par rapport à $G$ parallèlement à $F$ est l'application $s:E\rightarrow E$, $u=v+w\mapsto -v+w$.

\begin{center}\begin{tikzpicture}[scale=0.8]
\begin{scope}
	\clip (-2,-0.5) rectangle ++(6.5,4);
	\draw (-1.5,0) -- (5,0);
	\draw (1,-1) -- (2,4);
	\draw[->,>=latex,very thick] (1.2,0) -- (4,3);
	\draw[->,>=latex,very thick] (1.2,0) -- (-0.4,3);
	\draw[thin, densely dotted] (-1,0) -- (-0.4,3) -- (1.8,3)  -- (4,3) -- (3.4,0);
	\draw[->,>=latex,very thick] (1.2,0) -- (1.8,3) node[midway,left] {$w$};
	\draw[->,>=latex,very thick] (1.2,0) -- (3.4,0);
	\draw[->,>=latex,very thick] (1.2,0) -- (-1,0);
	\node at (2.7,-0.3) {$v=p(u)$};
\end{scope}
\node at (-1,3) {$s(u)$};
\node at (4.3,3){$u$};
\node at (1.5,3.2) {$G$};
\node at (4.2,-0.3) {$F$};
\node at (0,-0.3) {$-v$};
\end{tikzpicture}\end{center}

\subsubsection{Lemme}
$s$ est linéaire et $\forall u\in E$, $p(p(u))=u$.

\subsubsection{Démonstration}
\begin{itemize}
\item De même que pour la projection, on décompose $u$ et on conclut.
\item Soit $u=v+w$, $v\in F$, $w\in G$, on a $s(u)=-v+w$ et la décomposition de $s(u)$ est\\
$s(u)=-v+w$,$-v\in F$, $w\in G$. Donc $s(s(u))=s(-v+w)=-(-v)+w=v+w=u=id_E(u)$.
\end{itemize}

\subsection{La dérivation}
Soit $E=\mathcal{C}^1(\R,\R)$ l'espace vectoriel des fonctions dérivables $f$ de $\R$ dans $\R$ telles que $f'$ est continue. Soit $F=\mathcal{C}^0(\R,\R)$ l'espace vectoriel des fonctions réelles continues.\\

L'application $d:\mathcal{C}^1(\R,\R)\rightarrow \mathcal{C}^0(\R,\R)$, $f\mapsto f'$ est linéaire.

\subsection{L'intégration}
L'application $I=\mathcal{C}^0(\R,\R)\rightarrow \mathcal{C}^1(\R,\R)$, $f\mapsto(x\mapsto \displaystyle\int_{0}^{x}f(t)dt)$ est linéaire.

\subsection{La multiplication par $X$}
L'application $f:\K_n[X]\rightarrow \K_{n+1}[X]$, $P(X)\mapsto XP(X)$ est linéaire.

\subsection{La transposition}
L'application $T:M_{n,p}(\K)\rightarrow M_{p,n}(\K)$, $A\mapsto {}^tA$ est linéaire.

\subsection{La trace}
L'application $tr: M_n(\K)\rightarrow \K$, $A\mapsto tr(A)=\sum\limits_{i=1}^n a_{i,i}$ est linéaire.

\newpage

\section{Ensembles}
\subsection{Image}
Soit $f :E\rightarrow F$ une application linéaire. L'image de $f$ est $Im(f)=\{ f(x) | x\in E \} \subseteq F$.

\subsubsection{Proposition}
\begin{enumerate}
\item Si $E'\subseteq E$ est un sous-espace vectoriel, alors $f(E')$ est un sous-espace vectoriel de $F$.

\item En particulier, $Im(f)$ est un sous-espace vectoriel.

\item Si $(\beta_i)_{1\leq i\leq n}$ est une base de $E$ alors $Im(f)=Vect((f(\beta_i))_{1\leq i\leq n})$.
\end{enumerate}

\subsubsection{Démonstrations}
\begin{enumerate}
\item On a $O_F=f(O_E)\in f(E')$. Soient $u,v\in f(E')$ et $\lambda$, $\mu \in \K$. On pose $u=f(u')$ et $v=f(v')$ pour des $u',v'\in E'$. Donc $\lambda u+\mu v=\lambda f(u')+\mu f(v')=f(\lambda u'+\mu v')\in f(E')$.\\

\item Soit $A \in M_{n,p}(\K)$ et soit $f: \K^p\rightarrow \K^n$, $X\mapsto AX$. \\
Alors $Im(f)=\{AX|X\in \K p\}=Im(A)=Vect(A_i)_{1\leq i \leq p}$ où $A_i$ sont les colonnes de $A$.\\

\item Soit $y \in Im(f)$, on a $y=f(x)$ pour un $x \in E$.\\
Les $\beta_j$ forment une base donc on a $\displaystyle x=\sum\limits_{j=1}^nx_j\beta_j$, et donc $\displaystyle y=f(x)=\sum\limits_{j=1}^n x_j f(\beta_j)$.
\end{enumerate}

\subsection{Noyau}
Soit $f:E\rightarrow F$ une application linéaire, le noyau de $f$ est $\ker(f)=\{ v\in E | f(v)=O_F \} \subseteq E$.

\subsubsection{Proposition}
$\ker(f)$ est un sous espace vectoriel de $E$.

\subsubsection{Démonstration}
On a $f(O_E)=O_F$, donc $O_E \in \ker(f)$. Soient $u,v \in \ker(f)$ et $\lambda, \mu \in \K$.\\
On a $f(\lambda u + \mu v)=\lambda f(u)+\mu f(v)=\lambda O_F +\mu O_F=O_F$ et donc $\lambda u+\mu v\in \ker(f)$.

\subsubsection{Exemple}
Soit $A \in M_{n,p}(\K)$ et soit $f: \K^p \rightarrow \K^n$, $X\mapsto AX$.
Alors $\ker(f)=\{ X\in \K^p | AX=0 \}$ est l'espace des solutions du système homogène $AX=0$.

\subsection{Propriété des projections}
Soient $F,G \subseteq E$ deux sous-espaces supplémentaires d'un espace vectoriel $E$.\\
Soit $p : E\rightarrow E$, la projection sur $F$ le long de $G$. On a $Im(p)=F$ et $\ker(p)=G$.

\subsubsection{Démonstration}
Si $u=v+w$ où $v\in F$ et $w\in G$ alors $p(u)=v$ donc $Im(p)\subseteq F$.

Si $u \in F$ on a $u=u+0$, $u\in F$, $0\in G$ et donc $p(u)=u$ donc $F\subseteq Im(p)$.

On a $p(u)=0 \Leftrightarrow v=0  \Leftrightarrow u=w\in G$ donc $\ker(p)=G$.

\newpage

\subsection{Théorème}
Soit $f:E\rightarrow F$ une application linéaire alors $f$ est injective si et seulement si $\ker(f)=\{ O_E \}$.

\subsubsection{Démonstration}
$\Rightarrow$\\
Soit $u\in \ker(f)$, on a donc $f(u)=O_F$ ; $f$ est linéaire donc on a aussi $O_F=f(O_E)$.\\
$f$ est injective donc $u=O_E$ et $\ker(f)=\{ O_E \}$.\\

$\Leftarrow$\\
Soient $u,u' \in E$ tel que $f(u)=f(u')$, on a $f(u-u')=f(u)-f(u')=O_F$.\\
Donc $u-u'\in \ker(f)=\{ O_E \}$ et $u=u'$.

\subsubsection{Remarque}
En particulier, pour montrer que $f$ est injective, il suffit de vérifier que $f(x)=O_F\Rightarrow x=O_E$.

\subsubsection{Exemple}
Soit $f: \K_n[X] \rightarrow \K_{n+1}[X]$, $P(x)\mapsto XP(X)$. Déterminons $\ker(f)$.\\

Si $P(X)=a_0+a_1X+...+a_n X^n$ alors $XP(X)=a_0X+a_1X^2+...+a_n X^{n+1}$,\\
donc $XP(X)=0 \Leftrightarrow$ $(a_i)_{1\leq i \leq n}=0 \Leftrightarrow P(X)=0$.\\

Donc $\ker(f)=\{ 0 \}$, $f$ est injective et on a $Im(f)=\K_n[X]\backslash \K^*=vect(X,X^2,...,X^{n+1})$.

\subsection{L'espace vectoriel $\mathcal{L}(E,F)$}
Soient E et F deux espaces vectoriels et $\mathcal{F}(E,F)$ l'ensemble des applications $f: E\rightarrow F$.\\
On munit $\mathcal{F}(E,F)$ de la loi de composition interne $+: \mathcal{F}(E,F)\times\mathcal{F}(E,F) \rightarrow \mathcal{F}(E,F)$ définie par $(f+g)(x)=f(x)+g(x)$ et de la loi de composition externe $\K\times \mathcal{F}(E,F) \rightarrow \mathcal{F}(E,F)$ définie par $(\lambda f)(x)=\lambda f(x)$

\subsubsection{Proposition}
\begin{enumerate}
\item $\mathcal{F}(E,F)$ muni de ces lois est un espace vectoriel.
\item $\mathcal{L}(E,F)=\{$ application linéaire $E\rightarrow F$ $\}$ est un sous espace vectoriel de $\mathcal{F}(E,F)$.
\end{enumerate}

\subsubsection{Démonstration}
\begin{enumerate}
\item exercice.

\item L'élément neutre $O_{\mathcal{F}(E,F)}$ est l'application nulle $E\rightarrow F$, $x\mapsto 0$ qui appartient à $\mathcal{L}(E,F)$.\\
Soient $f,g \in \mathcal{L}(E,F)^2$ et $\lambda, \mu \in \K^2$ ; $u,v \in E^2$ et $\lambda', \mu' \in \K^2$.\\

$\begin{array}{rcl}(\lambda f+\mu g)(\lambda' u+ \mu' v)&=&\lambda f(\lambda' u+\mu' v)+\mu g(\lambda 'u+\mu' v)\\
&=&\lambda\lambda'f(u)+\lambda\mu'f(v)+\mu\lambda'g(u)+\mu\mu'g(v) \\
&=& \lambda'(\lambda f(u)+\mu g(u))+\mu'(\lambda f(v)+\mu g(v))\\
&=& \lambda'(\lambda f+\mu g)(u)+\mu'(\lambda f+\mu g)(v)
\end{array}$\\

Donc on a bien $\lambda f+\mu g\in \mathcal{L}(E,F)$
\end{enumerate}

\newpage

\section{Composition et inverse d'applications linéaires}
\subsection{Proposition}
Soient $f:E\rightarrow F$ et $g:F\rightarrow G$ deux applications linéaires.\\
Alors $g\circ f$ est une application linéaire $E\rightarrow G$.

\subsubsection{Démonstration}
Soient $u,v \in E^2$ et $\lambda, \mu \in \K^2$, on a :\\

$\begin{array}{rcl}(g\circ f)(\lambda u+\mu v)&=& g(f(\lambda u+\mu v)) \\
&=& g(\lambda f(u)+\mu f(u))\\
&=& \lambda g(f(f(u)))+\mu g(f(u))\\
&=&\lambda (g\circ f)(u)+\mu (g\circ f)(u)\end{array}$

\subsection{Remarque}
La composition est compatible avec la structure d'espace vectoriel :
$$g\circ (f_1+f_2)=g\circ f_1+g\circ f_2$$
$$(g_1+g_2)\circ f=g_1\circ f+g_2\circ f$$
$$(\lambda g)\circ f=\lambda g\circ f=g\circ (\lambda f)$$

\subsubsection{Terminologie}
Soient $E$ et $F$ deux espaces vectoriels,
\begin{itemize} \item Une application linéaire bijective de $E$ sur $F$ est appelée isomorphisme d'espaces vectoriels.\\
Les deux espaces $E$ et $F$ sont alors dits isomorphes.
\item Un endomorphisme bijectif de $E$ est appelé automorphisme de $E$.\\
L'ensemble des automorphismes est noté $GL(E)$.
\end{itemize}

\subsection{Proposition}
Soient $E$ et $F$ deux espaces vectoriels.\\
Si $f:E\rightarrow F$ est un isomorphisme, alors $f^{-1} : F\rightarrow E$ est un isomorphisme.

\subsubsection{Démonstration}
$f$ est une application bijective $E\rightarrow F$ donc $f^{-1}$ est une application bijective de $F$ sur $E$.\\
Il faut démontrer que $f^{-1}$ est linéaire.\\

Soient $u,v \in E^2$ et $\lambda, \mu \in \K^2$, pour montrer que $f^{-1}(\lambda u+\mu v)=\lambda f^{-1}(u)+\mu f^{-1}(v)$,\\
il suffit de montrer que leurs images par $f$ sont égales car $f$ est injective.\\

On a $f(f^{-1}(\lambda u+\mu v))=\lambda u+\mu v$ et
$f(\lambda f^{-1}(u)+\mu f^{-1}(v))=\lambda f(f^{-1}(u))+\mu f(f^{-1}(v))=\lambda u+\mu v$.\\\\
Ainsi $f^{-1}(\lambda u+\mu v)=\lambda f^{-1}(u)+\mu f^{-1}(v)$ donc $f^{-1}$ est linéaire et enfin $f^{-1}$ est un isomorphisme.

\newpage

\section{Applications linéaires en dimension finie}
\subsection{Construction et caractérisation}
\subsubsection{Théorème}
Soient $E$ et $F$ deux espaces vectoriels. Supposons que $E$ est de dimension finie et que $(\beta_i)_{1\leq i\leq n}$ est une base de $E$. Alors pour tout choix de $n$ vecteurs $(v_i)_{1\leq i\leq n}$, il existe une et une seule application linéaire $f:E\rightarrow F$ telle que $f(\beta_i)=v_i$.

\subsubsection{Démonstration}
\textit{Unicité}\\
Supposons qu'il existe une application linéaire $f:E\rightarrow F$ telle que $f(\beta_j)=v_j$, $1\leq j\leq n$.\\
Pour $x\in E$, il existe des scalaires uniques $x_i$ tels que $\displaystyle x=\sum\limits_{i=1}^nx_i\beta_i$.\\
Comme $f$ est linéaire, on a : $\displaystyle f(x)=f(\sum\limits_{i=1}^nx_i\beta_i)=\sum\limits_{i=1}^nx_if(\beta_i)=\sum\limits_{i=1}^nx_iv_i$.\\

Donc si elle existe, $f$ est unique.\\

\textit{Existence}\\
Soient $f:E\rightarrow F$, $x\mapsto\sum\limits_{i=1}^nx_iv_i$, $(x,y) \in E^2$, $(\lambda,\mu) \in \K^2$, $x_i$ et $y_i$ les coordonnées de $x$ et de $y$.\\
On a $\displaystyle \lambda x+\mu y=\lambda\sum\limits_{i=1}^nx_i\beta_i+\mu \sum\limits_{i=1}^ny_i\beta_i=\sum\limits_{i=1}^n(\lambda x_i+\mu y_i)\beta_i$.\\

Donc les $\lambda x_i+\mu y_i$ sont les coordonnées de $\lambda x+\mu y$ et on a\\
$\displaystyle f(\lambda x+\mu y)=\sum\limits_{i=1}^n(\lambda x_i+\mu y_i)v_i=\lambda(\sum\limits_{i=1}^nx_iv_i)+\mu(\sum\limits_{i=1}^ny_iv_i)=\lambda f(x)+\mu f(y)$\\\\
Alors $f$ est bien linéaire, les coordonnées de $\beta_j$ sont $(0,0,...,1,0,...,0)$ et $f(\beta_j)=v_j$, $1\leq j\leq n$.

\subsection{Rang d'une application}
\subsubsection{Définition}
Le rang d'une application $f$ est $rg(f)=\dim(Im(f))=dim(Vect((f(\beta_i))_{1\leq i\leq n}))$.

\subsubsection{Proposition}
Soit $f :E\rightarrow F$ une application linéaire entre espaces vectoriels de dimensions finies.\\
On a $rg(f)\leq \min(\dim(E),\dim(F))$.

\subsubsection{Démonstration}
Soit $(\beta_i)_{1\leq i\leq n}$ une base de $E$.\\
$Im(f)$ est engendré par $(f(\beta_i))_{1\leq i\leq n}$ donc on a $\dim(Im(f))\leq n=\dim(E)$.\\
$Im(f)$ est un sous-espace vectoriel de $F$ donc on a $\dim(Im(f))\leq \dim(f)$.\\

Finalement $rg(f)\leq \min(\dim(E),\dim(F))$.

\newpage

\subsection{Théorème du rang}
Soit $f:E\rightarrow F$ une application linéaire.\\
Si E est de dimension finie, $\dim(E)=\dim(\ker(f))+\dim(Im(f))=\dim(\ker(f))+rg(f)$.

\subsubsection{Démonstration}
Soit $(\beta_{i})_{1\leq i\leq p}$ une base de $\ker(f)$. On la complète en une base $(\beta_i)_{1\leq i\leq n}$ de $E$.\\\\
Il suffit de montrer que $f(\beta_{p+1}),...,f(\beta_n)$ est une base de $Im(f)$ puisqu'on aurait\\
alors $\dim(\ker(f))+\dim(Im(f))=p+(n-p)=n=\dim(E)$.\\

$Im(f)$ est engendré par $f(\beta_1),...,f(\beta_p),f(\beta_{p+1}),...,f(\beta_n)$, or $\forall j \in \llbracket 1;p \rrbracket$, $f(\beta_j)=0$\\
donc $Im(f)$ est engendré par $f(\beta_{p+1}),...,f(\beta_n)$.\\

Il existe des scalaires $(\lambda_{j})_{p+1\leq j\leq n}$ tels que $\displaystyle\sum\limits_{\mathclap{j=p+1}}^n\lambda_{j}f(\beta_{j})=f(\sum\limits_{\mathclap{j=p+1}}^n\lambda_j\beta_j)=0$.\\\\
Donc le vecteur $\displaystyle v=\sum\limits_{\mathclap{j=p+1}}^n\lambda_j\beta_j$ appartient à $\ker(f)$ et $-v$ est une combinaison linéaire de $(\beta_i)_{1\leq i\leq p}$, ainsi il existe des $(\lambda_i)_{1\leq i\leq p}$ tels que $-v=\displaystyle\sum\limits_{i=1}^p\lambda_i\beta_i$ donc $\displaystyle\sum\limits_{i=1}^n\lambda_i\beta_i=\sum\limits_{i=1}^p\lambda_i\beta_i+\sum\limits_{i=p+1}^n\lambda_i\beta_i=v-v=0$.

Or $(\beta_i)_{1 \leq i\leq n}$ est une base de $E$ donc $\displaystyle\sum\limits_{j=1}^n\lambda_i\beta_i=0\Rightarrow \lambda_j=0$, $\forall j\in \llbracket 1; n\rrbracket$.\\

Ainsi les $\lambda_i$ s'annulent et donc $(f(\beta_i))_{1\leq i\leq n}$ est une sous-famille libre.

\subsubsection{Remarque}
Dans la pratique, cette formule sert à déterminer la dimension du noyau connaissant le rang ou bien le rang connaissant le noyau.

\subsection{Propriétés des applications linéaires}\label{PropApplLin}
Soit $f:E\rightarrow F$ une application linéaire entre espaces vectoriels de dimension finie.\\
\begin{enumerate}
\item Si $\dim(E)<\dim(F)$ alors, f n'est pas surjective
\item Si $\dim(E)>\dim(F)$ alors, f n'est pas injective
\end{enumerate}\bigskip

De plus, si $\dim(E)=\dim(F)$ alors les assertions suivantes sont équivalentes :
\begin{enumerate}
\item $f$ est bijective
\item $f$ est injective
\item $f$ est surjective
\end{enumerate}

\subsubsection{Démonstration}
\begin{enumerate}
\item Par le théorème du rang, on a $\dim(Im(f))=\dim(E)-dim(\ker(f))$.\\
Or $\dim(E)<\dim(F)$ donc $rg(f)<\dim(f)$ et $f$ n'est pas surjective.

\item On a $\dim(Im(f))\leq \dim(F)$.
Par le théorème du rang, on a $\dim(\ker(f))=\dim(E)-rg(f)$
Donc $\dim(\ker(f))\geq \dim(E)-\dim(F)>0$. Donc $f$ n'est pas injective.
\end{enumerate}

\bigskip

De plus, si $\dim(E)=\dim(F)$ :\\
$f$ est injective si et seulement si $\ker(f)=\{0\}$, donc d'après le théorème du rang,\\
$f$ est injective si et seulement si $\dim(Im(f))=\dim(E)$.\\
Or $\dim(E)=\dim(F)$, donc $f$ est injective si et seulement si $\dim(Im(f))=\dim(F)$.

Cela équivaut à $Im(f)=F$, c'est-à-dire $f$ est surjective.

\newpage

\subsection{Dimensions d'espaces isomorphes}
Soit $f:E\rightarrow F$ un isomorphisme d'espace vectoriel, alors $E$ est de dimension finie\\
si et seulement si $F$ l'est et alors $\dim(E)=\dim(F)$.

\subsubsection{Démonstration}
$f$ est surjective donc si $E$ est de dimension finie, $F$ est engendré par l'image d'une base de $E$.\\
Donc $F$ est de dimension finie et $\dim(F)\leq \dim(E)$.\\
L'application $f^{-1}:F\rightarrow E$ est aussi un isomorphisme, donc $\dim(E)\leq \dim(F)$.

\subsection{Lemme de Grassmann}
Soient $E$ un espace vectoriel et $F,G$ deux sous-espaces vectoriels de dimension finie,\\
on a $\dim(F+G)=\dim(F)+\dim(G)-\dim(F\cap G)$.

\subsubsection{Première démonstration}
Soit $(\beta_i)_{1\leq i\leq r}$ une base de $F\cap G$, on la complète en une base $\beta_1,...,\beta_r,\beta'_{r+1},...,\beta'_{p}$ de $F$ et en une base $\beta_1,...,\beta_r,\beta''_{r+1},...,\beta''_{q}$ de $G$.\\

Il suffit de montrer que $\B=\beta_1,...,\beta_r, \beta'_{r+1},...,\beta'_{p}, \beta''_{r+1},...,\beta''_{q}$ est une base de $F+G$ puisque $\B$ contient $p+q-r=\dim(F)+\dim(G)-\dim(F\cap G)$ vecteurs.

Le sous-espace engendré par $\B$ contient $F$ et $G$ donc $\B$ engendre $F+G$.\\

Si l'on a $\displaystyle \sum\limits_{i=1}^r\lambda_i\beta_i+\sum\limits_{j=r+1}^p\lambda'_j\beta'_j+ \sum\limits_{k=i+1}^q\lambda''_k\beta''_k=u+v+w=0$, alors le vecteur $u+v=-w$\\
appartient à $F\cap G$. $u\in F\cap G$ donc on a $v\in F\cap G$, ce qui implique que $\lambda'_j=0$ pour tout $j$\\
 car les $\beta'_j$ complètent la base de $F\cap G$ on a donc $\displaystyle 0=\sum\limits_i=1^r\lambda_i\beta_i+ \sum\limits_{k=i+1}^q\lambda''_k\beta''_k$\\
ce qui implique que $\lambda_i=0$ et $\lambda'_i=0$ pour tout $i$ car $\beta_1,...,\beta_r,\beta''_{r+1},...,\beta''_{q}$ est une famille libre.\\

\subsubsection{Seconde démonstration}
Soit $f:\begin{matrix}F\times G\rightarrow E \\ (u,v)\mapsto u+v\end{matrix}$, $F\times G$ est un espace vectoriel doté des lois $\begin{matrix} (u,v)+(u',v')=(u+u',v+v')\\ \lambda (u,v)=(\lambda u, \lambda v)\end{matrix}$ et $f$ est une application linéaire.\\

On a $Im(f)=\{f(u,v) | (u,v)\in F\times G \}=\{ u+v|u\in F, v\in G \}=F+G$ \\
et $\ker(f)=\{(u,v)\in F\times G | u+v=0 \}=\{(x,-x) | x\in F\cap G \}$.\\

On a un isomorphisme d'espaces vectoriels $\begin{matrix}F\cap G \rightarrow \ker(f) \\ x\mapsto (x,-x)\end{matrix}$
d'inverse $\begin{matrix}\ker(f)\rightarrow F\cap G\\(u,v)\mapsto u\end{matrix}$.\\

On a $\dim(F\times G)=\dim(F)+\dim(G)$ car si $(\beta_i)_{1\leq i\leq p}$ est une base de $F$ et $(\beta'_i)_{1\leq i\leq p}$ est une base de $G$ alors $(\beta_1,0),...,(\beta_p,0),(0,\beta'_1),...,(0,\beta'_q)$ est une base de $F\times G$.\\

Ainsi $\dim(F+G)=\dim(Im(f))=\dim(F\times G)-\dim(\ker(f))=\dim(F)+\dim(G)-\dim(F\cap G)$.

\newpage

\subsection{Caractérisation des projections et des symétries}
Soient $E$ un espace vectoriel et $p$ et $s$ des endomorphismes de $E$.

\begin{enumerate}
\item $p$ est une projection si et seulement si $p^2=p$. Dans ce cas, $p$ est la projection sur $F=Im(p)$ le long de $G=\ker(p)$. En particulier, $F$ et $G$ sont supplémentaires.
\item $s$ est une symétrie si et seulement si $s^2=Id_E$. Dans ce cas, $F=\ker(s-Id_E)$ et \\
$G=\ker(s+Id_E)$ sont supplémentaires et $s$ est la projection sur $F$ parallèlement à $G$.
\end{enumerate}

\subsubsection{Démonstration}

\begin{enumerate}
\item On sait déjà que $p^2=p$ si p est une projection. Montrons la réciproque.

\begin{itemize}[label=$\bullet$]

\item $F=Im(p)$ et $G=\ker(p)$ sont supplémentaires.\\
Soit $u\in E$. On a :
$$u=p(u)+(u-p(u))$$\\
Ici, $p(u)\in Im(p)$ et $(u-p(u))\in\ker(p)$ car $p(u-p(u))=p(u)-p^2(u)=p(u)-p(u)=0$.\\
Donc $E=F+G$.\\

Soit $u\in F\cap G$, $u=p(v)$ pour un $v\in$ E. Donc :
$$0=p(u)=p^2(v)=p(v)=u$$
et $F\cap G=\{0\}$.\\

\item $p$ est la projection sur $F$ le long de $G$.\\
Soit $u\in E$. Écrivons $u=v+w$, $v\in F$, $w\in G$. On a :
$$p(u)=p(v)+p(w)=p(v)$$
Écrivons $v=p(v')$ pour un $v'\in E$. On a $p(v)=p^2(v')=p('v)=v$.\\
Donc $p(u)=v$ et $p$ est bien la projection sur $F$ le long de $G$.
\end{itemize}

\bigskip\item On sait déjà que $s^2=Id_E$ si $s$ est une symétrie. Montrons la réciproque.

\begin{itemize}[label=$\bullet$]
\item $F=\ker(s-Id_E)$ et $G=\ker(s+Id_E)$ sont supplémentaires.\\

Notons que $u\in E$ appartient à $F$ si et seulement si $s(u)=u$ et $u\in G$ si et seulement si $s(u)=u$. Supposons que $u\in F\cap G$, alors $u=s(u)=-u$ et $u=0$ donc $F\cap G$={0}.\\

Soit $u\in E$. On a :
$$u=\frac{1}{2}(u+s(u))+\frac{1}{2}(u-s(u))=v+w$$
en posant $v=\frac{1}{2}(u+s(u))$ et $w=\frac{1}{2}(u-s(u))$.\\

Alors $s(v)=\frac{1}{2}(s(u)+s^2(u))=\frac{1}{2}(s(u)+u)=v$ donc $v\in F$.\\

Et de plus $s(w)=\frac{1}{2}(s(w)-s^2(w))=\frac{1}{2}(s(u)-w)=-w$ donc $w\in G$.\\\\

Enfin $E=F+G$.\\

\item $s$ est la symétrie par rapport à $F$ parallèlement à $G$.\\

Soit u$\in E$, écrivons $u=v+w$, $v\in F, w\in G$.\\
On a alors $s(u)=s(v)+s(w)=v+(-w)$ donc $s$ est bien une symétrie.
\end{itemize}
\end{enumerate}

\newpage

\section{Matrice d'une application linéaire}
\subsection{Définition}
Soit $f:E\rightarrow F$ une application linéaire entre espaces vectoriels de dimension finie.\\
Soient $\B=(\beta_i)_{1\leq i\leq p}$ une base de $E$ et $\B'=(\beta'_i)_{1\leq i\leq n}$ une base de $F$.\\
La matrice de $f$ dans les bases $\B,\B'$ est $A\in M_{n,p}(\K)$ telle que $f(\beta_j)=\sum\limits_{i=1}^n a_{i,j}\beta'_i$, $1\leq j\leq p$.

\subsubsection{Remarque}
La colonne $A_j$ contient les coordonnées dans $\B'$ du $j$-ème vecteur de $\B$.

\subsubsection{Notation}
On note $A=Mat(f,\B,\B')$.

\subsection{Proposition}
Soit $A\in M_{n,p}(\K)$ et soit $f:\K^p\rightarrow \K^n$, $X\mapsto AX$ alors la matrice de $f$ dans les\\
bases canoniques de $\K^p$ et $\K^n$ est $A$.

\subsubsection{Démonstration}
$f(e_j)=Ae_j=A_j=\displaystyle\sum\limits_{i=1}^na_{i,j}e_i$

\subsubsection{Exemples}
Soit $f:\begin{matrix} \K_4[X]\rightarrow \K_4[X]\\P\mapsto P+(1-X)P'\end{matrix}$. On cherche la matrice de $f$ dans les bases $\B=\B'=(1,X,...,X^4)$.\\

On a $f(1)=1$, $f(X)=1$, $f(X^2)=-X^2+2x$, $f(X^3)=3X^2-2X^3$ et $f(X^4)=4X^3-3X^4$.\\

Alors $A=Mat(f,\B,\B')=\begin{pmatrix}
1 & 1 & 0  & 0  & 0 \\
0 & 0 & 2  & 0  & 0 \\
0 & 0 & -1 & 3  & 0 \\
0 & 0 & 0  & -2 & 4 \\
0 & 0 & 0  & 0  & -3
\end{pmatrix}$

\bigskip\bigskip\bigskip

Soient $A \in M_{n,p}(\K)$ et $f:\begin{matrix}\K^p\rightarrow \K^n\\X\mapsto AX\end{matrix}$ alors la matrice de $f$ dans les bases $(e_i)_{1\leq i\leq p}$ et $(e_i)_{1\leq i\leq n}$ est la matrice $A$ elle-même.\\

\bigskip\bigskip\bigskip

Soit $f:\begin{matrix}\R_3[X] \rightarrow \R_2[X]\\P\mapsto P'\end{matrix}$.
On a $f(1)=0$,$f(X)=1$, $f(X^2)=2X$ et $f(X^3)=3X^2$.\\

Donc la matrice de $f$ dans les bases $1,X,X^2,X^3$ et $1,X,X^2$ est $A=\begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 2 & 0\\
0 & 0 & 0 & 3
\end{pmatrix}$

\newpage

\subsection{Inverse d'une matrice}
Si $A\in M_n(\K)$ admet un inverse à droite $A'$ alors $A$ est inversible d'inverse $A'$.

\subsubsection{Démonstration}
On démontre d'abord l'existence d'un inverse à gauche $A''$ puis l'égalité $A''=A'$.\\

\begin{enumerate}
\item Soit $f:M_n(\K) \rightarrow M_n(\K)$, $M \mapsto MA$ une application linéaire.\\
Si $MA=0$, alors $0=(MA)A'=M(AA')=MI_n=M$ donc f est injective.\\

Par la propriété \ref{PropApplLin}, $f$ est aussi surjective donc il existe $A''\in M_n(\K)$ tel que $f(A'')=I_n$. Cela signifie que $A''A=I_n$ donc $A''$ est inverse à gauche de $A$.\\

\item On a $A'=I_nA'=(A''A)A'=A''(AA')=A''I_n=A''$.
\end{enumerate}

\subsection{Opérations}
Soient $f,g :E\rightarrow F$ deux applications linéaires et soient $\B$ une base de $E$ et $\B'$ une base de $F$.
\begin{enumerate}
\item $Mat(f+g,\B,\B')=Mat(f,\B,\B')+Mat(g,\B,\B')$
\item $Mat(\lambda f,\B,\B')=\lambda Mat(f,\B,\B')$
\end{enumerate}

\subsubsection{Remarque}
On a un isomorphisme d'espaces vectoriels $\mathcal{L}(E,F)\rightarrow M_{n,p}(\K)$, $f\mapsto Mat(f,\B,\B')$.\\

Son inverse envoie une matrice $A\in M_{n,p}(\K)$ sur l'unique application linéaire $f:E\rightarrow F$\\
telle que $f(\beta_j)=\displaystyle\sum\limits_{i=1}^na_{i,j}\beta'_i$, $1\leq j\leq p$.

\subsection{Composition}
Soient $f :E\rightarrow F$ et $g:F\rightarrow G$ deux applications linéaires.\\
Soient $\B$ une base de $E$, $\B'$ une base de $F$ et $\B''$ une base de $G$. Alors
$$Mat(g\circ f,\B,\B'')=Mat(g,\B',\B'')\cdot Mat(f,\B,\B')$$

\subsubsection{Démonstration}
Notons $\B=(\beta_i)_{1\leq i\leq p}$, $\B'=(\beta'_i)_{1\leq i\leq n}$ et $\B''=(\beta''_i)_{1\leq i\leq q}$.\\
Posons $A=Mat(f,\B,\B')$ et $B=Mat(g,\B',\B'')$.\\
\begin{spacing}{0.2}$$\begin{array}{rcccl}(g\circ f)(\beta_j) =g(f(\beta_j)) &=& g(\displaystyle\sum\limits_{i=1}^na_{i,j}\beta'_i) &=&\displaystyle\sum\limits_{i=1}^n a_{i,j}g(\beta'_i)\\\\
&=&\displaystyle\sum\limits_{i=1}^na_{i,j}g(\beta'_{i}) &=&\displaystyle\sum\limits_{i=1}^na_{i,j}\sum\limits_{k=1}^qb_{k,i}\beta'_k\\\\
&=&\displaystyle\sum\limits_{k=1}^q(\sum\limits_{i=1}^nb_{k,i}a_{i,j})\beta''_k &=&\displaystyle\sum\limits_{k=1}^q(BA)_{k,j}\beta''_k\end{array}$$\end{spacing}

\newpage

\subsection{Matrice d'un endomorphisme}
Soit $f:E\rightarrow E$ un endomorphisme d'un espace vectoriel $E$ de base $\B$.\\
On note $Mat(\B)=Mat(f,\B,\B)$.

\subsubsection{Théorème}
Soit $f: E\rightarrow E$ un endomorphisme d'un espace vectoriel de dimension finie et soit $\B$ une base de $E$, soit $A=Mat(f,\B)$
\begin{enumerate}
\item $f$ est bijective si et seulement si $A$ est inversible
\item si $f$ est bijective alors $Mat(f^{-1},\B)=A^{-1}$
\end{enumerate}

\subsubsection{Exemples}
\begin{enumerate}
\item Si $n=\dim(E)$, on a $Mat(Id_E,\B)=I_n$.
\item Soit $h_\lambda :E\rightarrow E$, $u\mapsto \lambda$ l'homothétie de rapport $\lambda$. On a $Mat(h_\lambda,\B)=\lambda I_n$.
\item Soit $s:E\rightarrow E$, $u\mapsto -u$ la symétrie centrale. On a $Mat(s,\B)=-I_n$.
\item Soit $r_{\theta}:\R^2\rightarrow \R^2$ la rotation d'angle $\theta\in \R$. Soit $\B$ la base canonique de $\R^2$,\\
on a $Mat(r_\theta,\B)=
\begin{pmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{pmatrix}$.
\end{enumerate}

\subsection{Puissance}
Soit $f:E\rightarrow E$, un endomorphisme d'un espace vectoriel de base $\B$,\\
on a $Mat(f^p,\B)=Mat(f,\B)^p$, $p\in \N$.

\subsubsection{Exemple}
$f=r_\theta  : \R^2\rightarrow \R^2$
$Mat(f^p,\B)=\begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{pmatrix}^p=\begin{pmatrix}
\cos(p\theta) & -\sin(p\theta) \\
\sin(p\theta) & \cos(p\theta)
\end{pmatrix}=Mat(r_\theta{}^p,\B)$

\subsection{Matrice d'un isomorphisme}
Soit $f:E\rightarrow F$ une application linéaire entre espaces vectoriels de dimensions finies.\\
Soient $\B$ une base de $E$, $\B'$ une base de $F$ et $A=Mat(f,\B,\B')$.

\begin{enumerate}
\item $f$ est bijective si et seulement si $A$ est inversible.
\item si $f$ est bijective, on a $Mat(f^{-1},\B',\B)=a^{-1}$.
\end{enumerate}

\subsubsection{Démonstration}
Supposons $f$ bijective, posons $B=Mat(f^{-1},\B,\B')$.\\
On a $BA=Mat(f^{-1},\B',\B) =Mat(f,\B,\B') =Mat(Id_E,\B,\B) =I_n$ où $n=\dim(E)$. \\

De même, $AB=I_m$ où $m=\dim(F)$, donc $n=m$ et $A$ est inversible d'inverse $B$.\\

Réciproquement supposons $A$ inversible.\\
Soit $g:F\rightarrow E$ l'application linéaire telle que $Mat(g,\B',\B)=A^{-1}$ alors\\
$Mat(g\circ f,\B,\B)=Mat(g,\B',\B)\cdot Mat(f,\B,\B')=A^{-1}A=I_n$.\\

Donc la matrice de $g\circ f$ est la matrice identité ce qui signifie que $g\circ f=Id_E$.\\
De même, $f\circ g=Id_F$.\\

Ainsi $f$ est bijective.

\newpage

\section{Changement de base}
\subsection{Application linéaire, matrice, vecteur}
Soient $E$ un espace vectoriel de dimension finie, et soit $\B=(\beta_i)_{1\leq i\leq p}$ une base de $E$.\\
Pour chaque $u\in E$, il existe un unique $p$-uplet de coordonnées $(x_i)_{1\leq i\leq p}$ tel que
$u=\sum x_i\beta_i$.

\subsubsection{Notation}
$Mat(u,\B)=\begin{pmatrix}
x_1 \\ \vdots \\ x_p
\end{pmatrix}$ est le vecteur des coordonnées de $u$ dans $\B$.

\subsubsection{Remarque}
L'application $\phi_\B : \begin{array}{rcl} E &\rightarrow& \K^p \\ u &\mapsto& Mat(u,\B) \end{array}$ est un isomorphisme d'espaces vectoriels.

\subsection{Proposition}
Soit $f:E\rightarrow F$ une application linéaire entre espaces vectoriels de dimension finie.\\
Soient $\B=(\beta_i)_{1\leq i\leq p}$ et $\B'=(\beta_i')_{1\leq i\leq n}$ des bases E et F, alors on a $Mat(f(x),\B')=Mat(f,\B',\B)\cdot Mat(x,\B)$.\\

Autrement dit, on a $y=AX$ si $X$ est le vecteur des coordonnées de $x$ et $y$ est le vecteur des coordonnées de $f(x)$ et $A$ est la matrice de $f$.

\subsubsection{Démonstration}
On a $\displaystyle f(x)=f\left(\sum\limits_{j=1}^px_j\beta_j\right)=\sum\limits_{j=1}^px_jf(\beta_j) =\sum\limits_{j=1}^p\left(x_j\sum\limits_{i=1}^na_{i,j}\beta_i'\right) =\sum\limits_{i=1}^n\left(\sum\limits_{j=1}^pa_{i,j}x_j\right)\beta_i'$.

\subsection{Matrice de passage d'une base à une autre}
Soit $E$ un espace vectoriel de dimension $n$.

\subsubsection{Définition}
Soient $\B$ une base de $E$ et $\B'$ une seconde base de $E$.\\
La matrice de passage de la base $\B$ vers la base $\B'$, notée $P_{\B,\B'}$, est la matrice de taille $n\times n$ dont la $j$-ème colonne est formée des coordonnées du $j$-ème vecteur de la base $\B'$ dans la base $\B$.

\subsubsection{Lemme}
Soient $\B=(\beta_{1},...,\beta_{n})$ une base de $E$, $A\in M_n(\mathbb{K})$ et $\forall j\in \llbracket 1;n \rrbracket$, $\displaystyle v_j=\sum_{i=1}^{n}a_{ij}\beta_{j}$.\\

Alors $(v_i)_{1\leq i\leq n}$ est une base de $E$ si et seulement si $A$ est inversible, et dans ce cas la matrice de passage de $\B$ à $(v_i)_{1\leq i\leq n}$ est $A$.

\subsubsection{Démonstration}
Soit $J:E\rightarrow E$ l'endomorphisme de $E$ tel que :
$$J(\beta_{j})=v_j,\hspace{1em} 1\leq j \leq n$$

Alors $A=Mat(J,\B)$ et $(v_i)_{1\leq i\leq n}$ est une base $\Leftrightarrow$ $J$ est bijective $\Leftrightarrow$ $A$ est inversible.\\

\subsubsection{Exemple}
Soit $E=\R^2$, et
$\beta_{1}=\begin{pmatrix} 1 \\ 0 \end{pmatrix},
\beta_{2}=\begin{pmatrix} 1 \\ 1 \end{pmatrix},
\beta_{1}'=\begin{pmatrix} 1 \\ 2 \end{pmatrix},
\beta_{2}'=\begin{pmatrix} 5 \\ 4 \end{pmatrix}$\\

On considère les bases $\B=(\beta_{1},\beta_{2})$ et $\B'=(\beta_{1}',\beta_2')$, et l'on cherche\\
la matrice de passage de $\B$ à $\B'$. \\

Il faut exprimer $\beta_{1}'$ et $\beta_{2}'$ comme combinaisons linéaires de $\beta_{1}$ et $\beta_{2}$. On a :
$$\beta_{1}'=-\beta_{1}+2\beta_{2} \text{ et } \beta_{2}'=\beta_{1}+4\beta_{2}$$

et donc
$$P_{\B,\B'}=\begin{pmatrix} -1 & 1 \\ 2 & 4 \end{pmatrix}$$

\subsection{Propriétés}
\begin{enumerate}
\item Soient $E$ un espace vectoriel de dimension finie et $\B$ et $\B'$ deux bases de $E$, alors $$P_{\B,\B'}=Mat(Id_E,\B',\B)$$
\textbf{Remarque}\\
L'ordre des bases est inversé.\\

\item La matrice de passage d'une base $\B$ vers une base $\B'$ est inversible et son inverse est égale à la matrice de passage de $\B'$ vers $\B$.
$$P_{\B',\B}=P_{\B,\B'}^{-1}$$
\item Si $\B$, $\B'$ et $\B''$ sont trois bases, on a :
$$P_{\B,\B''}=P_{\B,\B'}\cdot P_{\B',\B''}$$
\item Soient $\B$, $\B'$ des bases de $E$, $x\in E$, $X=Mat(x,\B)$ et $X'=Mat(x,\B')$. On a :
$$X=P_{\B,\B'}\cdot X'$$
\end{enumerate}

\subsubsection{Démonstrations}
\begin{enumerate}
\item Soit $J: \begin{array}{rcl} E &\rightarrow& E \\ x &\mapsto& x \end{array}$ et $A=Mat(J,\B',\B)$.\\

On a :
$$J(\beta_{j}')=\sum_{i=1}^{n}a_{ij}\beta_{i}$$
$A_J$ est le vecteur des coordonnées de $J(\beta_{j}')= \beta_{j}'$ dans la base $\B$, donc $A_j$ est la j-ème colonne de $P_{\B,\B'}$.\\

\item On a $P_{\B,\B'}=Mat(Id_E, \B',\B)$. Donc :
$$P_{\B,\B'}^{-1}=Mat(Id_E,\B',\B)^{-1}=Mat(Id_E^{-1},\B,\B')=Mat(Id_E,\B,\B')=P_{\B',\B}$$
\smallskip
\item L'identité $Id_E:(E,\B'')\mapsto (E,\B)$ se factorise :
$$(E,\B'')\mapsto (E,\B')\mapsto (E,\B)$$
D'où l'égalité :
$$Mat(Id_E,\B'',\B)=Mat(Id_E,\B',\B)\cdot Mat(Id_E,\B'',\B')$$
$$P_{\B,\B''}=P_{\B,\B'}\cdot P_{\B',\B''}$$
\smallskip
\item $X=Mat(x,\B)=Mat(Id_E(x),\B',\B)\cdot Mat(\B',x)=P_{\B,\B'}\cdot X'$
\end{enumerate}

\subsection{Formule du changement de base}
\subsubsection{Propriété}
Soient $E$ et $F$ deux espaces vectoriels de dimension finie, $\B_E$ et $\B_E'$ deux bases de $E$\\
et $\B_F$ et $\B_F'$ deux bases de $F$, $P=P_{\B_E,\B_E'}$ et $Q=P_{\B_F,\B_F'}$,\\
$f:E\mapsto F$ une application linéaire, $A$ la matrice de $f$ dans les bases de $\B_E$ et $\B_F$\\
et $A'$ la matrice de $f$ dans les bases $\B_E'$ et $\B_F'$. Alors 

$$A'=Q^{-1}AP$$

\subsubsection{Démonstration}
L'application $f:(E,\B_E')\mapsto (F,\B_F')$ se factorise en :
$$(E,\B_E')\mapsto (E,\B_E)\mapsto (F,\B_F)\mapsto (F,\B_F')$$
Donc :
$$\begin{array}{rcl}A'=Mat(f,\B_E',\B_F')&=& Mat(Id_F,\B_F,\B_F')\cdot Mat(f,\B_E,\B_F)\cdot Mat(Id_E,\B_E',\B_E) \\\\
&=& P_{\B_F',\B_F}\cdot A\cdot P_{\B_E,\B_E'} \\\\
&=& Q^{-1}\cdot A\cdot P \end{array}$$

\subsubsection{Corollaire}
Dans le cas d'un endomorphisme, la formule se simplifie.\\
Soit $E$ un espace vectoriel de dimension finie. Soit $f:E\mapsto E$ un endomorphisme.\\
Soient $\B,\B'$ deux bases de $E$. Soit $A=Mat(f,\B)$ et soit $A'=Mat(f,\B')$.

On 	a :
$$A'=P^{-1}AP$$

\subsubsection{Remarque}
C'est souvent l'intérêt des changements de base de se ramener à des matrices plus simple.\\
Par exemple ici, $A'$ est une matrice diagonale donc il est facile de calculer les puissances $A'^k$, $k\in \N$, on peut donc déduire facilement les puissances de $A^k$.

\newpage

\subsubsection{Exemple}
Soient les bases de $E=\mathbb{R}^3$ :
$$\B_1 : \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix}, 
\begin{pmatrix} 0 \\ -1 \\ 0 \end{pmatrix}, 
\begin{pmatrix} 3 \\ 2 \\ -1 \end{pmatrix}, 
\hspace{5em}
\B_2 : \begin{pmatrix} 1 \\ -1 \\ 0 \end{pmatrix},
\begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix},
\begin{pmatrix} 0 \\ 0 \\ -1 \end{pmatrix}$$\\

Soit $f:\R^3 \rightarrow \R^3$ l'application linéaire dont la matrice dans la base $\B_1$ est :
$$A=\begin{pmatrix} 1 & 0 & -6 \\ -2 & 2 & -7 \\ 0 & 0 & 3 \end{pmatrix}$$\\

On cherche la matrice $A'$ de $f$ dans la base $\B_2$.\\

\begin{enumerate}
\item On calcule $$P=P_{\B_1,\B_2}=\begin{pmatrix} 1 & 0 & -3 \\ 2 & -1 & -1 \\ 0 & 0 & 1 \end{pmatrix}$$\\

\item On calcule
$$P^{-1}=\begin{pmatrix} 1 & 0 & 3 \\ 2 & -1 & 5 \\ 0 & 0 & 1 \end{pmatrix}$$\\

\item Enfin, d'après le corollaire, on a :

$$\begin{array}{rcl}A'&=& P^{-1}AP \\\\
&=& \begin{pmatrix} 1 & 0 & 3 \\ 2 & -1 & 5 \\ 0 & 0 & 1 \end{pmatrix}\cdot
\begin{pmatrix} 1 & 0 & -6 \\ -2 & 2 & -7 \\ 0 & 0 & 3 \end{pmatrix}\cdot
\begin{pmatrix} 1 & 0 & -3 \\ 2 & -1 & -1 \\ 0 & 0 & 1 \end{pmatrix} \\\\
&=& \begin{pmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{pmatrix}\end{array}$$
\end{enumerate}

\newpage

\section{Matrices semblables}
\subsection{Définition}
Soient $A,B\in M_n(\N)$.\\
La matrice $A$ est semblable à la matrice $B$ s'il existe une matrice inversible $P\in GL_n(\mathbb{K})$ telle que $$B=P^{-1}AP$$

\subsection{Propriété}
La relation de similitude est une relation d'équivalence, c'est à dire qu'elle est :
\begin{enumerate}
\item \textit{réflexive} : toute matrice $A$ est inversible à elle même.
\item \textit{symétrique} : si $A$ est semblable à $B$, alors $B$ est semblable à $A$.
\item \textit{transitive} : si $A$ est semblable à $B$ et $B$ est semblable à $C$, alors $A$ est semblable à $C$.
\end{enumerate}

\subsubsection{Démonstration}
\begin{enumerate}
\item $A=I_n^{-1}AI_n$.
\item Si $B=P^{-1}AP$, alors $A=(P^{-1})^{-1}BP^{-1}$.
\item Si $B=P^{-1}AP$ et C=$Q^{-1}BQ$, alors $C=Q^{-1}P^{-1}APQ=(PQ)^{-1}A(PQ)$.
\end{enumerate}

\subsection{Lemme}
Deux matrices semblables ont la même trace.

\subsubsection{Démonstration}
Si $B=P^{-1}AP$, alors $tr(B)=tr(P^{-1}(AP))=tr(APP^{-1})=tr(A)$.

\subsection{Trace}
Soit $f:E\mapsto E$ une endomorphisme d'un espace vectoriel de dimension finie.\\
On pose $tr(f)=tr(Mat(f,B))$ où $B$ est une base quelconque de $E$.

\subsection{Propriété}
Soit $A\in M_n(\mathbb{K})$ telle que $A^2=A$. On a $rg(A)=tr(A)$.

\subsubsection{Démonstration}
Soit $p:\mathbb{K}^n \mapsto \mathbb{K},X\mapsto AX$, $p^2=p$ donc $p$ est une projection.\\
Soient $F$ et $G$ les sous-espaces de $\K^n$ tels que $p$ est la projection sur $F$ le long de $G$.\\

On a $F=Im(p)$ et $G=Ker(p)$.

Soit $B$ une base $(\beta_{1},...,\beta_{r},\beta_{r+1},...,\beta_{n})$ de $\K^n$ telle que $(\beta_{1},...,\beta_{r})$ est une base de $F$ et $(\beta_{r+1},...,\beta_{n})$ est une base de $G$.\\

On a donc :
$$rg(A)=rg(p)=tr(Mat(p,B))=tr(p)=tr(A)$$


\end{document}
