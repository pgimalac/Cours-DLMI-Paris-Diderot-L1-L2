\documentclass[a4paper,10pt]{book} %type de document et paramètres


\usepackage{lmodern} %police de caractère
\usepackage[english,french]{babel} %package de langues
\usepackage[utf8]{inputenc} %package fondamental
\usepackage[T1]{fontenc} %package fondamental

\usepackage[top=3cm, bottom=3cm, left=4cm, right=2cm]{geometry} %permet de paramétrer les marges par défaut
\usepackage{changepage} %permet de modifier localement une mise en page (marges,...) : utilisé pour la page de garde
%\usepackage{multicol} %permet de mettre plusieurs colonnes (\begin{multicols}{2} \end{multicols} jusqu'à 10 colonnes)
\usepackage[pdftex, pdfauthor={Pierre Gimalac}, pdftitle={Probabilités IV}, pdfsubject={Probabilités}, pdfkeywords={Mathématiques,Probabilités}, colorlinks=true, linkcolor=black]{hyperref} %permet de se déplacer dans le pdf depuis le sommaire en cliquant sur les titres, ainsi que de parametrer les meta données du PDF
%\usepackage{url} %permet de mettre des URL actifs \url{}
%\let\urlorig\url
%\renewcommand{\url}[1]{\begin{otherlanguage}{english}\urlorig{#1}\end{otherlanguage}}

\usepackage{mathtools} %maths (à developper, utile par exemple pour enlever les espaces dus aux boites $\sum_{\mathclap{1\le i\le j\le n}} X_{ij}$)
\usepackage{amssymb} %maths
\usepackage{amsthm} %maths
\usepackage{amsmath} %maths
\usepackage{mathrsfs} %maths (par exemple les lettres caligraphiées)
\usepackage{dsfont} % le 1 de la fonction indicatrice
\usepackage{stmaryrd} %maths (par exemple les ensembles d'entiers \rrbracket \llbracket)
\usepackage{calrsfs} %maths (par exemple les notations des ensembles)
%\usepackage{yhmath} % permet de noter les arcs de cercle avec \wideparen{AOB}
%\usepackage{xlop} %permet d'afficher des opérations mathématiques
%\usepackage[squaren,Gray]{SIunits} %permet de noter des unités proprement
%\usepackage{esdiff} %permet d'écrire la dérivée avec la notation de Leibniz \diff{v}{t}

\usepackage{graphicx} %permet d'insérer des images proprement (ajoute des parametres)
%\usepackage{wrapfig} %permet de mettre des images à coté d'un texte
%\usepackage{pdfpages} %permet d'insérer un pdf \includepdf[pages={1-2}]{truc.pdf}
\usepackage{enumitem} %permet de changer le label d'une liste \begin{itemize}[label=$\cdot$]
%\usepackage{ulem} %permet de souligner/barrer du texte
%\usepackage{soul} %permet de souligner/barrer du texte
%\usepackage{cancel} %permet de barrer du texte /cancel{text}


\usepackage{tikz} %package trooop bien permet de dessiner tout et n'importe quoi ! \begin{tikzpicture}
%\usetikzlibrary{automata,positioning} % pour dessiner des automates
%\usepackage{circuitikz} %permet de dessiner des circuits logiques (entre autre) avec la syntaxe de tikz (\begin{circuitikz}) par exemple \node[american not port] pour le 'non'
%\usepackage{listings} %permet d'inserer du code dans le fichier (\lstset{language=Java} \begin{lstlisting} \end{lstlisting} )


\newcommand{\R}{\mathbb{R}}
\newcommand{\Rpe}{\mathbb{R}_{+}^{*}}
%\newcommand{\Rb}{\overline{\mathbb{R}}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
%\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\E}{\mathbb{E}} % espérance
\renewcommand{\P}{\mathbb{P}} % fonction de probabilité
\newcommand\abs[1]{\left|#1\right|}
\newcommand{\tq}{~|~}
%\newcommand\fra[2]{\genfrac{}{}{0pt}{1}{#1}{#2}}
%\newcommand\equi[1]{\renewcommand{\arraystretch}{0.3}~\begin{matrix}\sim\\#1\end{matrix}~\renewcommand{\arraystretch}{1}}
%\newcommand{\dl}{développement limité }
%\newcommand{\dls}{développements limités }
%\newcommand{\ev}{espace vectoriel }
%\newcommand{\evs}{espaces vectoriels }
%\newcommand{\sev}{sous-espace vectoriel }
%\newcommand{\sevs}{sous-espaces vectoriels }
%\newcommand{\displayAmath}{\displaystyle}
\newcommand{\lime}[4]{#1\underset{\mathclap{#2 \rightarrow #3}}{\longrightarrow} #4}
%\newcommand{\supp}{\mathrm{supp}~} % support
\newcommand{\Ima}{\mathrm{Im}} % image
%\newcommand{\Inv}{\mathrm{Inv}~} % nombre d'inversion d'une permutation
%\newcommand{\ord}{\mathrm{ord}~} % ordre
%\newcommand{\com}{\mathrm{com}~} % comatrice
%\newcommand{\oversim}[1]{\overset{\sim}{#1}}
%\newcommand{\legendre}[2]{\left(\frac{#1}{#2}\right)}
\newcommand{\Uni}{\mathrm{Unif}} % loi uniforme
\newcommand{\Ber}{\mathrm{Ber}} % loi de Bernoulli
\newcommand{\Bin}{\mathrm{Bin}} % loi binomiale
\newcommand{\Geo}{\mathrm{Geo}} % loi géométrique
\newcommand{\Poi}{\mathrm{Poi}} % loi de Poisson
\newcommand{\indi}{\mathds{1}} % fonction indicatrice
\newcommand{\Var}{\mathrm{Var}} % variance
\newcommand{\Cov}{\mathrm{Cov}} % covariance
\newcommand{\Exp}{\mathrm{Exp}} % loi exponentielle
\newcommand{\Nor}{\mathcal{N}} % loi normale
\DeclareMathOperator{\ind}{\perp \!\!\! \perp}

\begin{document}

\begin{titlepage}
\newgeometry{margin=2.7cm}
\thispagestyle{empty}
\begin{center}
\vspace*{7cm}
\Huge \textsc{Probabilités IV}\\
\vspace{1.5cm}
\Large Pierre Gimalac\\
\vspace{0.5cm}
\large \textit{Licence de Mathématiques}
\vfill
\end{center}
\large \textit{Janvier - Mai 2018}
\hfill 
\large Cours de Cyrille Lucas
\restoregeometry
\end{titlepage}

\renewcommand{\contentsname}{Sommaire}
\thispagestyle{empty}
\tableofcontents
\thispagestyle{empty}

\chapter{Probabilités finies}
\section{Définition ensembliste d'une expérience aléatoire}
\subsection{Expérience aléatoire}
\subsubsection{Définition}
Une expérience aléatoire est une expérience dont l'issue est incertaine.

\subsubsection{Ensemble des résultats possibles}
On associe à une expérience l'ensemble des résultats possibles, noté $\Omega$, appelé l'univers.\\

Dans le cadre du chapitre 1, $\Omega$ sera fini. On note alors $\abs{\Omega}$ ou $\#\Omega$ le cardinal de cet ensemble.

\subsubsection{Exemples}
\begin{itemize}
\item pile ou face : $\Omega=\{P,F\}$ ou $\{0,1\}$ (selon la notation).
\item jeté de dés : $\Omega=\{1,2,3,4,5,6\}$ pour un dé à 6 faces.
\item tirage de cartes : $\Omega=\{1,2,...,10,V,D,R\}\times \{\clubsuit,\spadesuit,\lozenge,\heartsuit\}$.
\item certains phénomène physiques comme la désintégration d'un atome radioactif : $\Omega=\R$.
\end{itemize}

\subsection{Vocabulaire ensembliste}
Soient $A,B\in \Omega^2$.

\subsubsection{Événement}
On appelle événement tout sous ensemble de $\Omega$.

\subsubsection{Ensemble des évènements}
L'ensemble des évènements est $P(\Omega)$.

\subsubsection{Notation}
Les éléments de $\Omega$ sont en général désignés par $\omega$.\\

$\{\omega\}$ est un évènement, on le lit "l'expérience aléatoire a produit le résultat $\omega$".

\subsubsection{Résultat}
On dit que $\omega\in \Omega$ est un résultat possible.

\subsubsection{Implication}
Le fait qu'un évènement $A$ implique un évènement $B$ se note $A\subset B$.

\subsubsection{Union}
L'évènement défini par la réalisation de $A$ ou de $B$ est $A\cup B$.

\subsubsection{Intersection}
L'évènement défini par la réalisation de $A$ et de $B$ est $A\cap B$.

\subsubsection{Complémentaire}
L'évènement défini par la non-réalisation de $A$ est $\Omega\backslash\{A\}$ ou encore $A^c$ ou ${}^cA$.

\subsubsection{Évènement impossible}
L'évènement impossible est $\emptyset$.

\subsubsection{Évènement certain}
L'évènement certain est $\Omega$.

\subsubsection{Évènements incompatibles}
On note l'incompatibilité de deux évènements $A$ et $B$ $A\cap B=\emptyset$.

\subsection{Observation de plusieurs expériences}
On a deux expériences aléatoires admettant respectivement $\Omega_1$ et $\Omega_2$ comme univers.

\subsubsection{Univers de deux expériences}
On note $\Omega_1\times \Omega_2$ l'ensemble des couples $(\omega_1,\omega_2)$ où $\omega_1\in \Omega_1$, $\omega_2\in\Omega_2$.\\

$\Omega=\Omega_1\times\Omega_2$ est l'univers correspondant à la réalisation des deux expériences aléatoires.

\subsubsection{Exemple}
Le lancé de deux dés distincts (un rouge et un vert) à six faces.\\

On a $\Omega_1=\Omega_2=\{1,2,3,4,5,6\}$ où $\Omega_1$ et $\Omega_2$ sont les univers de chacun des dés et alors $\Omega=\Omega_1\times \Omega_2$ est l'univers des possibles du lancé de deux dés : $\Omega=\{(1,1),(1,2),...,(6,6)\}$.

\subsection{Règles de dénombrement}
\subsubsection{Permutations}
Le nombre de permutations de $\{1,...,n\}$ où $n\in \N^*$ est $n!$.\\

La démonstration se fait facilement par récurrence.

\subsubsection{Lemme des Bergers}
Soit $k\geq 1$ un entier, soient $E$ et $F$ deux ensembles finis, soit $f : E\rightarrow F$ une application telle que $\forall b\in F$, $\#\{a\in E, f(a)=b\}=k$, alors $\#E=k\#F$.

\subsubsection{Arrangements}
Le nombre de façons de choisir $k\in \N$, $k\leq n$ éléments sans répétition en tenant compte de l'ordre dans un ensemble à $n$ éléments est $A^k_n=\frac{n!}{(n-k)!}$.

\subsubsection{Combinaisons}
Le nombre de façons de choisir $k\in \N$, $k\leq n$ éléments sans répétition sans tenir compte de l'ordre dans un ensemble à $n$ éléments est $C^k_n=\binom{n}{k}=\frac{n!}{k!(n-k)!}$.\\

Il s'agit du nombre de sous ensembles à $k$ éléments d'un ensemble à $n$ éléments.

\subsubsection{Preuve}
Appelons $E$ l'ensemble des $k$-uplets d'éléments de $\{1,...,n\}$ qui n'ont pas de répétition.\\

$E=\{(x_1,...,x_k)\tq \forall i\in \llbracket 1;k\rrbracket$, $x_i\in \llbracket 1;n\rrbracket$ et $\forall j\neq i, x_i\neq x_j\}$. $\#E=\frac{n!}{(n-k)!}$.\\

Soit $F$ l'ensemble des sous-ensembles de $\{1,...,n\}$ à $k$ éléments : $F=\{A\subset \{1,...,n\}\tq \#A=k\}$.

On pose alors $f:E\rightarrow F$, $(x_1,...,x_n)\mapsto \{x_1,...,x_n\}$.\\
Soit $b\in F$, $b$ s'écrit ${b_1...,b_k}$. $\{a\in E\tq f(a)=b\}$ est l'ensemble des permutations de $\{b_1...,b_k\}$ dont le cardinal est $k!$.\\

D'après le lemme des bergers, $\#E=k!\#F$.

\subsubsection{Binôme de Newton}
Soient $a,b\in \R$, $n\in \N$, $\displaystyle (a+b)^n =\sum_{k=0}^n\binom{n}{k}a^kb^{n-k}$.\\

La preuve se fait par récurrence.

\subsubsection{Remarque}
Cette formule se généralise à n'importe quels éléments commutant d'un anneau.

\section{Espace de probabilité fini}
\subsection{Mesure de probabilités}
\subsubsection{Principe}
L'idée est de mesurer la vraisemblance qu'un évènement se produise.

\subsubsection{Axiomes de probabilités}
Soit $\Omega$ un ensemble fini, on appelle probabilité (ou mesure de probabilités) sur $\Omega$ toute fonction $\P$ définie sur $P(\Omega)$ telle que :
\begin{enumerate}
\item Soit $\omega\in P(\Omega)$, $\P(\{\omega\})\in [0,1]$.
\item $\P(\Omega)=1$.
\item $\forall A$, $B$ deux évènements incompatibles, $P(A\cup B)=\P(A)+\P(B)$.\\
Cet axiome est appelé propriété d'additivité.
\end{enumerate}

\subsubsection{Remarque}
On a $\Omega\cup \emptyset=\Omega$ et $\Omega\cap \emptyset=\emptyset$ donc $\P(\Omega\cup \emptyset)=\P(\Omega)+\P(\emptyset)=\P(\Omega)$ d'où  $\P(\emptyset)=0$.

\subsection{Espace de probabilités}
Le couple $(\Omega, \P)$ est appelé espace de probabilité. Il peut aussi être noté $(\Omega, P(\Omega),\P)$.

\subsection{Propriété}
Soit $(\Omega,\P)$ un espace de probabilité fini, alors pour tout évènement $A\in P(\Omega)$\\on a $\displaystyle \P(A)=\sum_{\omega\in A}\P(\{\omega\})$.

\subsubsection{Preuve}
$A=\cup_{\omega\in A}\{\omega\}$ est une réunion $\displaystyle \P(A)=\sum_{\omega\in A}\P(\{\omega\})$.

\subsection{Famille de poids}
La famille $(p_\omega)_{\omega\in \Omega}=(\P(\{\omega\}))_{\omega\in\Omega}$, appelée famille de poids caractérise $\P$, et vérifie $\displaystyle \sum_{\omega\in \Omega}p_\omega=1$.\\

Réciproquement, si $(q_\omega)_{\omega\in\Omega}$ est une famille de réels tels que $\forall \omega\in \Omega$, $q_\omega\in [0,1]$ et $\displaystyle \sum_{\omega\in \Omega}q_\omega=1$, la fonction $Q:P(\Omega)\rightarrow [0,1]$, $\displaystyle A\mapsto \sum_{\omega\in A}q_\omega$ est une probabilité sur $\Omega$.

\subsubsection{Preuve}
$\displaystyle Q(\Omega)=\sum_{\omega\in \Omega}q_\omega=1$ et on prouve au passage $\forall A\in P(\Omega)$, $Q(A)\leq 1$.\\

Soient $A$ et $B$, deux évènements tels que $A\cap B=\emptyset$,\\$\displaystyle Q(A\cup B)=\sum_{\omega\in A\cup B}q_\omega=\sum_{\omega\in A}q_\omega+\sum_{\omega\in B}q_\omega=Q(A)+Q(B)$ donc $Q$ est additive et $Q$ est une probabilité sur l'ensemble fini $\Omega$.

\subsection{Probabilité uniforme}
Soit $\Omega$ fini, on appelle mesure de probabilité uniforme sur $\Omega$ la fonction $\P$ définie par $\forall A\in P(\Omega)$, $\P(A)=\frac{\#A}{\#\Omega}$. Il s'agit d'une mesure de probabilité.

\subsubsection{Preuve}
$\P$ est la probabilité associée à la famille de poids $p_\omega=\P(\{\omega\})=\frac{1}{\#\Omega}$.\\

$\forall \omega\in \Omega$, $p_\omega\in [0,1]$ et $\displaystyle \sum_{\omega\in \Omega}p_\omega=1$.\\

On l'appelle probabilité uniforme car $\forall \omega$, $\omega'\in \Omega, p_\omega=p_{\omega'}$.

\newpage

\subsection{Probabilité binomiale}
On donne $N\geq 1$ et $p\in [0;1]$. On appelle probabilité binomiale de paramètres $(N,p)$ la probabilité sur $\llbracket 0,N\rrbracket=\{0,...,N\}$ associé à la famille de poids $\displaystyle p_k=\binom{N}{k}p^k(1-p)^{N-k}=\P(\{k\})$.

\subsubsection{Preuve}
\begin{enumerate}
\item $\forall k\in \llbracket 0,N\rrbracket$, $p_k\geq 0$
\item $\displaystyle \sum_{k=0}^Np_k=\sum_{k=0}^N\binom{N}{k}p^k(1-p)^{N-k}$. D'après le binôme de Newton, $\displaystyle (a+b)^N=\sum_{k=0}^N\binom{N}{k}a^kb^{N-k}$ donc $\displaystyle \sum_{k=0}^Np_k=(p+(1-p))^{N}=1$.
\end{enumerate}

\subsection{Formules de calcul}
Soit $(\Omega,\P)$ un espace probabilisé fini.

\subsubsection{Propositions}\label{prop1-II.3.1}
\begin{itemize}
\item $\forall A\in P(\Omega)$, $\P(A^c)=1-\P(A)$.
\item $\forall A,B\in P(\Omega)$, si $B\subseteq A$ alors $\P(B)=P(A)-P(A\backslash B)$.
\item $\forall A,B\in P(\Omega)$, $\P(A\cup B)=\P(A)+\P(B)-\P(A\cap B)$.
\end{itemize}

\subsubsection{Preuve}
Soient $A$,$B$ deux évènements avec $B\subseteq A$, $B$ et $A\backslash B$ vérifient $B\cap A\backslash B=\emptyset$ donc d'après la propriété d'additivité, $\P(B)+\P(A\backslash B)=\P(B\cup (A\backslash B))=\P(A)$.\\

$A\cup B=A\cup (B\backslash A)=A\cup (B\backslash (A\cap B))$ et cette union est disjointe.\\

Alors $\P(A\cup B)=\P(A)+\P(B\backslash (A\cap B))=\P(A)+\P(B)-\P(A\cap B)$ d'après la première partie de la preuve.

\subsubsection{Proposition}
$\forall n\in \N^*$, si $(A_n)$ est une famille finie d'ensembles disjoints telle que $\forall i,j\leq n$,\\$i\neq j \Rightarrow A_i\cap A_j=\emptyset$, alors $\displaystyle \P(\bigcup_{i=1}^{n-1}A_i)=\sum_{i=1}^n\P(A_i)$.

\subsubsection{Preuve}
Par récurrence sur $n$ : $\displaystyle (\bigcup_{i=1}^{n-1}A_i)$ et $A_n$ sont disjoints donc $\displaystyle \P(\bigcup_{i=1}^nA_i)=\P(\bigcup_{i=1}^{n-1}A_i)+\P(A_n)$.

\subsubsection{Remarque}
Cela fonctionne pour des unions finies.

\subsubsection{Formule de Poincaré}
Soit $(A_i)_{1\leq i\leq n}$ une famille finie d'évènements d'un espace probabilisé $(\Omega,\P)$ fini.
$$ \P(\bigcup_{i=1}^nA_i)=\sum_{1\leq l\leq n}(-1)^{l+1}~~~~\sum_{\mathclap{1\leq i_1<...<i_l\leq n}}~~~~\P(\cap_{j=1}^lA_{i,j}) $$

\subsubsection{Preuve}
La preuve se fait facilement par récurrence.

\subsubsection{Remarque}
En particulier, $\P(A\cup B\cup C)=\P(A)+\P(B)+\P(C)-\P(A\cap B)-\P(B\cap C)-\P(C\cap A)+\P(A\cap B\cap C)$.

\subsubsection{Monotonicité}
Si $A\subseteq B$, $\P(A)\leq \P(B)$.

\subsubsection{Preuve}
La preuve se déduit de \ref{prop1-II.3.1} page \pageref{prop1-II.3.1}, deuxième proposition.

\section{Variables aléatoires}
\subsection{Définition}
On appelle variable aléatoire toute application de $\Omega$ fini dans $\R$. Une variable aléatoire sert à désigner le résultat d'une expérience aléatoire.

\subsubsection{Exemple}
On pose $\Omega=\{1,2,3,4,5,6\}^3$ qui modélise l'ensemble des résultats d'un lancer de 3 dés équilibrés à 6 faces.

$X_3:\begin{array}{rcl}
\Omega&\rightarrow& \R\\
(x_1,x_2,x_3)&\mapsto& x_3
\end{array}$ est une variable aléatoire qui modélise le résultat du troisième lancer. On mettra par la suite la probabilité uniforme sur $\Omega$ (puisque le dé est équilibré et les lancers sont successifs).

\subsubsection{Images directe et réciproque}
On s'intéresse aux résultats possibles de la variable aléatoire, c'est l'image directe, mais aussi pour chaque résultat x possible, à son image réciproque : l'ensemble des issues (de $\Omega$) qui donnent lieu à ce résultat (dans $\R$).

\subsubsection{Exemple} 
En reprenant l'exemple précédant, on que l'image directe de $X_3$ est $X_3(\{1,...,6\}^3)$ et l'image réciproque de $\{1\}$ est $X_3^{-1}(\{1\})=\{\omega\in \Omega\tq X_3(\omega)=1\}=\{(x,y,1)$ où $x,y\in \llbracket 1;6\rrbracket\}$.

\subsection{Loi d'une variable aléatoire}
\subsubsection{Idée}
On veut transformer l'information que l'on a sur $P(\Omega)$ en une information sur la variable aléatoire.

\subsubsection{Définition}
Soit $(\Omega,\P)$ un espace probabilisé fini, $X :\Omega \rightarrow \R$ une variable aléatoire. On appelle loi de la variable aléatoire $X$ la famille de poids $(\P(X^{-1}(\{x_k\})))_{x_k\in \Ima(X)}$ où $\Ima(X)=\{x_1,...,x_k,...,x_N\}$ est l'ensemble fini des valeurs réelles prises par $X$.

\subsubsection{Remarque}
On écrit indifféremment $X^{-1}(\{x_k\})=\{\omega\in \Omega, X(\omega)\in \{x_k\}\}$=$\{\omega\in \Omega, X(\omega)=x_k\}$=$\{X=x_k\}$ et $\P(\{X=x_k\})=\P(X=x_k)$.

\subsubsection{Remarque}
La probabilité associée ) cette famille de poids (également appelée "loi de $X$") est donnée par $\displaystyle Q(A)=\sum_{\mathclap{x\in \Ima(X)\cap A}}\P(X$=$x)$=$\displaystyle \sum_{\mathclap{x\in \Ima(X)\cap A}}\P(X^{-1}(\{x\}))$ et les $X^{-1}(\{x\})$ sont disjoints (une seule image par $\omega\in \Omega$) donc $\displaystyle Q(A)=\P(\bigcup_{\mathclap{x\in \Ima(X)\cap A}} X^{-1}(\{x\}))=\P(X^{-1}(A))=\P(X\in A)$.

\subsubsection{Loi de Bernoulli}
Pour $\Omega=\{P,F\}$, $X :\{P,F\}\rightarrow \R$ telle que $X(P)=0$, $X(F)=1$. On se donne $p\in [0,1]$ et $\P$ la probabilité sur $\Omega$ vérifiant $\P(\{P\})=1-p$, $\P(\{F\})=p$.\\

On dit que $X$ suit une loi de Bernoulli de paramètre $p$ et on note $X\sim \Ber(p)$.

\subsubsection{Preuve}
On a $\Ima(X)$=$\{0,1\}$, $\P(X^{-1}(\{0\}))$=$\P(X$=$0)$=$\P(\{P\})$=$1-p$ et $\P(X^{-1}(\{1\}))$=$\P(X$=$1)$=$\P(\{F\})$=$p$.

\subsubsection{Loi binomiale}
Soit $N$ un entier fini, $\Omega=\{P,F\}^N$. On munit $\Omega$ de la probabilité définie par\\
$\displaystyle \P(\{(a_1,...,a_N)\})=p^{\#\{i\leq N, a_i=F\}}\times (1-p)^{\#\{j\leq N, a_j=P\}}=p^{\#\{i\leq N, a_i=F\}}\times (1-p)^{N-\#\{i\leq N, a_i=F\}}$.\\

On considère la variable aléatoire $X$ : $\Omega\rightarrow \R$, $(a_1,...,a_n)\mapsto \#\{i\leq N, a_i=F\}$.\\

On a $\Ima(X)=\llbracket 0,N\rrbracket$. $P(X=k)=\P(\{(a_1,...,a_n)\in \Omega$, $\#\{i\leq N, a_i=F\}=k\})$.\\

On remarque que $\forall \omega \in X^{-1}(\{k\})$, $\P(\{\omega\})=p^k(1-p)^{N-k}$. On veut donc calculer $\#X^{-1}(\{k\})$. On remarque que les positions des $F$ sont les sous ensembles à $k$ éléments de $\{1,...,N\}$. Il y a donc $\binom{N}{k}$.\\

On en déduit que $\P(X=k)=\binom{N}{k}p^k(1-p)^{N-k}$. On dit alors que $X$ suit la loi binomiale de paramètres $(N,p)$.

\section{Exemples classiques}
\subsection{Main de poker}
On considère une main de poker : c'est à dire les sous ensembles à 5 éléments de $C_{52}=\{1,2,...,10,V,D,R\}\times \{\clubsuit,\spadesuit,\lozenge,\heartsuit\}$.\\

On appelle $\Omega_1$ l'ensemble des mains possibles, $\#\Omega_1=\binom{52}{5}=2598960$ (on a ici une probabilité uniforme de tirer chaque carte).\\

On pose $A$ l'ensemble des mains contenant un carré. $\#A=13\cdot 48$.\\

La probabilité de tirer une main contenant un carré lors d'un tirage uniforme dans un jeu de $52$ cartes est $p=\frac{\# A}{\#\Omega}=\frac{13\cdot 48}{\binom{5}{52}}\approx 2.40\cdot 10^{-4}$.\\

On pose $B$ l'ensemble des mains contenant un full, c'est-à-dire 3 cartes de même hauteur et les 2 autres de même hauteur : $\# B=13\cdot \binom{4}{3}\cdot 12\cdot \binom{4}{2}=13\cdot 4\cdot 12\cdot 6=3744$. Alors la probabilité d'obtenir un full lors du tirage d'une main uniforme est $p_B=\frac{\# B}{\#\Omega}\approx 1.44\cdot 10^{-3}$.

\subsection{Roulette française}
Une roulette est composée de 37 cases numérotées de 0 à 36. La case numérotée 0 est verte, les cases impaires de 1 à 35 sont noires et les cases paires de 2 à 36 sont rouges. La probabilité est uniforme d'avoir chaque case.\\

On a alors que la probabilité d'obtenir une case verte est $\frac{1}{37}$, la probabilité d'obtenir une case rouge est $\frac{18}{37}$ et la probabilité d'obtenir une case verte est $\frac{18}{37}$.\\

On réalise 10 lancers successifs, on veut connaître la probabilité d'obtenir 6 fois une case noire, 3 fois une case rouge et une fois la case verte (sans ordre).\\

On pose $\Omega=\{0,...,36\}^{10}$, on a $\#\Omega=37^{10}$ et on munit $\Omega$ de la probabilité uniforme.\\

On considère $A\subset \Omega$ qui représente l'évènement. Pour calculer $\P(A)$, il suffit de calculer $\# A$ puis $\frac{\# A}{\#\Omega}$.
$\# A=\binom{10}{1}\times \binom{9}{6}\times \binom{3}{3}\times 18^6\times 18^3\times 1=\binom{10}{6,3,1}\times 1\times 18^3\times 18^6$.

\hspace*{1.95cm}\begin{tikzpicture}
\draw[->] (0,0) -- (0,2);
\node at (1.8,-0.2) {Choix de la place du vert};

\draw[->] (0.8,0.33) -- (0.8,2);
\node at (2.75,0.1) {Choix de la place des noires};

\draw[->] (1.9,0.66) -- (1.9,2);
\node at (3.8,0.4) {Choix de la place des rouges};

\draw[->] (2.8,1) -- (2.8,2);
\node at (4,0.78) {Numéros des noires};

\draw[->] (3.8,1.33) -- (3.8,2);
\node at (5,1.1) {Numéros des rouges};

\draw[->] (4.55,1.66) -- (4.55,2);
\node at (5.6,1.45) {Numéro du vert};
\end{tikzpicture}

On a donc $\P(A)=\frac{\# A}{\# \Omega}=\binom{10}{6,3,1}\times \frac{1}{37}\times \left(\frac{18}{37}\right)^3\times \left(\frac{18}{37}\right)^6$.

\subsubsection{Coefficient multinomial}
On définit $\binom{n}{k_1,...,k_p}=\binom{n}{k_1}\times \binom{n-k_1}{k_2}\times ...\times \binom{n-k_1-...-k_{p-1}}{k_p}=\frac{n!}{k_1!...k_p!}$ où $n,k_1,...,k_p\in \N$ et $\displaystyle n=\sum_{i=1}^p k_i$.\\

\chapter{Probabilités discrètes}
\section{Univers et évènements de cardinal infini}
\subsection*{Motivation}
On veut étendre les résultats du chapitre 1 à des univers qui englobent un nombre infini de résultats possibles.

\subsubsection*{Exemple}
On joue à pile ou face jusqu'à obtenir face pour la première fois. On s'intéresse au nombre de parties jouées.

\subsection{Ensembles dénombrables}
\subsubsection{Définition}
On dit qu'un ensemble $\Omega$ est dénombrable s'il existe une bijection $\phi : \Omega \rightarrow \N$.

\subsubsection{Théorème de Cantor-Bernstein}
Soient E et F deux ensembles, s'il existe une injection de E dans F et une injection de F dans E alors il existe une bijection de E dans F.

\subsubsection{Application}
Soit $\phi_1: \N\rightarrow\Q$, $n\mapsto n$. $\phi_1$ est injective.

Soit $\phi_2: \Q\rightarrow \N$, $\frac{p}{q}\mapsto 2^{\abs{p}}3^q5^{signe(\frac{p}{q})}$ avec $p\in \Z$ et $q\in \N^0$ tels que $pgcd(p,q)=1$, $signe(\frac{p}{q}=1$ si $\frac{p}{q}>0$ et $0$ sinon). $\phi_2$ est injective.\\

D'après le théorème, $\Q$ est dénombrable.

\subsubsection{Propriétés et exemples d'ensembles dénombrables}
$\N\times \N$ est dénombrable, si n est un entier naturel et $A_1,...,A_n$ des ensembles dénombrables alors $A_1\times...\times A_n$ est dénombrable.\\

Si $A$ et $B$ sont dénombrables, $A\cup B$ est dénombrable et $A\cap B$ est dénombrable ou fini. Si les $(A_i)_{i\in \N}$ sont dénombrables, leur union est dénombrable.\\

$\{0,1\}^\N$ n'est pas dénombrable. $\P(\N)$ et $\R$ non plus. Ces trois ensembles indénombrables sont en bijections.

\subsection{Mesure de probabilité}
\subsubsection{Mesure de probabilité sur un ensemble dénombrable}
Soit $\Omega$ un ensemble dénombrable, on appelle probabilité ou mesure de probabilité toute fonction $\P : P(\Omega)\rightarrow [0,1]$ vérifiant $\P(\Omega)=1$ et $\forall (A_i)_{i\in \N}$ famille dénombrable d'éléments disjoints, $\displaystyle\P(\bigcup_{i\in \N} A_i)=\sum_{i\in \N}\P(A_i)$.

\subsubsection{Remarque}
La deuxième propriété est appelée formule de $\sigma$-additivité.

\subsubsection{Remarque}
On remarque que $\displaystyle \sum_{i\in \N} \P(A_i)$ est une série convergente.

\subsection{Rappels sur les séries à termes positifs}
Soit $(a_n)_{n\in \N}$ une suite de réels à valeurs positives. On a deux cas possibles:
\begin{itemize}
\item La suite $\displaystyle (\sum_{k=1}^n a_k)_{n\in \N}$ est bornée, la série est convergente, la limite existe et se note $\displaystyle \sum_{k\geq 1}a_k$.
\item La suite n'est pas bornée, on dit que la série n'est pas sommable, on note $\displaystyle \sum_{k\geq 1}a_k=+\infty$.
\end{itemize}

\subsection{Premières propriétés de $\P$}
\subsubsection{Propriété}\label{prop1.4.1}
Pour toute suite croissante d'évènements $(A_n)_{n\in \N}$, on a $\displaystyle \P(\bigcup_{n\in \N} A_n)=\lim\limits_{n\rightarrow +\infty} \P(A_n)$.\\

\begin{itemize}[label=$\cdot$]
\item $A_n$ est croissant si et seulement si $\forall i\leq j$, $A_i\subseteq A_j$
\item $\displaystyle x\in \bigcup_{n\in \N}A_n$ si et seulement si $\exists m\in \N$ tel que $x\in A_m$.
\end{itemize}

\subsubsection{Preuve}
On introduit $B_0=A_0$ et pour $n\in \N^*$, $B_n=A_n\backslash A_{n-1}$.\\
Alors les $B_n$ forment une famille disjointe.\\

$\displaystyle \bigcup_{n\in\N} B_n=\bigcup_{n\in \N} A_n$ car si $\displaystyle x\in \bigcup_{n\in \N}B_n$ alors il existe k tel que $x\in B_k=A_k\backslash A_{k-1}$ donc $x\in A_k$ et $\displaystyle x\in \bigcup A_n$. Réciproquement, si $\displaystyle x\in \bigcup_{n\in \N}A_n$, posons $k=\min\{n\tq x\in A_n\}$, on remarque que $x\in A_k$ et $x\notin A_{k-1}$ donc $x\in B_k$ donc $\displaystyle x\in \bigcup_{n\in \N}B_n$.\\

On a donc $\displaystyle \bigcup_{n\in \N} A_n=\bigcup_{n\in \N}B_n$ donc $\displaystyle \P(\bigcup_{n\in\N}A_n)=\P(\bigcup_{n\in\N}B_n)$, or $\displaystyle \bigcup_{n\in\N}B_n$ est une union disjointe donc d'après la formule de $\sigma$-additivité, $\displaystyle \P(\bigcup_{n\in\N}B_n)=\lim\limits_{n\rightarrow +\infty} \sum_{k=0}^n\P(B_k)$.

\subsubsection{Intermède}
Pour montrer que $\P$ est additive, on commence par montrer que $\P(\emptyset)=0$. On considère la famille $(\Omega,\emptyset, ...)=(A_n)_{n\in\N}$, $\displaystyle \bigcup_{n\in\N}A_n=\Omega$ donc d'après la formule de $\sigma$-additivité, $\displaystyle \P(\Omega)=\P(\Omega)+\sum_{k\geq 1}\P(\emptyset)$ donc $\P(\emptyset)=0$.\\

Soient $A$ et $B$ disjoints, $\P(A\cup B)=\P(A\cup B\cup \emptyset\cup...)=\P(A)+\P(B)$ donc $\P$ vérifie l'additivité et si $A\subseteq B$, $\P(A\backslash B)=\P(B)-\P(A)$.\\
Donc $\P(B_k)=\P(A_k)-\P(A_{k-1})$ et $\P(B_0)=\P(A_0)$ donc $\displaystyle \P(\bigcup_{n\in \N} B_n)=\lim_{x\rightarrow +\infty}\P(A_0)+\sum_{k=1}^n\P(A_k)-\P(A_{k-1})=\lim_{x\rightarrow +\infty}\P(A_n)=\P(\bigcup_{n\in \N}A_n)$.

\subsubsection{Remarque}
Au cours de cette preuve, on a vu que la $\sigma$-additivité impliquait l'additivité.\\
Donc les formules de calcul vues au chapitre précédent sont encore valables.

\subsection{Proposition}
Soient A, B deux parties de $\Omega$.\\

\begin{enumerate}
\item $\P(A^c)=1-\P(A)$
\item si $B\subseteq A$, $\P(B)=\P(A)-\P(A\backslash B)$
\item $\P(A\cup B)=\P(A)+\P(B)-\P(A\cap B)$
\item $\P$ vérifie la formule de Poincaré écrite au chapitre précédent
\item si $B\subseteq A$, alors $\P(B)\leq \P(A)$ (propriété de monotonicité)
\end{enumerate}

\subsubsection{Preuve}
$\P$ est $\sigma$-additive donc additive et les preuves du chapitre 1 fonctionnent.

\subsection{Sous-additivité}
Pour toute famille $(A_n)_{n\in \N}$ d'évènement quelconques, $\displaystyle \P(\bigcup_{i\in \N}A_i)\leq \sum_{i=0}^\infty \P(A_i)$

\subsubsection{Preuve}
$\displaystyle \bigcup_{i\in \N}A_i=\bigcup_{i\in \N}(A_i\backslash \bigcup_{k=0}^{i-1}A_k)$ et la famille $\displaystyle (A_i\backslash (\bigcup_{i=0}^{i-1}A_k))_{i\in \N}$ est une famille d'ensembles deux à deux disjoints.\\

$\displaystyle \P(\bigcup_{i\in \N}(A_i\backslash \bigcup_{k=0}^{i-1}A_k))=\sum_{i\in \N}\P(A_i\backslash \bigcup_{k=0}^{i-1}A_k))\leq \sum_{i\in \N}\P(A_i)$ par monotonicité.

\subsubsection{Remarque}
$\displaystyle \sum_{i\in \N}\P(A_i)$ peut valoir $+\infty$, être dans $[1;+\infty[$ ou être dans $[0;1[$ (c'est le cas intéressant).

\subsection{Mesure de probabilité}
La mesure de probabilité $\P$ est entièrement définie par la famille dénombrable de poids\\ $p_\omega=\P(\{\omega\})$ qui vérifie $(p_\omega)_{\omega\in \Omega}$ est à valeurs dans $[0,1]$, $\displaystyle \sum_{\omega\in \Omega}p_\omega=1$.

\subsubsection{Réciproquement}
La donnée d'une famille de poids vérifiant les deux propriétés ci-dessus définit la mesure de probabilité Q à travers la relation $\displaystyle \forall A\in P(\Omega), Q(A)=\sum_{\omega\in A}q_\omega$.

\subsubsection{Preuve}
Si $\P$ est une probabilité, $\forall \omega\in \Omega$, $\displaystyle \P(\{\omega\})=\P(\bigcup_{\omega\in \Omega}\{\omega\})$ car cette union est dénombrable et disjointe, donc l'égalité découle de la propriété de $\sigma$-additivité. $\displaystyle \P(\bigcup_{\omega\in \Omega}\{\omega\})=\P(\Omega)=1$.\\

Réciproquement, il faut vérifier que la fonction $Q$ : $P(\Omega)\rightarrow \R$, $\displaystyle A\mapsto \sum_{\omega\in A}q_\omega$ est une probabilité.\\
$Q(A)\geq 0$ car les $q_\omega$ sont positifs, $Q$ est monotone car on considère une somme de termes positifs, $\displaystyle Q(\Omega)=\sum_{\omega\in \Omega}q_\omega=1$ et donc $\forall A\in P(\Omega)$, $Q(A)\leq 1$.\\
Il reste à montrer la $\sigma$-additivité : soit $(A_n)$ une famille d'évènements de $P(\Omega)$ 2 à 2 disjoints, $$Q(\bigcup_{n\in \N}A_n)=\sum_{\omega\in\bigcup_{n\in\N}A_n}q_\omega=\sum_{n\in \N}\sum_{\omega\in A_n}q_\omega=\sum_{n\in \N}Q(A_n)$$
donc $Q$ est $\sigma$ additive et $Q$ est une mesure de probabilité.

\section{Variables aléatoires sur $\N$}
\subsection{Schéma de Bernoulli}
Il s'agit de représenter la succession d'expériences aléatoires ayant deux issues (succès et échec).\\

Dans un premier temps on s'intéresse à $N\in\N$ expériences avec $N\in\N$ fini.\\

On regarde l'espace probabilisé $(\{0,1\}^N,\P)$ où $\P$ est la probabilité associée à la famille de poids $\P(\{(a_1,...,a_N)\})=p^{\sum_{i\in [N]} a_i}(1-p)^{N-\sum_{i\in [N]} a_i}$.

\subsubsection{Remarque}
$[N]=\llbracket 1 ;N\rrbracket$ et donc $\sum_{i\in [N]} a_i$ est le nombre de 1 dans $(a_1,...,a_N)$. Il s'agit donc bien d'une famille de poids de masse totale 1.

\newpage

\subsection{Loi géométrique}
On regarde la fonction $T^N : \{0,1\}^N \rightarrow \N^*, (a_1,...,a_n)\mapsto \left\{\begin{array}{l}
k\text{ si }a_1=...=a_{k-1}=0\text{ et }a_k=1
\\
N+1 \text{ s'il n'y a que des }0
\end{array}\right.$.

$T^N$ est une variable aléatoire.\\

$T^N$ prend ses valeurs dans $\llbracket 1; N+1\rrbracket$, $(T^N)^{-1}(\{N+1\})=\{(0,...,0)\}$, $\P(T^N=N+1)=\P((T^N)^{-1}(\{N+1\}))=\P(\{(0,...,0)\})=(1-p)^N$.

Soit $k\in \{1,...,N\}$, 
$(T^N)^{-1}(\{k\})=\{(a_1,...,a_N)$ tels que $a_1=...=a_{k-1}=0$ et $a_k=1\}$ et 
$$\begin{array}{rcl}\P(T^N=k)
&=&\displaystyle \P((T^N)^{-1}(\{k\}))=\P(\{(a_1,...,a_N) \tq a_1=...=a_{k-1}=0\text{ et }a_k=1\})\\\\
&=&\displaystyle \P(\bigcup_{m=1}^{N-k+1}\{(a_1,...,a_N)\tq a_1=...=a_{k-1}=0\text{ et }a_k=1, \sum_{i\in[N]} a_i=m\})\\\\
&=&\displaystyle  \sum_{m=1}^{N-k+1}\P(\{(a_1,...,a_N), \sum_{i\in[N]}a_i=m, a_1=...=a_{k-1}=0\text{ et }a_k=1\})\\\\
&=& \displaystyle\sum_{m=1}^{N-k+1}p^m(1-p)^{N-m})\binom{N-k}{m-1}\\\\
&=& \displaystyle \sum_{j=0}^{N-k}p^{j+1}(1-p)^{N-j-1})\binom{N-k}{j}\\\\
&=&\displaystyle p(1-p)^{k-1}\sum_{j=0}^{N-k}\binom{j}{N-k}p^j(1-p)^{N-k-j}\\\\
&=&\displaystyle p(1-p)^{k-1}
\end{array}$$

\subsubsection{Remarque}
Cette probabilité ne dépend pas de N. On introduit donc la loi sur $\N^*$ appelée loi géométrique définie par $P(T=k)=p(1-p)^{k-1}$. On vérifie au passage que les $(p(1-p)^{k-1})_{k\in \N^*}$ forment une famille de poids de masse totale 1.\\

$\displaystyle\sum_{k=1}^{\infty} p(1-p)^{k-1}=p\sum_{k\in \N}(1-p)^k=\frac{p}{1-(1-p)}=1$.\\

On dit que T suit une loi géométrique de paramètre p et on note $T\sim \mathcal{G}eo(p)$.

\newpage

\subsection{Loi de Poisson}
On considère toujours le schéma de Bernoulli sur l'espace de probabilité $(\{0,1\}^N,\P)$, on appelle $X^N$ la variable aléatoire définie par $X^N :\begin{array}{rcl}\{0,1\}^N&\rightarrow& \N \\ (a_1,...,a_N)&\mapsto& \sum a_i\end{array}$.\\

On a $X_N\sim \Bin(N,p)$.\\

On souhaite décrire les évènements "rares" qui apparaissent lors de la répétition d'expériences (grand nombre, petite probabilité) : par exemple un accident d'avion ou une chute de cavalier lors d'un défilé.\\

Posons $p=\frac{\lambda}{N}$ avec $\lambda\in \Rpe$, et passons à la limite :\\
soit $k\in \N$, $\displaystyle \P(X^N=k)=\binom{N}{k}p^k(1-p)^{N-k}=\binom{N}{k}(\frac{\lambda}{N})^k(1-\frac{\lambda}{N})^{N-k}$.\\

Or $\displaystyle \binom{N}{k}\sim \frac{N^k}{k!}$, $\displaystyle (\frac{\lambda}{N})^k=\frac{\lambda^k}{N^k}$ et $\displaystyle \lime{(1-\frac{\lambda}{N})^{N-k}}{N}{+\infty}{e^{-\lambda}}$.\\

$\displaystyle \P(X^N=k)\sim\frac{N^k}{k!}\times \frac{\lambda^k}{N^k}\times e^{-\lambda}=e^{-\lambda}\frac{\lambda^k}{k!}$.\\

On introduit donc la loi de Poisson de paramètre $\lambda$ telle que $X\sim \Poi(\lambda)$ si et seulement si $\displaystyle \P(X=k)=e^{-\lambda}\frac{\lambda^k}{k!}$.

\subsubsection{Remarque}
$\displaystyle \sum_{k=0}^\infty \P(X=k)=1$ car $\displaystyle \sum_{k=0}^\infty e^{-\lambda}\frac{\lambda^k}{k!}=e^{-\lambda}\sum_{k=0}^\infty \frac{\lambda^k}{k!}=1$.

\chapter{Conditionnement et indépendance}
\section{Conditionnement}
\subsection{Cadre}
On dispose d'une partie de l'information, on veut calculer des probabilités "sachant que".

\subsubsection{Exemple}
Lors d'une partie de poker on connait sa propre main et la probabilité de victoire dépend de la main de l'adversaire.

\subsection{Probabilité conditionnelle}
Soit $(\Omega,\P)$ un espace probabilisé fini ou dénombrable. Soit $B$ un évènement tel que $\P(B)\neq 0$. On définit la "probabilité conditionnelle de $A$ sachant $B$" par $\P(A\tq B)=\frac{\P(A\cap B)}{\P(B)}$ également parfois notée $\P_B(A)$.

\subsection{Proposition}\label{proposition1.1.1}
L'application $\P_B$ : $P(\Omega)\rightarrow \R^+$, $A\mapsto \P_B(A)$ est une probabilité sur $\Omega$.

\subsubsection{Remarque}
Cela justifie la notation $\P_B(A)$.

\subsubsection{Démonstration}
\begin{itemize}
\item $\displaystyle \P_B(\Omega)=\frac{\P(\Omega\cap B)}{\P(B)}=\frac{\P(B)}{\P(B)}=1$
\item Soit $(A_n)_{n\in\N}$ une suite d'évènements disjoints, $\displaystyle \P_B(\bigcup_{n\geq 0}A_n)=\frac{\P(B\cap\bigcup_{n\geq 0}A_n)}{\P(B)}=\frac{\P(\bigcup_{n\geq 0}(A_n\cap B))}{\P(B)}$.\\

Or $(A_n\cap B)_{n\in \N}$ est une suite d'évènements disjoints et $\P$ est une probabilité donc par $\sigma$-additivité on a :\\
$$\P_B(\bigcup_{n\in\N} A_n)=\frac{\sum_{n\in \N}\P(A_n\cap B)}{\P(B)}=\sum_{n\in \N}\frac{\P(A_n\cap B)}{\P(B)}=\sum_{n\in \N}\P_B(A_n)$$

Ainsi $\P_B$ est $\sigma$-additive (donc additive et monotone, $\P_B(\Omega)=1$ donc à valeurs dans $[0,1]$).
\end{itemize}

\newpage

\subsection{Formules de probabilités totales}
\subsubsection{Partition finie}
$(A_n)_{1\leq n\leq N}$ est une partition finie de $\Omega$ si $\bigcup_{1\leq n\leq N}A_n=\Omega$ et $\forall i\neq j$, $A_i\cap A_j=\emptyset$.

\subsubsection{Proposition}\label{proposition1.2.1}
Soit $A_n$ une partition finie de $\Omega$, pour tout évènement $B\in P(\Omega)$, on a $\P(B)=\sum_{n=1}^N\P(B\cap A_n)$.

\subsubsection{Preuve}
$B=\bigcup_{n=1}^N(B\cap A_n)$ et cette union est disjointe.

\subsubsection{Proposition}\label{proposition1.2.2}
Soit $(A_n)_{1\leq n\leq N}$ une partition finie de $\Omega$, telle que $\forall n\in \llbracket 1,n\rrbracket$, $\P(A_n)>0$, alors pour tout évènement $B\in P(\Omega)$, on a $\P(B)=\sum_{n=1}^N\P(A_n)\P_{A_n}(B)$.

\subsubsection{Preuve}
$\P(B\cap A_n)=\P_{A_n}(B)\times \P(A_n)$ dans la proposition \ref{proposition1.2.1}.

\subsubsection{Exemple}
On cherche la probabilité de tirer au moins un as dans l'expérience imbriquée ci-dessous :\\
On lance une pièce équilibrée : on tire une carte si on obtient pile et deux si on obtient face.\\

Ici la description de l'expérience est conditionnelle, les probabilités auxquelles on a accès sont des probabilités conditionnelles. On note $A$ l'évènement "tirer au moins un as".\\

$\P_P(A)=\frac{4}{52}=\frac{1}{13}$ et 
$\P_F(A)=1-\frac{\binom{48}{2}}{\binom{52}{2}}=1-\frac{48\cdot 47}{52\cdot 51}$\\

$P$,$F$ est une partition finie de $\Omega$ donc d'après la formule des probabilités totales,\\on a $\P(A)=\P(P)\cdot \P_P(A)+\P(F)\cdot \P_F(A)=\frac{1}{26}+\frac{1}{2}-\frac{48\cdot 47}{52\cdot 51\cdot 2}$.

\subsection{Théorème de Bayes}
On s'intéresse à un conditionnement "renversé".\\
Mathématiquement, si $\P(A)>0$ et $\P(B)>0$, on peut définir $\P_B(A)$ et $\P_A(B)$.

\subsubsection{Proposition}\label{proposition1.3.1}
Soient $A$,$B$ deux évènements tels que $\P(A)>0$ et $\P(B)>0$, alors $\P_B(A)=\P_A(B)\cdot \frac{\P(A)}{\P(B)}$.

\subsubsection{Preuve}
$\P_B(A)=\frac{\P(A\cap B)}{\P(B)}=\frac{\P(A\cap B)}{\P(A)}\cdot \frac{\P(A)}{\P(B)}$.

\subsubsection{Théorème de Bayes}
Si l'on dispose d'une partition finie $(A_n)_{1\leq n\leq N}$ d'évènements de $\Omega$ de probabilités non nulles et $B$ un évènement de probabilité non nulle alors
$$\forall ~1\leq i\leq N,~ \P_B(A_i)=\frac{\P_{A_i}(B)\P(A_i)}{\sum_{n=1}^N\P_{A_n}(B)\P(A_n)}$$

\subsubsection{Exemple}
Dans l'expérience ci-dessus, $\P_{A}(P)=\frac{\P_P(B)\cdot \P(P)}{\P_P(B)\cdot \P(P)+\P_F(B)\cdot \P(F)}\simeq 0.34$.

\section{Indépendance}
\subsection{Indépendance de deux évènements}
On dit que deux évènements sont indépendants quand on constate que la réalisation ou non de l'un des deux n'affecte pas la probabilité de l'autre.

\subsubsection{Définition}
Soient $A$ et $B$ deux évènements, on dit que $A$ et $B$ sont indépendants (et on note $A\ind B$) pour la probabilité $\P$ si $\P(A\cap B)=\P(A)\cdot \P(B)$.

\subsubsection{Remarque}
$A \ind B \Leftrightarrow B\ind A$.

\subsubsection{Remarque}
$\emptyset$ est indépendant de tous les autres évènements.

\subsubsection{Remarque}
Informellement, on se demande si la réalisation ou non de A influe sur la probabilité de B. Si c'est le cas, il n'y a pas d'indépendance.

\subsection{Proposition}\label{proposition2.1.1}
Si $A$ est un évènement tel que $\P(A)\neq 0$, on a $A\ind B \Leftrightarrow \P_A(B)=\P(B)$.

\subsubsection{Preuve}
$\P_A(B)=\frac{\P(A\cap B)}{\P(A)}=\P(B)$.

\subsubsection{Remarque}
La notion d'indépendance peut parfois être contre intuitive.\\

On considère un schéma de Bernoulli avec $n=2$, $p=\frac{1}{2}$, on pose $A_1=\{X_1=1\}$, $A_2=\{X_2=1\}$ et $B=\{X_1+X_2=0\}\cup \{X_1+X_2=2\}$.\\

$\P(A_1)=\frac{1}{2}, \P(B)=\P(\{(0,0),(1,1)\})=\frac{1}{4}+\frac{1}{4}=\frac{1}{2}$.

$\P(A_1\cap B)=\P(\{(1,1)\})=\frac{1}{4}=\P(A_1)\times \P(B)$. Donc $B$ et $A_1$ sont indépendants.

\subsection{Indépendance de $n$ évènements}
Soit $n\in \N$, $n\geq 2$.\\

La notion d'indépendance ne suffit pas à décrire les familles de plus de deux évènements.

\subsubsection{Définition}
Soient $A_1,...,A_n$ des évènements, on dit que la famille $(A_1,...,A_n)$ est indépendante si et seulement si $\forall k\leq n$, $k\in\N^*$, $\forall 1\leq i_1<...<i_k\leq n$, $\displaystyle \P(\bigcap_{j=1}^kA_{i_j})=\prod_{j=1}^{k}\P(A_{i_j})$.

\subsection{Indépendance de variables aléatoires}
\subsubsection{Sommes doubles}
Soit $(a_{i,j})_{i,j\in \N^*{}^2}$ une famille de réels positifs, on pose $\displaystyle E=\{ \sum_{i=1}^n \sum_{j=1}^m a_{i,j}, n\in \N^*, m\in \N^* \}$.

Si $E$ est borné, on dit que la famille $(a_{i,j})_{i,j\in\N^*{}^2}$ est sommable et on note $\sum_{i,j\in \N^*{}^2}=\sup E$.

\subsection{Théorème de Fubini pour les séries positives}
$\displaystyle \sum_{i,j\in \N^*{}^2}a_{ij}=\sum_{i\geq 1}\sum_{j\geq 1}a_{i,j}=\sum_{j\geq 1}\sum_{i\geq 1}a_{i,j}$.\\

De plus, $(a_{ij})$ est sommable si et seulement si au moins une des séries est convergente.

\subsubsection{Preuve}
On montre que :
\begin{enumerate}
\item Si $u_i$ est une suite à termes positifs, $\displaystyle \sum_{i\geq 1}u_i=\sup\{\sum_{i=1}^nu_i, n\in \N^*\}$
\item Si $A_{i,j}$ sont des réels positifs, $\displaystyle \sup\{A_{ij}, i\in\N^*, j\in \N^* \}=\sup\{\sup\{ A_{i,j},j\in\N^* \}, i\in\N^* \}$.
\end{enumerate}
\bigskip

\begin{enumerate}
\item $\displaystyle (\sum_{i=1}^nu_i)_{n\in\N^*}$ est une suite croissante donc $\displaystyle \lim\limits_{n\rightarrow +\infty}\sum_{i=1}^nu_i=\sup\{\sum_{i=1}^nu_i, n\in\N^* \}$.\\
Cette égalité est vraie si la série converge mais aussi si la série diverge au sens où $+\infty=+\infty$.

\item On procède par double inégalité.\\

Soit $\epsilon>0$, on note $T_g$ le terme de gauche de l'égalité et $T_d$ le terme de droite.\\

Par définition de $\sup\{A_{i,j}, i\in \N^*, j\in \N^* \}$, il existe $n_0$ et $m_0$ tels que $A_{n_0,m_0}\geq T_g-\epsilon$ mais $A_{n_0,m_0}\leq\sup\{A_{n_0,j}, j\in\N^*\}$ et $A_{n_0,m_0}\leq\sup\{ \sup\{ A_{i,j},j\in\N^* \}, i\in \N^* \}$.\\

Donc $T_g-\epsilon\leq A_{n_0,m_0}\leq T_d$.\\

Par définition de $T_d$, il existe un indice $i_0$ tel que $\sup\{ A_{i_0,j}, j\in \N^* \}\geq T_d-\epsilon$ et par définition de ce $\sup$, il existe $j_0$ tel que $A_{i_0,j_0}\geq \sup\{ A_{i_0,j}, j\in \N^* \}-\epsilon\geq T_d-2\epsilon$. $T_g\geq T_d-2\epsilon$ donc $T_g=T_d$.
\end{enumerate}

\subsection{Théorème de Fubini pour les séries absolument convergentes}
Si $(a_{i,j})$ est une suite de réels telle que $\abs{a_{ij}}$ est sommable, alors $\displaystyle \sum_{i\geq 1}\sum_{j\geq 1} \abs{a_{i,j}}=\sum_{j\geq 1}\sum_{i\geq 1} \abs{a_{i,j}}$. On note $\sum_{i,j}a_{i,j}$.

\subsubsection{Preuve}
On sépare $a_{i,j}$ en $a_{i,j}^+$ et $a_{i,j}^-$ puis on applique le théorème précédent sur chacune.

\subsubsection{Remarque}
L'hypothèse $\sum\sum\abs{a_i,j}<+\infty$ est indispensable.

\subsection{Poids produits}
\subsubsection{Proposition}\label{proposition2.3.1}
Soient $\Omega_1,\Omega_2$ deux ensembles dénombrables, $(p_i^1)_{i\in\Omega_1}$ et $(p_j^2)_{j\in\Omega_2}$ deux familles de poids sur $\Omega_1$ et $\Omega_2$ respectivement, avec $p_i^1\geq 0$ et $p_j^2\geq 0$, $\displaystyle\sum_{i\in\Omega_1}p_i^1=1$ et $\displaystyle\sum_{j\in\Omega_2}p_j^1=1$.\\

Alors $q_{i,j}=p_i^1\cdot p_j^2$ définit une famille de poids sur l'ensemble $\Omega_1\times \Omega_2$.

\subsubsection{Preuve}
Évident.

\subsubsection{Notation et nomenclature}
$q_{i,j}$ s'appelle la famille des poids produits et la probabilité associée à cette famille de poids s'appelle la probabilité produit.

\subsection{Indépendance de variables aléatoires}
\subsubsection{Cas avec 2 variables aléatoires}
Soient $X_1, X_2$ deux variables aléatoires à valeurs dans $E_1$ et $E_2$ respectivement.\\
On dit que $X_1\ind X_2$ si et seulement si $$\forall x_1,x_2\in E_1\times E_2, ~\P(X_1=x_1\text{ et }X_2=x_2)=\P(X_1=x_1)\times \P(X_2=x_2)$$
c'est à dire si la famille des poids de la loi de $(X_1,X_2)$ sur $E_1\times E_2$ est la famille des poids produit.

\subsubsection{Cas avec $n$ variables aléatoires}
Soient $n\in \N^*$, $X_1,...,X_n$ des variables à valeurs dans $E_1,...,E_n$.\\
On dit que la famille est indépendante si et seulement si $$\forall x_1,...,x_n\in E_1\times...\times E_n,~\P(X_1=x_1\text{ et ... et }X_n=x_n)=\prod_{i=1}^{n}\P(X_i=x_i)$$

\subsubsection{Exemple}
On considère les variables correspondant aux lancers successifs dans un schéma de Bernoulli.\\

$\displaystyle \P(\{(a_1,...,a_n)\})=p^{\sum a_i}(1-p)^{n-\sum a_i}=\prod_{i=1}^{n}p^{\indi_{X_i=1}}(1-p)^{\indi_{X_i=0}}$ où $X_i$ est le résultat de la i-ème expérience.

\newpage

\subsubsection{Caractérisation de l'indépendance}
Soient $X$ et $Y$ deux variables aléatoires à valeurs dans tout $E$ et tout $F$ respectivement, alors $X$ et $Y$ sont indépendantes si et seulement si pour tout $A,B$ vérifiant $A\subseteq E$, $B\subseteq F$, on a $\P(X\in A\text{ et }Y\in B)=\P(X\in A)\P(Y\in B)$.\\

\textbf{Preuve}\\

Soient $A=\{a\}$ et $B=\{b\}$, avec $a\in E$ et $b \in F$,\\
$\P(X\in A \text{ et }Y\in B)=\P(X\in\{a\}) \P(Y\in\{b\})$, $\P(X=a\text{ et }Y=b)=\P(X=a)\P(Y=b)$ donc $X$ et $Y$ sont deux variables aléatoires indépendantes.\\

On suppose $X$ et $Y$ indépendantes, $$\P(\{\omega \in \Omega : X(\omega)\in A\text{ et }Y(\omega)\in B \})=\P(\bigcup_{\substack{a\in A\\b\in B}}\{\omega \in \Omega : X(\omega)=a\text{ et }Y(\omega)b \})$$

Cette union est dénombrable car $A\subseteq E=\Ima(X)$ et $B\subseteq F=\Ima(Y)$ sont dénombrables. De plus elle est disjointe car si $\omega_1\in \{\omega\in \Omega : X(\omega)=a_1, Y(\omega)=b_1 \}$ et\\
$\omega_1\in \{\omega\in \Omega, X(\omega)=a_2, Y(\omega)=b_2 \}$, alors $a_1=X(\omega_1)=a_2$ et $b_1=Y(\omega_1)=b_2$.\\\\

$\begin{array}{rcl}\displaystyle\text{Donc }\P(X\in A\text{ et }Y\in B)&=&\displaystyle\sum_{\substack{a\in A\\b\in B}}\P(\{\omega\in \Omega, X(\omega)=a\text{ et }Y(\omega)=b \})\\\\
&=& \displaystyle\sum_{\substack{a\in A\\b\in B}}\P(X=a)\P(Y=b) \\\\
&=& \displaystyle\sum_{a\in A}\sum_{b\in B}\P(X=a)\P(Y=b) \\\\
&=& \displaystyle\sum_{a\in A}\P(X=a)\sum_{b\in B}\P(Y=b)\\\\
&=& \displaystyle\P(X\in A)\P(Y\in B)
\end{array}$

\subsubsection{Non-exemple}
On considère le lancer de trois dés équilibrés à 6 faces de couleurs différentes (noir, blanc, rouge). On appelle $X$ et $Y$ les variables aléatoires donnant respectivement le résultat du dé rouge et la somme des trois dés. On veut savoir si $X$ et $Y$ sont indépendantes.\\

$X$ suit une loi uniforme sur $\Omega=\{1,2,3,4,5,6\}$ : \\ $P(Y=3)=\frac{1}{6}^3$, $\P(Y=18)=\frac{1}{6}^3$ et $\P(X=6\text{ et }Y=18)=\P(Y=18)=\frac{1}{6}^3$,\\
or $\P(X=6)\P(Y=18)=\frac{1}{6}^4$ donc $X$ et $Y$ ne sont pas indépendants.



\chapter{Espérance de variables aléatoires discrètes}
\section{Définition}
\subsection{Variables aléatoires positives}
Soit $X$ une variable aléatoire positive sur $(\Omega,\P)$. On appelle espérance de $X$ et l'on noté $\E[X]$ la quantité $\displaystyle \E[X]=\sum_{x\in \Ima(X)}x\P(X=x)$.\\

On dit que $\E[X]$ est le gain moyen théorique.

\subsubsection{Remarque}
$\E[X]$ existe dans $\R^*\cup\{+\infty\}$.\\

C'est une somme finie ou dénombrable et si elle est dénombrable, la série converge vers $+\infty$.

\subsection{Variables aléatoires sommables}
On dit que $X$ à valeurs dans $\R$ est une variable aléatoire sommable si et seulement si\\ $\E[\abs{X}]<+\infty$.

\subsubsection{Remarque}
$(\abs{X})$ est bien une variable aléatoire positive donc $\E[\abs{X}]$ est définie.

\subsection{Espérance d'une variable aléatoire sommable}
L'espérance d'une variable aléatoire sommable $X$ est $\E[X]=\sum_{x\in \Ima(X)}x\P(X=x)$.

\subsubsection{Remarque}
Cette série est absolument convergente car $\E[\abs{X}]<+\infty$.

\subsubsection{Preuve}
$\begin{array}{rcl}\displaystyle \sum_{x\in \Ima(X)}\abs{x}\P(X=x)&=&\displaystyle\sum_{x\in\Ima(X)\cap \R^-}\abs{x}\P(X=x)+\sum_{x\in \Ima(X)\cap \R^+}\abs{x}\P(X=x)\\&\leq& \displaystyle\sum_{x\in \Ima(\abs{X})}\abs{y}\P(\abs{X}=y)+\sum_{y\in \Ima(\abs{x})}\abs{z}\P(\abs{X}=y)<+\infty\end{array}$.\\

Donc $\sum x\P(X=x)$ est convergente.

\subsubsection{Remarque}
On dit parfois "admet une espérance" au lieu de "est sommable". On dit aussi $X\in \mathcal{L}^1$ ou $X$ est dans $\mathcal{L}^1$.

\subsubsection{Remarque}
La série étant absolument convergente, sa somme ne dépend pas de l'ordre de sommation des termes.

\subsubsection{Exemple}
On considère le jeu suivant : une série de pile ou face dans laquelle si le premier pile apparait à un indice $i$ pair, on gagne $\frac{2^i}{i}$, s'il apparait à un indice $i$ impair, on perd $\frac{2^i}{i}$.\\

$\begin{array}{rcl}G&=&\displaystyle \sum_{i\in \N^*}\P(\text{premier pile apparait en i})\times \frac{2^i}{i}(-1)^i\\
&=&\displaystyle \sum_{i\in \N^*}\frac{1}{2^i}\times \frac{2^i}{i}(-1)^i\\
&=&\displaystyle \sum_{i\in\N^*}\frac{(-1)^i}{i}<\infty\end{array}$

mais la variable aléatoire n'admet pas d'espérance.\\

Le calcul de $G$ dépend de l'ordre dans lequel on énumère les résultats.

\subsection{Propriétés de l'espérance}
\subsubsection{Définition}
On note $\mathcal{L}^1(\Omega,\P)$ l'ensemble des variables aléatoires sommables définies sur $(\Omega,\P)$.

\subsubsection{Propriété}
Soit $X\in \L^1(\Omega,\P)$, on a $\E[X]=\sum_{\omega\in \Omega}X(\omega)\P(\{\omega\})$.

\subsubsection{Preuve}
$\displaystyle \Omega=\bigcup_{\omega\in\Im(X)}X^{-1}(\{x\})$,

$\begin{array}{rcl}\displaystyle \sum_{\omega\in \Omega}X(\omega)\P(\{\omega\})&=&\displaystyle \sum_{x\in \Ima(X)}\sum_{\omega\in X^{-1}(\{x\})}X(\omega)\P(\{\omega\})\\
&=&\displaystyle \sum_{x\in \Ima(X)}\sum_{\omega\in X^{-1}(\{x\})}x\P(\{\omega\})\\
&=&\displaystyle \sum_{x\in \Ima(X)}x\sum_{\omega\in X^{-1}(\{x\})}\P(\{\omega\})\\
&=&\displaystyle \sum_{x\in \Ima(X)}x\P(X=x)\end{array}$

\subsection{Linéarité de l'espérance}
\begin{enumerate}
\item $\forall X\in \mathcal{L}^1(\Omega,\P)$, $\forall \lambda\in \R$, 
\begin{itemize}
\item $\lambda X\in \mathcal{L}^1(\Omega,\P)$
\item $\E[\lambda X]=\lambda \E[X]$
\end{itemize}
\item $\forall X,Y \in \mathcal{L}^1(\Omega,\P)$,
\begin{itemize}
\item $X+Y \in \mathcal{L}^1(\Omega,\P)$
\item $\E[X+Y]=\E[X]+\E[Y]$
\end{itemize}
\end{enumerate}

\subsubsection{Preuve}
\begin{enumerate}
\item $\displaystyle \E[\abs{\lambda X}]=\sum_{\omega\in \Omega}\abs{\lambda X(\omega)}\P(\{\omega\})=\sum_{\omega\in\Omega}\abs{\lambda}\abs{X(\{\omega\})}\P(\{\omega\})=\abs{\lambda}\E[\abs{X}]<\infty$\\

Alors $\displaystyle \E[\lambda X]=\sum_{\omega\in \Omega}\lambda X(\omega)\P(\{\omega\})=\lambda \E[X]$

\item $\displaystyle \E[\abs{X+Y}]=\sum_{\omega\in \Omega}\abs{X(\omega)+Y(\omega)}\P(\{\omega\})\leq \sum_{\omega\in\Omega}(\abs{X(\omega)}+\abs{Y(\omega)})\P(\{\omega\})\leq 
\sum_{\omega\in\Omega}\abs{X(\omega)}\P(\{\omega\})+\sum_{\omega\in\Omega}\abs{Y(\omega)}\P(\{\omega\})<+\infty$ et $X+Y\in \mathcal{L}^1(\Omega,\P)$.

$\displaystyle \E[X+Y]=\sum_{\omega\in \Omega}(X(\omega)+Y(\omega))\P(\{\omega\})$
\end{enumerate}

\subsubsection{Remarque}
$\displaystyle \sum_{n\geq 0}(a_n+b_n)\neq \sum a_n+\sum b_n$ en général, sauf si $\sum a_n$ et $\sum b_n$ sont absolument convergentes. C'est le cas ici donc $\E[X+Y]=\E[X]+\E[Y]$.

\subsubsection{Remarque}
En règle générale, + est la seule opération qui se comporte ainsi avec l'espérance.

\subsection{Corollaire}
$\mathcal{L}^1(\Omega,\P)$ est un $\R$ espace vectoriel et $\E$ : $\mathcal{L}^1(\Omega,\P)\rightarrow \R$, $X\mapsto \E[X]$.
Il s'agit d'une forme linéaire.

\subsubsection{Exemple}
Soit $A$ un évènement, $X = \indi_A$ est une variable aléatoire $X :$
$\begin{array}{rcl}
    \Omega &\rightarrow& \R \\
    \omega &\mapsto& 
        \left\{\begin{array}{rl} 
            1&\text{ si }\omega\in A \\ 
            0 &\text{ sinon}
        \end{array}\right.
\end{array}$.

$X$ est à valeurs positives, $\E[X] =\E[\indi_A]=\P(\indi_A=1)=\P(A)$ donc $\indi_A$ est sommable.\\

\textit{Généralisation}\\
Si $Y\sim \Ber(p)$ avec $p\in[0,1]$, $Y$ est sommable et $\E[Y] =p$.

\subsubsection{Loi binomiale}
Soit $X$ une variable aléatoire telle que $X\sim \Bin(n,p)$. $X$ est à valeurs positives, 

$\begin{array}{rcl}\E[X] &=& \displaystyle \sum_{k=0}^n k\times \binom{n}{k}p^k(1-p)^{n-k} \\\smallskip
&=& \displaystyle \sum_{k=0}^n \frac{k \cdot n!}{k!(n-k)!}p^k(1-p)^{n-k} \\\smallskip
&=& \displaystyle \sum_{k=1}^n \frac{n!}{(k-1)!(n-k)!}p^k(1-p)^{n-k} \\\smallskip
&=& \displaystyle \sum_{k=1}^n \frac{n \cdot (n-1)!}{(k-1)!(n-1-(k-1))!}p^k(1-p)^{n-k} \\\smallskip
&=& \displaystyle n\cdot p\cdot \sum_{k=1}^n\frac{(n-1)!}{(k-1)!(n-1-(k-1))!}p^k(1-p)^{n-k} \\
&=& \displaystyle n\cdot p\cdot \sum_{k=0}^{n-1}\binom{n-1}{i}p^i(1-p)^{n-1-i}\\
&=& \displaystyle n\cdot p
\end{array}$

\subsubsection{Loi géométrique}
Soit $X\sim \Geo(p)$, $p\in ]0,1[$, $X$ est à valeurs positives, $$\E[X] = \sum_{k=1}^{+\infty} k\times (1-p)^{k-1}\times p = p\frac{1}{(1-(1-p))^2}=\frac{1}{p}$$

\subsubsection{Loi de Poisson}
Soit $X\sim \Poi(\lambda)$, $X$ est à valeurs positives et $$\E[X]=\sum_{k=0}^{+\infty}ke^{-\lambda}\frac{\lambda^k}{k!} = \sum_{k=1}^\infty e^{-\lambda}\frac{\lambda^k}{(k-1)!} = \lambda e^{-\lambda}\sum_{k=1}^\infty \frac{\lambda^{k-1}}{(k-1)!} =\lambda$$

\subsubsection{Autre loi}
Soit $X$ à valeurs dans $\N^*$, une variable aléatoire telle que $\displaystyle \P(X=k) =\frac{6}{\pi^2}\frac{1}{k^2}$, $X$ est à valeurs positives et $X$ n'est pas sommable car $$\E[X]=\sum_{k=1}^\infty k\frac{6}{\pi^2 k^2}=\frac{6}{\pi^2}\sum_{k=1}^\infty \frac{1}{k}=+\infty$$

\section{Propriétés et applications}
\subsection{Théorème de transfert}
Soit $X : \Omega \rightarrow \R$ une variable aléatoire discrète, $f: \R\rightarrow \R$ une fonction, on s'intéresse à $f(X(\omega))$. Pour calculer si $\E[f(X)]$ existe, il faudrait à priori calculer la loi de $f(X)$. Le théorème de transfert permet de trouver l'espérance en connaissant seulement la loi de $X$.

\subsection{Énoncé}
On pose $Y = f(X)$ avec $X$ : $\Omega\rightarrow\R$ une variable aléatoire et $f:\R\rightarrow \R$ une fonction, alors 
\begin{enumerate}
\item si $\displaystyle \sum_{x\in \Ima(X)}\abs{f(x)}\P(X=x)<+\infty$, alors $Y$ est sommable
\item si $Y$ est sommable, $\displaystyle \E[Y]=\sum_{x\in \Ima(X)}f(x)\P(X=x)$.
\end{enumerate}

\subsubsection{Preuve}
On a $\displaystyle \{Y=y\}=\{\omega : f(X)=y\}=\bigcup_{\substack{x\in \Ima(X)\\f(x)=y}}\{\omega : X(\omega) = x\}=\bigsqcup_{x\in \Ima(X)\cap f^{-1}(\{y\})}\{X=x\}$\\\\

$\displaystyle \P(Y=y)=\sum_{x\in \Ima(X)\cap f^{-1}(\{y\})}\P(\{X=x\})$\\\\

$\begin{array}{rclll}
\E[\abs{Y}] &=&\displaystyle \sum_{y\in \Ima(Y)}\abs{y}\P(Y=y)\displaystyle &=& \displaystyle\sum_{y\in \Ima(Y)}\abs{y}\sum_{x\in\Ima(X)\cap f^{-1}(\{y\})}\P(X=x)\\\\
&=& \displaystyle \sum_{y\in \Ima(Y)}\abs{y}\sum_{x\in\Ima(X)}\P(X=x)\delta_{y=f(x)}&=& \displaystyle \sum_{y\in \Ima(Y)}\sum_{x\in\Ima(X)}\P(X=x)\delta_{y=f(x)}\abs{y} \\\\
&=& \displaystyle \sum_y\sum_x\P(X=x)\abs{f(x)} \delta_{y=f(x)}
\end{array}$

D'après Fubini-Tonelli, $\displaystyle\E[\abs{Y}]=\sum_{\mathclap{x\in \Ima(X)}}\P(X=x)\abs{f(x)}\sum_{\mathclap{A\in \Ima(Y)}}\delta_{j=f(x)}=\sum_{\mathclap{x\in \Ima(X)}}\abs{f(x)}\P(X=x)$.\\

La preuve de la 2e proposition est la même en enlevant les valeurs absolues et en utilisant Fubini au lieu de Fubini-Tonelli.

\subsection{Variance et écart-type}
\subsubsection{Moment}
Soit $k\in \N^*$, on dit que $X$ admet un moment d'ordre $k$ si $X^k$ est sommable. Si c'est le cas, on appelle la quantité $\E[X^k]$ le $k$ ième moment de $X$.

\subsubsection{Remarque}
$X$ admet un moment d'ordre 1 si et seulement si $X$ est sommable. Le premier moment de $X$ s'appelle l'espérance de $X$.

\subsubsection{Variance}
Si $X$ admet un moment d'ordre 2, on appelle variance de $X$ et on note $\Var(X)$ la quantité $\Var(X)=\E[(X-\E[X])^2]$.

\subsubsection{Propriété}
Soit $(j,k)$ deux entiers non nuls avec $j\leq l$, si $X$ admet un moment d'ordre $k$ alors $X$ admet un moment d'ordre $j$.

\subsubsection{Preuve}
On sait que $\abs{x^j}\leq 1+\abs{x}^k$.\\

$\displaystyle \E[\abs{X}^j]\leq \sum_{x\in\Ima(X)}(1+\abs{x}^k)\P(X=k)\leq \sum_{x\in \Ima(X)}\abs{x}^k\P(X=k)=1+\E[\abs{X^k}]$ donc $x$ admet un moment d'ordre $j$.

\subsubsection{Formule de Huygens}
$\E[(X-\E(X))^2]=\E[X^2-2X\E[X]+\E[X]^2]=\E[X^2]-2\E[X]\E[X]+\E[X]^2=\E[X^2]-\E[X]^2$

\subsubsection{Écart-type}
Si $X$ admet un moment d'ordre 2, on appelle écart-type et on note $\sigma$ la quantité\\$\sigma = \sqrt{\Var(X)}=\sigma_X=\sigma(X)$.

\subsubsection{Remarque}
$\Var(X)\geq 0$ car c'est l'espérance d'une variable aléatoire positive.

\subsubsection{Propriétés de la variance}
Soit $X$ une variable aléatoire réelle admettant un moment d'ordre 2, $a\in \R$ et $b\in \R$, alors
\begin{enumerate}
\item $\Var(aX+b)=a^2\Var(X)$
\item Si $\Var(X)=0$ alors $X$ est constante et $\P(X=\E[X])=1$.
\end{enumerate}

\subsubsection{Preuve}
\begin{enumerate}
\item $\Var(aX+b)=\E[(aX+b-a\E[X]-b)^2]=\E[a^2(X-\E[X])^2]=a^2\Var(X)$\\

\item Si $\Var(X)>0$, alors $X$ est constante presque surement, c'est à dire $\exists c\in \R$, $\P(X=c)=1$ (et donc $\E[X]=c$ d'où $\P(X=\E[X])=1$).\\

On a $\Var(X)=\E[(X-\E[X])^2]$ donc si $Y=X-\E[X]$, $\Var(X)=\sum_{y\in \Ima(Y)}y^2\P(Y=y)$ d'après le théorème de transfert.\\

Donc si $\Var(X)=0$, on a $\forall y\in \Ima(Y)$, $y^2\P(Y=y)=0$ donc soit $y=0$, soit $\P(Y=y)=0$ et donc $\P(Y=0)=1$ donc $\P(X=\E[X])=1$.
\end{enumerate}

\subsection{Variances de variables aléatoires usuelles}
\begin{enumerate}
\item $X\sim \Ber(p)$ : $\Var(X) = p(1-p)$
\item $X\sim \Bin(p)$ : $\Var(X) = np(1-p)$
\item $X\sim \Geo(p)$ : $\Var(X) = \frac{1-p}{p^2}$
\item $X\sim \Poi(p)$ : $\Var(X) = \lambda$
\end{enumerate}

\subsubsection{Démonstrations}
\begin{enumerate}
\item \textbf{Loi de Bernoulli}\\
Soit $X\sim \Ber(p)$, $\Var(X) =\E[X^2]-\E[X]^2$ par la formule de Huygens.\\Or $\E[X] = p$ et $\E[X^2] =1\times \P(X^2=1) + 0\times \P(X^2 = 0)=\P(X^2=1)=\P(X=1)=p$. Enfin $\Var(X)=p-p^2$.\\

\item \textbf{Loi binomiale}\\
Soit $X \sim \Bin(n,p)$, $\Var(X)=\E[X(X-1)+X]-\E[X]^2=\E[X(X-1)]+\E[X]-\E[X]^2$ par la formule de Huygens et la linéarité de l'espérance.\\

Or $\displaystyle \E[X(X-1)]=\sum_{k=0}^nk(k-1)\P(X=k)$ par le théorème de transfert.\\
$\begin{array}{rcl}\displaystyle \E[X(X-1)]&=& \displaystyle \sum_{k=2}^nk(k-1)\binom{n}{k}p^k(1-p)^{n-k} ~~=~~ \sum_{k=2}^nk(k-1)\frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}\\
&=&\displaystyle  p^2n(n-1) \sum_{k=2}^n\frac{(n-2)!}{(k-2)! ((n-2)-(k-2))!}p^{n-2}(1-p)^{(n-2)-(k-2)}\\
&=&\displaystyle  p^2n(n-1)\sum_{k=0}^{n-2}\binom{n-2}{k}p^k(1-p)^{n-2-k}=p^2n(n-1)\end{array}$\\

Enfin, on a donc $\Var(X)=p^2n(n-1)+np-(np)^2=np(1-p)$.\\

\item \textbf{Loi géométrique}\\
$\begin{array}{rcl}\text{Soit }X\sim \Geo(p)\text{, }\E[X(X-1)]&=&\displaystyle \sum_{k=2}^{+\infty}k(k-1)\P(X=k)=p(1-p)\sum_{k=2}^{+\infty}k(k-1)(1-p)^{n-2} \\
&=& \displaystyle p(1-p)\sum_{k=2}^{+\infty}((1-p)^k)''=p(1-p)(\sum_{k=2}^{+\infty}(1-p)^k)''\\
&=&\displaystyle p(1-p)(\frac{1}{1-(1-p)})'' = p(1-p)(\frac{2}{p^3})\end{array}$\\

Enfin, on a $\Var(X)=p(1-p)\frac{2}{p^3}+\frac{1}{p}-\frac{1}{p^2}=\frac{1-p}{p^2}$.\\

\item \textbf{Loi de Poisson}\\
$\begin{array}{rcccl}\text{Soit }X\sim\Poi(\lambda)\text{, } \E[X(X-1)]&=&\displaystyle \sum_{k=0}^{+\infty}k(k-1)\P(X=k) &=& \displaystyle  \sum_{k=2}^{+\infty}k(k-1)e^{-\lambda}\frac{\lambda^k}{k!}\\
&=&\displaystyle \sum_{k=2}^\infty e^{-\lambda}\frac{\lambda^k}{(k-2)!} &=& \displaystyle e^{-\lambda}\lambda^2\frac{k=2}{+\infty}\frac{\lambda^{k-2}}{(k-2)!}\\\\

&=&e^{-\lambda}\lambda^2e^\lambda=\lambda^2\end{array}$\\

Enfin, on a donc $\Var(X)=\lambda^2+\lambda-\lambda^2=\lambda$.
\end{enumerate}

\subsection{Inégalités usuelles}
\subsubsection{Inégalité de Markov}
Soit $Z$ une variable aléatoire discrète positive, alors pour tout $a$ positif, $\P(Z\geq a)\leq \frac{\E[Z]}{a}$.

\subsubsection{Preuve}
On introduit la variable aléatoire $Y = a \indi_{Z\geq a}$.\\

On a l'inégalité $Z\geq Y$ c'est à dire $Z\geq a\indi_{Z\geq a}$, donc $\E[Z]\geq \E[a\indi_{Z\geq a}]=a\P(Z\geq a)$.\\
Ainsi $\P(Z\geq a)\leq \frac{\E[Z]}{a}$.

\subsubsection{Remarque}
Markov est utile si $\E[X]<+\infty$, c'est à dire si $Z$ est sommable, sinon la proposition est vraie mais triviale.

\subsubsection{Inégalité de Bienaymé-Tchebytchev}
Soient $X$ une variable aléatoire admettant un moment d'ordre 2 et $a>0$, alors $$\P(\abs{X-\E[X]}\geq a)\leq \frac{\Var(X)}{a^2}$$

\subsubsection{Preuve}
On pose $Y = (X-\E[X])^2$, $Y$ est une variable aléatoire positive, on applique Markov avec $b>0$, $\P((X-\E[X])^2\geq b)\leq \frac{\E[(X-\E[X])^2]}{b}$ et $\P(\abs{X-\E[X]}\geq \sqrt{b})\leq \frac{\Var(B)}{b}$ et on pose $a = \sqrt{b}$.

\subsubsection{Inégalité de Cauchy-Schwartz}
Soient $X$ et $Y$ deux variables aléatoires qui admettent un moment d'ordre 2, alors $XY$ est sommable et $\E[XY]^2\leq \E[X^2]\E[Y^2]$

\subsubsection{Preuve}
\begin{enumerate}
\item $\abs{xy}\leq \frac{x^2+y^2}{2}$ car $(\abs{x}-\abs{y})^2\geq 0$ donc $\abs{XY}\leq \frac{X^2+\abs{Y}^2}{2}$ et $\E[\abs{XY}]\leq \frac{\E[X^2]+\E[Y^2]}{2}<\infty$.\\
\item Si $\E[Y^2]=0$, on a $\P(Y=0)=1$ donc l'inégalité est vraie.\\
Sinon $E[(X-\lambda Y)^2]\geq 0$, $\E[X^2]-2\lambda\E[XY]+\lambda^2\E[Y^2]\geq 0$.\\
On prend $\lambda = \frac{\E[XY]}{\E[Y^2]}\in \R$, $\E[X^2]-2\frac{\E[XY]^2}{\E[Y^2]}+\frac{\E[XY]^2}{\E[Y^2]}\geq 0$.
\end{enumerate}

\subsubsection{Corollaire}
En prenant $Y = 1$, $\E[X]^2\leq \E[X^2]$.

\subsection{Espérance et indépendance}
La famille $(X_1,...,X_n)$ de variables aléatoires discrètes est indépendantes si et seulement si $\forall f_1,...,f_n$ des fonctions telles que $\forall i\leq n$, $f_i(X_i)$ est sommable, alors $\E(\prod_{i=1}^nf_i(X_i))=\prod_{i=1}^n\E[f_i(X_i)]$.

\subsubsection{Preuve}
On va montrer le sens direct.\\

$\begin{array}{rcl}\displaystyle \E\left[~\abs{\prod_{i=1}^nf_i(X_i)}~\right]&=&\displaystyle \E\left[\prod_{i=1}^n\abs{f_i(X_i)}\right]\\
&=&\displaystyle 
\sum_{x_1,...,x_n \in \Ima(X_1,...,X_n)} \prod_{i=1}^n\abs{f_i(x_i)}\P((X_1,...,X_n)=(x_1,...,x_n))\\
&=&\displaystyle \sum_{x_1,...,x_n}\prod_{i=1}^n(\abs{f_i(x_i)}\P(X_i=x_i))\text{ par fubini-tonelli }\\
&=&\displaystyle \prod_{i=1}^n (\sum_{x_i\in \Ima(X_i)}\abs{f_i(x_i)}\P(X_i=x_i)) = \prod_{i=1}^n\E[\abs{f_i(X_i)}]\end{array}$

Donc $\displaystyle \prod_{i=1}^n f_i(X_i)$ est sommable et on répète le calcul ci-dessus sans la valeur absolue et en utilisant Fubini au lieu de Fubini Tonelli.

Le sens indirect s'obtient en prenant $f_i = \indi_{x_i}$.

\subsubsection{Conséquence}
Si $X$ et $Y$ sont sommables et indépendantes, on a $\E[XY]=\E[X]\E[Y]$.\\

La réciproque est fausse.

\subsection{Covariance}
On appelle $\Cov(X,Y)= \E[(X-\E[X])(Y-\E[Y])]=\E[XY]-\E[X]\E[Y]$ la covariance de $X$ et $Y$. Elle est définie si $X$ et $Y$ admettent un moment d'ordre 2.

\subsubsection{Corrélation}
Si $\Cov(X,Y)=0$, on dit que les variables aléatoires $X$ et $Y$ sont non corrélées.\\

Si deux variables sont indépendantes, alors elles sont non corrélées.

\subsubsection{Propriété}
Soient $X$ et $Y$ deux variables aléatoires admettant un moment d'ordre 2, $\Var(X+Y)=\Var(X)+\Var(Y)+2\Cov(X,Y)$.

\subsubsection{Preuve}
La preuve est triviale, il suffit de développer.

\newpage

\section{Fonctions génératrices}
\subsection{Définition}
Soit $X$ une variable aléatoire à valeurs dans $\N$, on note $G_X(s)=\sum_{n\geq 0}s^n\P(X=n)$ la somme de la série entière, définie sur son disque de convergence.

La fonction $G_X$ est appelée fonction génératrice ou fonction génératrice des moments de $X$.

\subsection{Proposition}
\begin{enumerate}
\item Le rayon de convergence de la série entière est supérieur ou égal à 1.
\item $\forall s\in [-1,1]$, $G_X(s)=\E[s^X]$
\item $G_X$ est continue sur $[-1,1]$ et $\mathcal{C}^\infty$ sur $]-1,1[$.
\end{enumerate}

\subsubsection{Preuve}
On utilise $\sum_{n\in\N}\P(X=n)=1<+\infty$, les propriétés des séries entières et le fait que \\
$G_X(s)= \E[f(X)]$ où $f : \begin{array}{rcl}\N&\rightarrow &\R \\ n&\mapsto&s^n\end{array}$ pour $s \in [-1,1]$.

\subsection{Propriété utiles}
\subsubsection{Propriété}
La fonction caractéristique d'une variable aléatoire à valeurs dans $\N$ caractérise la loi, et on a $\displaystyle \P(X=n)=\frac{1}{n!}\left[\frac{d^{(n)}}{dz^{(n)}}G_X(z) \right]\tq_{z=0}$ (évalué en 0).

\subsubsection{Preuve}
La loi de $X$ est déterminée par les $\P(X=n)$, accessibles par la formule ci-dessus, obtenue par dérivation de la série entière.

\subsubsection{Propriété}
Si $X$ admet un moment d'ordre $k$, la dérivée $k$ ème à gauche en 1 de $G_X$ est donnée par $\displaystyle \frac{d^k}{dz^k}\tq_{z=1}=\sum_{n=k}^\infty n(n-1)...(n-k+1)\P(X=n)=\E[X(X-1)...(X-k+1)]$. On l'appelle le $k$-ème moment factoriel de $X$ ou moment factoriel d'ordre $k$.

\subsubsection{Preuve}
$\displaystyle \frac{G_X(z)-G_X(1)}{z-1}=\sum \P(X=n)\frac{1-z^n}{1-z}=\sum \P(X=n)(1+z+z^2+...+z^n)$ or $\sum n \P(X=n)$ converge dès que $X$ admet un moment d'ordre 1, donc on a convergence normale et en passant à la limite, $\lim\limits_{x\rightarrow 1^-}\frac{G_X(z)-G_x(1)}{z-1}=\E[X]$.

\subsubsection{Application}
Si $X$ admet un moment d'ordre 2, on a $\Var(X)=G_X''(1)+G_x'(1)-G_x'(1)^2$

\subsubsection{Propriété}
Soient $X$ et $Y$ deux variables aléatoires à valeurs dans $\N$ indépendantes, alors $G_{X+Y}(s)=G_X(s)\cdot G_Y(s)$

\subsubsection{Preuve}
$G_{X+Y}(s)=\E[s^{X+Y}]=\E[s^xs^y]=\E[s^x]\E[s^y]=G_X(s)\cdot G_Y(s)$

\subsection{Exemples}
\subsubsection{Bernoulli}
$G_X(s)=\E[s^X]=(1-p)+ps$

\subsubsection{Remarque}
Il s'agit d'un polynôme, comme les fonctions génératrices des variables aléatoires qui ne prennent qu'un nombre fini de valeurs.

\subsubsection{Binomiale}
$\displaystyle G_X(s)=\sum_{k=0}^ns^k\binom{n}{k}p^k(1-p)^{n-k}=(1-p+ps)^n$

\subsubsection{Remarque}
Si $X_1,...,X_n$ sont des $\Ber(p)$ indépendantes, $G_{X_1+...+X_n}\sim\Bin(n,p)$

\subsubsection{Géométrique}
$\displaystyle G_X(s)=\sum_{k=1}^\infty s^kp(1-p)^{k-1}=\frac{ps}{1-(1-p)s}$ lorsque $\displaystyle \abs{s}\leq\frac{1}{1-p}$

\subsubsection{Poisson}
$\displaystyle G_X(s)=\sum_{k=0}^\infty s^ke^{-\lambda}\frac{\lambda^k}{k!}=e^{-\lambda}e^{s\lambda}=e^{\lambda(s-1)}$

\subsubsection{Application}
Soit $X\sim\Poi(\lambda)$, $Y\sim\Poi(\mu)$, $X$ et $Y$ indépendantes, alors\\$\displaystyle G_{X+Y}(s)=G_X(s)G_Y(s)=e^{\lambda(s-1)}e^{(\mu(s-1))}=e^{(s-1)(\lambda+\mu)}$
donc $X+Y \sim(\Poi(\lambda+\mu))$.



\chapter{Variables aléatoires à densité}
\section{Introduction}
\subsection{Idée}
On considère des expériences aléatoires dont le résultat peut être tout réel d'un intervalle.

\subsection{Problème}
On ne se situe pas dans le cas où $\Omega$ est dénombrable : on va travailler avec $\Omega$ indénombrable et abstrait.\\

On s'intéresse ici aux probabilités de la forme $\P(X\in I)$ où $I$ est un intervalle réel.

\section{Variables aléatoires à densité}
\subsection{Densité de probabilité}
On appelle densité de probabilité toute fonction $f : \R\rightarrow \R^+$ continue par morceaux telle que $\displaystyle \int_\R f(x)dx = \int_{-\infty}^{+\infty}f(x)dx=1$.

\subsection{Variable aléatoire à densité}
On dit que $X$ est une variable aléatoire à densité de densité $f$ si $f$ est une densité et pour tout couple $(a,b)$ de réels où $a\leq b$, $\displaystyle \P(X\in [a,b]) =\int_a^bf(x)dx$.

\subsection{Propriétés}
Par analogie avec les chapitres précédents : \\
\begin{enumerate}
\item Si $A$ est le complémentaire de $[a,b]$, $\displaystyle A = ]-\infty,a[~\cup~]b,+\infty[$,\\$\displaystyle \P(X\in A)=1-\P(X\in [a,b])=1-\int_a^bf(x)dx$.

\item Si $\displaystyle A =\bigcup_{i\in \N}A_i$ avec les $A_i = [a_i, b_i]$ disjoints, $\displaystyle \P(X\in A)=\sum_{i\in \N}\P(X\in A_i)=\sum_{i\in \N}\int_{a_i}^{b_i}f(x)dx$.
\end{enumerate}

\newpage

\subsection{Corollaire}
$\forall a\in \R$, $\P(X = a)=0$ si $X$ est une variable aléatoire à densité.

\subsubsection{Démonstration}
$\P(X=a)=\P(X\in[a,a])=\int_a^af(x)dx=0$.\\

De plus, $\P(X\in [a,b])=\P(X\in ]a,b])= \P(X\in [a,b[)=\P(X\in ]a,b[)$.

\subsubsection{Remarque}
$\P(X\leq t) = \int_{-\infty}^t f(x)dx$.

\subsection{Fonction de répartition}
Soit $X$ une variable aléatoire à densité de densité $f_X$, on appelle fonction de répartition et on note $F_X$ la fonction $F_X$ : $\begin{array}{rcl}\R&\rightarrow&[0,1]\\t &\mapsto& \int_{-\infty}^tf_X(x)dx\end{array}=\P(X\leq t)$.

\subsection{Propositions}
\begin{enumerate}
\item $\lim\limits_{t\rightarrow -\infty} F_X(t)=0$
\item $\lim\limits_{t\rightarrow +\infty} F_X(t)=1$
\item $F_X$ est croissante, continue, dérivable, sauf aux point de discontinuité de $f_X$.
\end{enumerate}

\subsection{Propriété}
Si $X$ est une variable aléatoire à densité de fonction de répartition $F_X$,\\
alors $\forall a\leq b$, $\P(X\in [a,b])=F_X(b)-F_X(a)$

\section{Lois usuelles}
\subsection{Loi uniforme}
Soient $a<b$ deux réels.\\

On dit que $X$ est de loi uniforme sur $[a,b]$ si $X$ est une variable aléatoire à densité de densité $f_X$ où $\displaystyle f_X(t)=\frac{1}{b-a}\indi_{[a,b]}(t)$.\\

On vérifie que $f_X$ est bien une densité : $\displaystyle \int_{-\infty}^{+\infty}f_X(t)dt=\int_a^bf(t)dt=1$.\\

La fonction de répartition est définie par $\displaystyle F_X(t)=\P(X\leq t)=\int_{-\infty}^tf_X(x)dx=\left\{\begin{array}{rl}
0&\text{ si }t\leq a \\
\frac{t-a}{b-a} &\text{ si } t\in ]a,b[\\
1&\text{ si }t\geq b
\end{array}\right.$\\\\

On dit que $X$ est uniforme sur $[a,b]$ et on note $X\sim\Uni([a,b])$.\\

Soit $\epsilon>0$ et $x\in [a,b]$ tel que $\displaystyle a\leq x<x+\epsilon\leq b$, alors $\displaystyle \P(X\in [x,x+\epsilon]) = \int_x^{x+\epsilon}\frac{1}{b-a}dt = \frac{\epsilon}{b-a}$ qui ne dépend pas de $x$, d'où le nom uniforme.

\subsection{Loi exponentielle}
On se donne $\lambda>0$ et $\displaystyle f_X(t)=\lambda e^{-\lambda t}\indi_{\R^+}(t)$.\\
$f_X$ est une densité car elle est trivialement positive, continue par morceaux\\
et $\displaystyle \int_{-\infty}^{+\infty}f_X(t)dt=\int_0^\infty\lambda e^{-\lambda t}=1$.\\

Si $X$ est une variable aléatoire de densité $f_X$, la fonction de répartition de $X$ est définie par $\displaystyle F_X(t)=\int_{-\infty}^tf_X(x)dx=(1-e^{-\lambda t})\indi_{\R^+}(t)$.\\

La loi exponentielle est utilisée pour décrire des phénomènes sans mémoire.\\\\
$t>0$, $\epsilon>0$, $\displaystyle \P(X\in [t, t+\epsilon]\tq X\geq t)=\frac{\P([t,t+\epsilon])}{\P(X\geq t)}=\frac{\int_t^{t+\epsilon}\lambda e^{-\lambda x}dx}{e^{-\lambda t}}=e^{\lambda t}\left[-e^{-\lambda x}\right]_t^{t+\epsilon}=1-e^{-\lambda \epsilon}$.\\

On dit que $X\sim \Exp(\lambda)$.

\subsection{Loi gaussienne}
La fonction $g$ définie par $\displaystyle g(x)=\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}$ est une densité de probabilité. En effet, $g$ est positive, $g$ est continue et $g$ est intégrable au voisinage de l'infini. Son intégrale sur $\R$ vaut 1.

\subsubsection{Démonstration}
$\begin{array}{rcl}\displaystyle (\int_\R g(x)dx)^2 &=& \displaystyle \int_\R g(x)dx\int_\R g(y)dy = \int_\R\int_\R g(x)g(y)dxdy = \int_\R\int_\R\frac{1}{2\pi}e^{-\frac{x^2+y^2}{2}}dxdy \\\\
&=&\displaystyle \int_{R^2}\frac{1}{2\pi}e^{-\frac{x^2+y^2}{2}} d(x,y) = \int_{\R^+}\int_{0}^{2\pi}\frac{1}{2\pi}e^{-\frac{r^2}{2}}rdrd\theta \\\\
&=&\displaystyle \int_{\R^+}re^{-\frac{r^2}{2}}dr = \left[-e^{-\frac{r^2}{2}}\right]_0^{+\infty}=1 \text{ donc }\int_\R g(x)dx=1\end{array}$

\subsubsection{Remarque}
Il n'y a pas de forme explicite pour la primitive de $g$. Une variable aléatoire de densité $g$ est appelée gaussienne centrée réduite, et se note $X\sim \Nor(0,1)$.\\

Plus généralement, on dit que $X\sim\Nor(\mu, \sigma^2)$ si $X$ est à densité de densité $\displaystyle f_X(t)= \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$. On a alors $\mu$ la moyenne et $\sigma^2$ la variance de $X$.

\subsection{Proposition}
Si $X\sim\Nor(0,1)$ et $Y = aX+b$, alors $Y\sim\Nor(b,a^2)$.

\subsubsection{Preuve}
$\begin{array}{rcl}\displaystyle \P(Y\in [\alpha, \beta]) &=& \displaystyle \P(aX+b\in [\alpha,\beta]) = \P(aX\in [\alpha-b,\beta-b]) = \P(X\in [\frac{\alpha-b}{a},\frac{\beta-b}{a}]) \\ &=&\displaystyle \int_{\frac{\alpha-b}{a}}^{\frac{\beta-b}{a}}\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}dx=\int_\alpha^\beta\frac{e^{-\frac{(u-b)^2}{2a^2}}}{\sqrt{2\pi}}\frac{du}{a} = \int_\alpha^\beta \frac{e^{-\frac{(u-b)^2}{2a^2}}}{\sqrt{2\pi a^2}}du\end{array}$\\
donc $Y$ est bien à densité de densité $\displaystyle \frac{1}{\sqrt{2\pi a^2}}e^{-\frac{(x-b)^2}{2a^2}}$.

\subsubsection{Remarque}
La loi gaussienne sert à quantifier des erreurs de mesures aussi bien physiques que statistiques.

\section{Moments de variables aléatoires à densité}
\subsection{Espérance d'une fonction d'une variable aléatoire à densité}
\subsubsection{Définition}
Soient $X$ une variable à densité de densité $f_X$ et $\phi : \R\rightarrow \R$ une fonction continue par morceaux. On dit que $\phi(X)$ est intégrable (ou admet une moyenne) si $\displaystyle \int_\R \abs{\phi(t)}f_X(t)dt$ est fini.\\
On note alors $\displaystyle \E[\phi(X)]=\int_\R \phi(t)f_X(t)dt$.

\subsubsection{Remarque}
Cette définition est à rapprocher du théorème de transfert.

\subsubsection{Remarque}
Si $\phi(t)=t$, on dit que $X$ admet une espérance si $\displaystyle \int_\R \abs{t}f_X(t)dt$ est fini.\\
On aura alors $\displaystyle \E[X]=\int_\R tf_X(t)dt$.

\subsection{Proposition}
\begin{enumerate}
\item Si $a\in \R$ et $\phi(x)=a$ alors $\phi(X)$ est intégrable et $\E[\phi(X)]=a$.\\

\item Si $\abs{\phi}\leq \psi$ et $\displaystyle \int_\R\psi(x)f_X(x)dx < +\infty$ alors $\phi(X)$ est intégrable et\\
$\E[\phi(X)]\leq \E[\abs{\phi(X)}]\leq \E[\psi(X)]$.

\item Si $\phi_1$ et $\phi_2$ sont intégrables, $\E[a\phi_1(X)+b\phi_2(X)]=a\E[\phi_1(X)]+b\E[\phi_2(X)]$.
\end{enumerate}

\subsubsection{Preuve}
\begin{enumerate}
\item $\displaystyle \int_\R \abs{\phi(x)}f_X(x)dx=\int_\R\abs{a}f_X(x)dx=\abs{a}\int_\R f_X(x)dx$ puis on fait le même calcul sans les valeurs absolues.
\item $\displaystyle \int_\R\abs{\phi(x)}f_X(x)dx\leq \int_\R\psi(x)f_X(x)dx$ qui est fini.\\
$\displaystyle \int_\R \phi(x)f_X(x)dx\leq \int_\R\abs{\phi(x)}f_X(x)dx\leq \int_\R \psi(x)f_X(x)dx$
\item Immédiat avec l'inégalité triangulaire.
\end{enumerate}

\subsubsection{Proposition}
Soit $X$ une variable aléatoire à densité de densité $p_X$ vérifiant $\P(X\in \R_+)=1$. Si $X$ admet une espérance alors $\E[X]\geq 0$.

\subsubsection{Preuve}
$\P(X\in \R_+)=1$ donc $\P(X\in \R_-^*) = 0$ et $\displaystyle \int_{\R_-}p_X(x)dx=0$.\\

Or $p_X\geq 0$ donc $p_X$ est nulle sauf en un nombre fini ou dénombrable de points.\\

$\displaystyle \int_{\R_-}xp_X(x)dx=0$ donc $\E[X]=\displaystyle \int_\R xp_X(x)dx$.

\subsection{Moments de variables aléatoires à densité}
\subsubsection{Définition}
On appelle, si elle existe, la quantité $\displaystyle \E[X^n]=\int_\R x^n p_X(x)dx$ le moment d'ordre $n$ de la variable $X$ de densité $p_X$.\\

On dit donc que $X$ admet un moment d'ordre $n$ si $\displaystyle \int_\R \abs{x}^np_X(px)dx < +\infty$.

\subsubsection{Remarque}
Le moment d'ordre 1 est l'espérance.

\subsection{Propositions}
Si $X$ admet un moment d'ordre $n \geq 1$, alors $\forall i \leq n$, $X$ admet un moment d'ordre $i$.

\subsubsection{Preuve}
$\displaystyle \abs{x}^i\leq 1 +\abs{x}^n$ donc $\displaystyle \int_\R \abs{x}^i p_X(x)dx \leq \int_\R(1+\abs{x}^n)p_X(x)dx \leq \int_\R p_X(x)dx + \int_\R \abs{x}p_X(x)dx$\\
Donc $X$ admet un moment d'ordre $i$.

\subsection{Moment centré}
On introduit le moment centré d'ordre $n$ de la variable aléatoire $X$ $\E[(X-\E[X])^n]$ si $X$ admet un moment d'ordre $n\geq 1$.

\subsection{Variance}
On appelle variance de $X$ et on note $\Var(X)$ le moment centré d'ordre 2 de $X$ si $X$ admet un moment d'ordre 2.\\

\subsubsection{Formule de Huygens}
$\Var(X) =\E[(X-\E[X])^2] = \E[X^2]-\E[X]^2$

\subsection{Propriété}
Si $X$ admet un moment d'ordre 2, $\lambda,\mu\in \R^2$, alors $\Var(\lambda X + \mu) =\lambda^2\Var(X)$

\subsubsection{Preuve}
$(\lambda X+\mu - \E[\lambda X + \mu])^2 = (\lambda X - \lambda \E[X])^2  = \lambda^2(X-\E[X])^2$ donc\\$\Var(\lambda X +\mu)= \E[\lambda^2 (X- \E[X])^2] = \lambda^2\Var(X)$.

\newpage

\subsection{Moments des lois usuelles}
\subsubsection{Variables uniforme}
On a $\displaystyle p_X(x) = \frac{1}{b-a}\indi_{[a,b]}(x)$.\\

$\displaystyle \int_\R \frac{1}{b-a}\indi_{[a,b]}(x)\abs{x}dx = \int_a^b \frac{\abs{x}}{b-a}dx < +\infty$ donc $\E[X]$ existe et\\
$\displaystyle \E[X] = \int_\R \frac{1}{b-a}\indi_{[a,b]}(x)xdx = \int_a^b \frac{x}{b-a}dx = \frac{1}{b-a}\frac{b^2-a^2}{2} = \frac{a+b}{2}$.\\\\

$\displaystyle \E[X^2] = \int_\R \frac{x^2}{b-a} \indi_{[a,b]}(x)dx = \frac{a^2 + ab + b^2}{3}$.\\

Enfin $\Var(X) = \E[X^2] - \E[X]^2 = \frac{(b-a)^2}{12}$.

\subsubsection{Variables exponentielles}
$p_Y(x) = \lambda e^{-\lambda x}\indi_{\R^+}(x)dx$\\

$\displaystyle \int_\R \abs{x}\lambda e^{-\lambda x }\indi_{\R^+}dx < +\infty$\\

$\displaystyle \E[Y] = \int_\R x \lambda e^{-\lambda x} \indi_{\R^+}(x)dx = \int_0^{\R^+} \lambda x e^{-\lambda x}dx = \left[-x e ^{-\lambda x}\right]_0^{+\infty} + \int_0^{+\infty}e^{-\lambda x }dx = \frac{1}{\lambda}$\\

$\displaystyle \E[Y^2] = \int_0^{+\infty} x^2 \lambda e^{-\lambda x}dx = \left[-x^2e^{-\lambda x}\right]_0^{+\infty} + 2\int_0^{+\infty} xe^{-\lambda x}dx = \frac{2}{\lambda}\E[Y] - \frac{2}{\lambda^2}$.\\

Enfin $\Var(Y)= \frac{1}{\lambda^2}$.

\subsubsection{Variables gaussiennes}
$\displaystyle p_Z(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$\\

$\displaystyle \int_\R \abs{x}p_Z(x)dx < +\infty$ car $\abs{x}$ croît comme un polynôme et l'exponentielle décroit beaucoup plus vite.\\

$\displaystyle \E[Z] = \int_\R x\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}dx = \frac{1}{\sqrt{2\pi}}[e^{-\frac{x^2}{2}}]_\R = 0$ (la fonction est impaire).\\

$\displaystyle \E[Z^2] = \int_\R \frac{x^2}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}dx = \left[\frac{-xe^{-\frac{x^2}{2}}}{\sqrt{2\pi}}\right]_\R + \int_\R \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}dx = 1$.\\\\

Si $U = \sigma Z + \mu$, alors $U\sim \Nor (\mu, \sigma^2)$, $\E[U]=\sigma \times 0  + \mu =\mu$ et $\Var(U) = \sigma^2 \Var(z) = \sigma^2$.

\subsubsection{Remarque}
Dans la notation $\N(\mu, \sigma^2)$, $\mu$ est l'espérance et $\sigma^2$ la variance.

\newpage

\section{Inégalités pour les variables à densité}
\subsection{Inégalité de Markov}
Soit X une variable aléatoire à densité admettant un moment d'ordre 1, X à valeurs positives, alors on a
$$\forall a>0, \P(X\geq a)\leq \frac{\E[X]}{a}$$

\subsubsection{Preuve}
$\displaystyle a\P(X\geq a) = a\int_a^{+\infty} p_X(x)dx \leq \int_a^{+\infty}xp_X(x)dx$ car $a \leq x$ si $x \in [a; +\infty]$, et donc\\
$\displaystyle a\P(X\geq a) \leq \int_0^{+\infty} xp_X(x)dx = \E[X]$.

\subsection{Inégalité de Bienaymé-Tchébychev}
Soit Y une variable aléatoire à densité admettant un moment d'ordre 2, alors 
$$\forall t > 0, \P(\abs{Y-\E[Y]}\geq t)\leq \frac{\Var(Y)}{t^2}$$

\subsubsection{Preuve}
On regarde $X = (Y - \E[Y])^2$, une variable aléatoire à valeurs positives.\\
D'après l'inégalité de Markov, $\P(X\geq s)\leq \frac{\E[Y]}{s}$, donc $\P((Y-\E[Y])^2)\geq s)\leq \frac{\Var(Y)}{s}$.\\

Posons $t = \sqrt{s}$, $\P(\abs{Y - \E[Y]}\geq t) \leq \frac{\Var(Y)}{t^2}$

\section{Familles de variables aléatoires à densité, vecteur aléatoire à densité}
\subsection{Définition, première propriétés}
On dit que la famille $(X_1,...,X_n)$ est une famille de variables aléatoires à densité s'il existe une fonction $p : \R^n \rightarrow \R_+$ continue par morceaux telle que\\
$\forall x_1,...,x_n\in \R^n$, $\P(X1\leq x_1, ..., X_n\leq x_n) = \int_{-\infty}^{x_1} ... \int_{-\infty}^{x_n} p(t1,...,t_n)d(t_1,...,t_n)$.\\

La fonction $p$ est appelée densité disjointe de la famille $(X_1,...,X_n)$.\\

On dit aussi que le vecteur $X = \begin{pmatrix}X_1 \\ \vdots \\ X_n\end{pmatrix}$ est un vecteur aléatoire à densité de densité $p$.

\subsubsection{Remarque}
$\displaystyle \int_\R ... \int\R p(t_1,...,t_n)d(t_1,...,t_n) = 1$.

\newpage

\subsection{Formule des marginales}
Si $X=\begin{pmatrix}
X_1\\\vdots\\X_n
\end{pmatrix}$ est un vecteur aléatoire à densité de densité $p$, alors $X_1$ est une variable à densité de densité $\displaystyle f(x) = \int_\R ... \int_\R p(x,t_2,...,t_n)d(t_2,...,t_n)$.

\subsubsection{Preuve}
$\begin{array}{rcl}\displaystyle \P(X_1\leq x) &=&\displaystyle  \lim\limits_{M\rightarrow +\infty} \P(X_1\leq x, X_2\leq M, ..., X_n\leq M)\\\\
&=& \displaystyle \lim\limits_{M\rightarrow +\infty} \int_{-\infty}^x (\int_{-\infty}^M... \int_{-\infty}^M p(t,t_2,...,t_n)d(t_2,...,t_n) )dt\\\\
&=& \displaystyle \int_{-\infty}^x(\int_\R ... \int_\R p(t, t_2, ..., t_n)d(t_2,...,t_n))dt\end{array}$

\subsection{Espérance}
Si $f: \R^n \rightarrow \R$ continue par morceaux et $X$ un vecteur aléatoire de $\R^n$ à densité $f(X)$ est intégrable si $\displaystyle \int_\R ... \int_\R \abs{f(x_1,...,x_n)}p(x_1,...,x_n)d(x_1,...,x_n) < +\infty$.\\

On définit alors $\E[f(X)] = \int_\R...\int_\R f(x_1,...,x_n)p(x_1,...,x_n)d(x_1,...,x_n)$.

\subsection{Indépendance}
Cas d'un couple de variables aléatoires.

\subsubsection{Définition}
On dit que le couple $(X,Y)$ est un couple de variables aléatoires à densité indépendantes si $(X,Y)$ est à densité de densité $p_{(X,Y)}(x,y)=p_X(x)p_Y(y)$.

\subsubsection{Cas général}
La famille $(X_1,...,X_n)$ de variables aléatoires à densité est indépendante si c'est une famille à densité de densité $\displaystyle p_{(X_1,...,X_n)}=\prod_{i=1}^{n}p_{X_i}(x_i)$.

\subsection{Propriété}
Si $X$ est un vecteur à densité dont les coordonnées sont indépendantes et $f_1,...,f_n$ des fonctions telles que $f_1(X_1),...,f_n(X_n)$ sont intégrables, alors $\displaystyle \E[\prod_{i=1}^{n}f_i(X_i)] = \prod_{i=1}^{n}\E[f_i(X_i)]$.

\subsection{Covariance}
Si $(X,Y)$ est un couple à densité, on définit quand c'est possible\\
$\Cov(X,Y) = \E[(X-\E[X])(Y-E[Y])]$

\subsubsection{Remarque}
Si $X\ind Y$ alors $\Cov(X,Y) = 0$ mais on n'a pas la réciproque.


\chapter*{Loi des grands nombres}
\section*{Énoncé}
Soit $(X_i)_{i\in\N}$ une famille de variables aléatoires discrètes indépendantes, identiquement distribuées (elles suivent la même loi) et admettant un moment d'ordre 2.\\

On définit pour $n\geq 1$, $\displaystyle S_n = \sum_{i= 1}^nX_i$. $\forall \epsilon>0$, $$\lime{\P(\abs{\frac{S_n}{n}-\E[X_1]}\geq \epsilon)}{n}{+\infty}{0}$$

\subsection*{Preuve}
$\displaystyle \E[S_n] = \E[\sum_{i=1}^n X_i] =\sum_{i= 1}^n\E[X_i] = n\E[X_1]$.\\

$\displaystyle \Var(S_n) = \Var(\sum_{i =1}^nX_i) = \sum_{i=1}^n\Var(X_i)=n\Var(X_1)$ car les $X_i$ sont indépendantes.\\

D'après Bienaymé-Tchébytchev, $\displaystyle \P(\abs{S_n-\E[S_n]}\geq t)\leq \frac{\Var(S_n)}{t^2}$ $\forall t>0$.\\

On prend $t =\epsilon n$, $\displaystyle \P(\abs{\frac{S_n}{n}-\E[X_1]}\geq \epsilon)\leq \frac{n\Var(X_1)}{(\epsilon n)^2}=\frac{1}{n}\times \frac{\Var(X_1)}{\epsilon^2} \rightarrow 0$ quand $n \rightarrow +\infty$.

\end{document}